\makeheading{Lecture 26}
\textbf{Example}

If $ Z \sim N(0,1) $, find $ d $ such that $ P(|Z|<d)=0.9 $.
\begin{align*}
    P(|Z|<d) & =P(-d<Z<d)                            \\
             & =P(Z<d)-P(Z>-d)                       \\
             & =P(Z\le d)-\left[ 1-P(Z\le d) \right] \\
             & =2P(Z\le d)-1
\end{align*}
\begin{align*}
     & 2P(Z\le d)-1=0.90                   \\
     & \implies P(Z\le d)=\frac{0.90+1}{2} \\
     & \implies F(d)=0.95                  \\
     & \implies F^{-1}(F(d))=F^{-1}(0.95)  \\
     & \implies d=1.6449
\end{align*}

\chapter{Multivariate Distributions}
\section{Basic Terminology and Techniques}
We have models for a single RV (both discrete or cont.) but we often
care about two or more RV's at the same time (and their relationship)
Examples:
\begin{itemize}
    \item two stock returns
    \item heights and weights
    \item number of cards of a rank vs number of a suit
    \item treatment vs recovery time
    \item all machine learning classification and regression
\end{itemize}
In this course, we focus on all discrete random variables

\begin{defbox}
    \subsection{Definition (Joint Probability Function)}
    Let $ X_1,\ldots,X_n $ be $ n $ discrete random variables.
    We define the \emph{joint probability function} $ f(x_1,\ldots,x_n) $ of
    $ (X_1,\ldots,X_n) $ as
    \begin{align*}
        f(x_1,\ldots,x_n) & =P(X_1=x_1 \text{ and } \cdots \text{ and } X_n=x_n) \\
                          & =P(X_1=x_1,\ldots,X_n=x_n)
    \end{align*}
\end{defbox}

\begin{thmbox}
    \subsection{Theorem}
    \begin{itemize}
        \item $ \sum\limits_{\text{all } (x_1,\ldots,x_n)} f(x_1,\ldots,x_n)=1 $
        \item $ f(x_1,\ldots ,x_n)\ge 0 $ for all $ (x_1,\ldots,x_n) $
    \end{itemize}
\end{thmbox}

\textbf{Example}

Suppose we flip a coin 3 times. Let $ X= $ \# heads.
Let
\[ Y=\begin{cases}
        1,\,\text{ if first flip is a H} \\
        0,\, \text{ otherwise}
    \end{cases} \]
Find $ f(x,y) $.

\begin{tabular}{| *{5}{>{\centering\arraybackslash}p{2cm} |}}
    \hline
    $y\backslash x$ & 0               & 1                & 2                & 3               \\
    \hline
    $0$             & $ \sfrac{1}{8}$ & $ \sfrac{2}{8} $ & $ \sfrac{1}{8} $ & $\sfrac{0}{8} $ \\
    \hline
    $1$             & $ \sfrac{0}{8}$ & $ \sfrac{1}{8} $ & $ \sfrac{2}{8} $ & $\sfrac{1}{8} $ \\
    \hline
\end{tabular}
$ f(x,y) $ can be represented in a table or as a function of $ x $ and $ y $
(not usually a histogram).

Now suppose we are only interested in one of the random variables, e.g.\ suppose
we are only want to find out about $ X $.
\[ P(X=x)=f(0,0)+f(0,1)=\frac{1}{8} +0=\frac{1}{8} \]

\begin{defbox}
    \subsection{Definition (Marginal Probability Function)}
    Let $ X $ and $ Y $ be two discrete random variables.
    We define the \emph{marginal probability function} of $ X $ as
    \[ f_X(x)=\sum\limits_{\text{all } y}f(x,y) \]
    and the \emph{marginal probability function} of $ Y $ as
    \[ f_Y(y)=\sum\limits_{\text{all } x}f(x,y) \]
\end{defbox}

\begin{defbox}
    \subsection{Definition (Independent Random Variables)}
    $ X_1,\ldots,X_n $ are \emph{independent random variables} if
    and only if
    \[ f(x_1,\ldots ,x_n)=f_1(x_1)\cdots f_n(x_n) \]
    for all $ (x_1,\ldots,x_n) $.
\end{defbox}
From example: Are $ X $ and $ Y $ independent? No.
$ f(0,0)=\frac{1}{8} \neq f_X(0)f_Y(0)=\frac{1}{8}\cdot \frac{1}{2} $
shortcut: any $ 0 $ in your table $ \rightarrow $ dependent.
