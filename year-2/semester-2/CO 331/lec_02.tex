\makeheading{ 2020-01-08 }
\section{Fundamental Concepts}
Message source
$ \rightarrow $
$ \underbrace{
        \underset{\text{(binary strings)}}{\text{Source encoder}}
        \rightarrow
        \underset{\text{(adds redundancy to message)}}{\text{Channel encoder}}
        \underset{\overset{\Big\uparrow}{\text{noise}}}{\overset{\text{channel}}{\rightarrow}}
        \text{Channel decoder}}_{\text{this course}}$
$ \rightarrow \text{Source Decoder} \rightarrow \text{Message} $

\begin{defbox}
    \begin{definition}
        An \textbf{alphabet} $ A $ is a finite set of $ |A|= q\geqslant 2 $ symbols.
    \end{definition}
\end{defbox}

\begin{defbox}
    \begin{definition}
        A \textbf{word} is a finite sequence (\textbf{tuples} or \textbf{vectors})
        of symbols from an alphabet $ A $.
    \end{definition} \end{defbox}

\begin{defbox}
    \begin{definition}
        The \textbf{length} of a word is the number of symbols in it.
    \end{definition} \end{defbox}

\begin{defbox}
    \begin{definition}
        A \textbf{code} $ C $ over $ A $ is a finite set of words in $ A $
        with $ |C|\geqslant 2 $.
    \end{definition} \end{defbox}

\begin{defbox}
    \begin{definition}
        A \textbf{codeword} $ \bm{c} $ is a word in code $ C $.
    \end{definition} \end{defbox}

\begin{defbox}
    \begin{definition}
        A \textbf{block code} is a code where all codewords have the same length.
        A block code $ C $ of length $ n $ containing $ M $ codewords over $ A $
        is a subset $ C\subseteq A^n $, with $ |C|=M $. We refer to such a block
        code as an $ [n,M] $-code over $ A $.
    \end{definition}
\end{defbox}

\begin{exbox}
    \begin{example}[Block Code]
        Let $ A=\{0,1\} $ and $ C=\{00000,\,11100,\,00111,\,10101\} $.
        $ C $ is a $ [5,4] $-code over $ \{0,1\} $.
        \begin{table}[H]
            \centering
            \begin{tabular}{@{}ccc@{}}
                Messages & \textrightarrow{} & Codewords \\
                \midrule
                00       & \textrightarrow{} & 00000     \\
                10       & \textrightarrow{} & 11100     \\
                01       & \textrightarrow{} & 00111     \\
                11       & \textrightarrow{} & 10101     \\
            \end{tabular}
        \end{table}
        The encoding is a one-to-one map.
    \end{example}
\end{exbox}

The channel encoder transmits only codewords, but what's received by the channel
decoder might not be a codeword. For example, suppose the channel decoder
receives $ \bm{r}=11001 $. What should it do? In our above example, we can see
that $ \bm{r} $ is closest to $ 11100 $ and $ 10101 $ (only two bits are different),
so it's possible that the codeword was one of those two. However,
this may not be the case in practice.

\section{Assumptions About the Communications Channel}
\begin{enumerate}[label=(\arabic*)]
    \item The channels only transmit symbols from $ A $.
    \item No symbols are deleted, added, or transposed.
    \item Errors are random
\end{enumerate}

\begin{exbox}
    \begin{example}[Binary Symmetric Channel, BSC]
        Let $ A=\{0,1\} $, and $ p $ denote the symbol error probability.
        The encoding map is:
        \begin{center}
            \input{figures/bsc.pdf_tex}
        \end{center}
    \end{example}
    A similar encoding map can be drawn for $ A=\{0,1,2\} $,
    with symbol error probability $ \sfrac{p}{2} $.
\end{exbox}

Suppose that the symbols transmitted are $ X_1,X_2,\ldots $,
and the symbols received are $ Y_1,Y_2,\ldots $. Then for all
$ i\geqslant 1 $, $ j\geqslant 1 $, $ k\leqslant q $, the probability
that $ Y_i $ is received, given that $ X_i $ is transmitted is:
\[ P(Y_i=a_j\mid X_i=a_k)=
    \begin{cases}
        1-p,           & \text{if } j=k     \\
        \frac{p}{q-1}, & \text{if } j\neq k
    \end{cases} \]

\section{Notes about BSC}
\begin{enumerate}[label=(\roman*)]
    \item If $ p=0 $, the channel is perfect.
    \item If $ p=\sfrac{1}{2} $, the channel is useless.
    \item If $ \sfrac{1}{2}<p\leqslant 1 $, then simply flip all bits that are received.
    \item WLOG, we can assume $ 0<p<\sfrac{1}{2} $.
    \item Analogously, for a $ q $-ary channel, we can assume that $ 0<p<\frac{q-1}{q} $.
\end{enumerate}

\begin{defbox}
    \begin{definition}
        If $ \bm{x},\bm{y}\in A^n $, the \textbf{Hamming distance} $ d(\bm{x},\bm{y}) $ is
        the number of coordinate positions in which $ \bm{x} $ and $ \bm{y} $ differ.
    \end{definition} \end{defbox}

\begin{exbox}
    \begin{example}[Hamming Distance]
        Let $ \bm{x} = 10111 $ and $ \bm{y} = 01010 $. The Hamming distance
        of $ \bm{x} $ and $ \bm{y} $ is $  d(\bm{x},\bm{y})=4  $
        since $ \bm{x} $ and $ \bm{y} $ differ in the coordinate positions
        1, 2, 3, and 5.
    \end{example}
\end{exbox}

\begin{defbox}
    \begin{definition}
        Let $ C $ be an $ [n,M] $-code.
        The \textbf{Hamming distance $ d $ of a code} $ C $ is
        \[ d(C)=\min \{d(\bm{x},\bm{y}):\bm{x},\bm{y}\in C,\;\bm{x}\neq \bm{y}\} \]
    \end{definition} \end{defbox}

\begin{thmbox}
    \begin{theorem}
        $ d $ is a \textbf{metric}. For all $ \bm{x},\bm{y},\bm{z}\in A^n $:
        \begin{enumerate}[label=(\arabic*)]
            \item $ d(\bm{x},\bm{y})\geqslant 0 $
            \item $ d(\bm{x},\bm{y})=0 $ if and only if
                  $ \bm{x}=\bm{y} $
            \item $ d(\bm{x},\bm{y})=d(\bm{y},\bm{x}) $
            \item (Triangle inequality): $ d(\bm{x},\bm{z})\leqslant
                      d(\bm{x},\bm{y})+d(\bm{y},\bm{z}) $
        \end{enumerate}
    \end{theorem} \end{thmbox}
\begin{proof}
    (1)-(3) are trivially true.

    (4) Let $ \bm{x},\bm{y},\bm{z}\in A^n $. Suppose that $ \bm{x} $
    and $ \bm{z} $ differ in exactly $ a $ positions; that is, $ d(\bm{x},\bm{z})=a $.
    Out of the $ a $ positions in which $ \bm{x} $ and $ \bm{z} $ differ,
    there are $ b $ positions in which $ \bm{y} $ is identical to
    $ \bm{x} $, but not $ \bm{z} $. Out of the $ a $ positions,
    there are $ a-b $ positions in which $ \bm{y} $ is identical to
    $ \bm{z} $, but not $ \bm{x} $. Lastly, in the $ n-a $ positions
    where $ \bm{x} $ is identical to $ \bm{z} $, there are $ c $
    positions in which $ \bm{y} $ does not match either
    $ \bm{x} $ or $ \bm{z} $. We can see that
    $ d(\bm{x},\bm{y})=b+c $ and $ d(\bm{y},\bm{z})=a-b+c $. We get
    \[ d(\bm{x},\bm{y})+d(\bm{y},\bm{z})=(b+c) + (a-b+c)=a+2c\geqslant a \]
    Therefore $ d $ is a metric.
\end{proof}

\begin{defbox}
    \begin{definition}
        The \textbf{rate} (or \textbf{information rate}) of an $ [n,M] $-code $ C $ over
        $ A $, is
        \[ R=\frac{\log_q(M)}{n} \]
        where $ q=|A| $.

        If the source messages are all $ k $-tuples over $ A $, then $ M=q^k $, so we have
        \[ R=\frac{\log_q(q^k)}{n}=\frac{k}{n}  \]
    \end{definition} \end{defbox}

\begin{exbox}
    \begin{example}[Rate \& Distance of Code]
        Let $ A=\{0,1\} $ and $ C=\{00000,\,11100,\,00111,\,10101\} $ which is a
        $ [2,4] $-code over $ \{0,1\} $.
        \begin{itemize}
            \item Rate of code: $ R=\sfrac{2}{5} $
            \item Distance of code: $ d(C)=2 $, since the minimum distance
                  are from the pair of codewords $ 00111 $ and $ 10101 $ which
                  have Hamming distance of $ 2 $ as they differ in coordinate
                  positions $ 1 $ and $ 4 $.
        \end{itemize}
    \end{example}
\end{exbox}
