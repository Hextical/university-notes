\section{2020-01-08}
\subsection{Chapter 1: Introduction and Fundamentals}
Message source
$ \rightarrow $
$ \underbrace{
 \underset{\text{(binary strings)}}{\text{Source encoder}} 
 \rightarrow 
 \underset{\text{(adds redundancy to message)}}{\text{Channel encoder}} 
 \underset{\overset{\Big\uparrow}{\text{noise}}}{\overset{\text{channel}}{\rightarrow}} 
\text{Channel decoder}}_{\text{this course}}$
$ \rightarrow \text{Source Decoder} \rightarrow \text{Message} $

\begin{defbox}
    \subsection{Definition (Alphabet)}
    An \emph{alphabet} $ A $ is a finite set of $ q\ge 2 $ symbols.
\end{defbox}

Since we will be using the alphabet
$ A=\{0,1\} $ very often, we make the following definition.

\begin{defbox}
    \subsection{Definition (Binary Alphabet)}
    The alphabet $ A=\{0,1\} $ is a \emph{binary alphabet}.
\end{defbox}

\begin{defbox}
    \subsection{Definition (Word, Tuples, Vectors)}
    A \emph{word} is a finite sequence of symbols from $ A $ (\emph{tuples},
    or \emph{vectors}). We use the terms \emph{vector} and \emph{word}
    interchangeably for $ n $-tuple.
\end{defbox}

\begin{defbox}
    \subsection{Definition (Length)}
    The \emph{length} of a word is the number of symbols in it.
\end{defbox}

\begin{defbox}
    \subsection{Definition (Code)}
    A \emph{code} $ C $ over $ A $ is a finite set of words over $ A $.
    We define $ |C|\ge 2 $.
\end{defbox}

\begin{defbox}
    \subsection{Definition (codeword)}
    A \emph{codeword} is a word in $ C $.
\end{defbox}

\begin{defbox}
    \subsection{Definition (Block code)}
    A \emph{block code} is a code where all codewords have the same length.
    A block code $ C $ of length $ n $ containing $ M $ codewords over $ A $
    is a subset $ C\subseteq A^n $, with $ |C|=M $. We refer to such a block
    code as an $ [n,M] $-code over $ A $.
\end{defbox}

\begin{exbox}
    \subsection{Example (Block code)}
    
    \begin{tabular}{| *{1}{>{\centering\arraybackslash}p{6cm} |}}
        \hline
        Message $ \rightarrow $ Codeword\\
        \hline
        $ 00\rightarrow 00000 $\\
        $ 10\rightarrow 10110 $\\
        $ 01\rightarrow 01011 $\\
        $ 11\rightarrow 11101 $\\
        \hline
    \end{tabular}

    The alphabet is $ A=\{0,1\} $. We have a $ 5 $-tuple since the length
    of each word is $ n=5 $. The code is $ C=\{00000,\,10110,\,01011,\,11101\} $.
    Each element in $ C $ is a codeword, thus there are a total of $ 4 $ codewords.
\end{exbox}

\begin{exbox}
    \subsection{Example (Block code)}
    $ A=\{0,1\} $, $ C=\{00000,\,11100,\,00111,\,10101\} $ is a $ [5,4] $-code over
    $ \{0,1\} $.
    
    \begin{tabular}{| *{1}{>{\centering\arraybackslash}p{6cm} |}}
        \hline
        Messages $ \rightarrow $ codewords\\
        \hline
        $ 00\rightarrow 00000 $\\
        $ 10\rightarrow 11100 $\\
        $ 01\rightarrow 00111 $\\
        $ 11\rightarrow 10101 $\\
        \hline
    \end{tabular}
    
    This is an $ n $-coding (1-1) map.
\end{exbox}

- The channel encoder transmits only codewords. But, what's received by
the channel decoder might not be a codeword.

\subsection{Example}
Suppose the channel decoder receives $ r=11001 $. What should it do?

\subsection{Assumptions about the communications channel}
\begin{enumerate}[1)]
    \item Channels only transmit symbols from $ A $
    \item No symbols are deleted, added, or transposed
    \item (Errors are "random")
\end{enumerate}

\subsection{Example (Binary symmetric channel, BSC)}
$ q=2 $ ($ 0 $ or $ 1 $)

Suppose that the symbols transmitted are $ X_1,\,X_2,\,X_3,\,\ldots $,
and the symbols received are $ Y_1,\,Y_2,\,Y_3,\,\ldots $. Then for all
$ i\ge 1 $, and all $ i\le j,\,k\le q $,
\[ P_r(Y_i=a_j\mid X_i=a_k)=
\begin{cases}
    1-p,\,\text{if } j=k\\
    \frac{p}{q-1},\,\text{if } j\neq k
\end{cases} \]
Here, $ p $ is the symbol error probability.

\subsection{Notes about BSC}
\begin{enumerate}[(i)]
    \item if $ p=0 $, the channel is perfect
    \item if $ p=\nicefrac{1}{2} $, the channel is useless
    \item if $ \nicefrac{1}{2}<p\le 1 $, then simply flip all bits that aren't received
    \item WLOG, we'll assume that $ 0<p<\nicefrac{1}{2} $
    \item Analogously, for any $ q $-ary channel, we can assume that $ 0<p<\frac{q-1}{q} $
\end{enumerate}

\begin{defbox}
    \subsection{Definition (Hamming distance)}
    If $ x,y\in A^n $, the \emph{Hamming distance}, $ d(x,y) $ is
    the \# of coordinate positions in which $ x $ and $ y $ differ.
\end{defbox}

\begin{exbox}
    \subsection{Example (Hamming distance)}
    The hamming distance of $ 10111 $ and $ 01010 $ is 
    \[ d(10111,01010)=4 \]
\end{exbox}

\begin{defbox}
    \subsection{Definition (Hamming distance of a code)}
    Let $ C $ be an $ [n,M] $-code.
    The \emph{Hamming distance $ d $ of a code} $ C $ is
    \[ d(C)=\min \{d(x,y):x,y\in C,\,x\neq y\} \]
\end{defbox}

\begin{thmbox}
    \subsection{Theorem}
    $ d $ is a \emph{metric}. For all $ x,y,z\in A^n $:
    \begin{enumerate}[(i)]
        \item $ d(x,y)\ge 0 $, and $ d(x,y)=0 $ if and only if $ x=y $
        \item $ d(x,y)=d(y,x) $
        \item ($ \triangle $ inequality): $ d(x,z)\le d(x,y)+d(y,z) $
    \end{enumerate}
\end{thmbox}

\begin{defbox}
    \subsection{Definition (Rate)}
    The \emph{rate} (or \emph{information rate}) of an $ [n,M] $-code $ C $ over
    $ A $, is
    \[ R=\frac{\log_q(M)}{n} \]
    where $ q=|A| $.

    If the source messages are all $ k $-tuples over $ A $, then
    \[ R=\frac{\log_q(q^k)}{n}=\frac{k}{n}  \]
    that is, there are $ q^k $ source messages.
\end{defbox}

\begin{exbox}
    \subsection{Example (Rate)}
    $ A=\{0,1\} $, $ C=\{00000,\,11100,\,00111,\,10101\} $.
    
    We have a $ [2,4] $-code over $ \{0,1\} $.
    
    $ R=\frac{2}{5} $, and $ d(C)=2 $ since $ 00111 $ and $ 10101 $
    differ by $ 2 $ in the first and fourth bit.
\end{exbox}