\makeheading{ 2020-01-10 }

\section{Decoding Strategy}
Suppose we have an $ [n,M] $-code $ C $ over $ A $ of distance $ d $.
We need to adopt a strategy for the channel decoder (henceforth called the decoder).
If decoder receives an $ n $-tuple $ \symbf{r}\in A^n $, it must make some decision.
This decision may be one of
\begin{enumerate}[label=(\roman*)]
    \item No errors have occurred; accept $ \symbf{r} $ as a codeword.
    \item Errors have occurred; correct $ \symbf{r} $ to a codeword $ \symbf{c} $;
          e.g., $ 0 \rightarrow 0000,\,1 \rightarrow 1111,\,r=0001 $ corrected to
          $ 0000 $.
    \item Errors have occurred; no correction is possible.
\end{enumerate}

\section{Nearest Neighbour Decoding}
\subsection*{Incomplete Maximum Likelihood Decoding (IMLD)}
Correct $ \symbf{r} $ to the unique codeword $ \symbf{c} $ for which
$ d(\symbf{r},\symbf{c}) $ is smallest.
If $ \symbf{c} $ is not unique, reject $ \symbf{r} $.
\subsection*{Complete Maximum Likelihood Decoding (CMLD)}
Same as IMLD, except ties are broken arbitrarily.

\textbf{Question}: Is IMLD a reasonable strategy?

\begin{Theorem}{}{imld}
    IMLD selects the codeword $ \symbf{c} $ that maximizes $ \Prob{\symbf{r}\given \symbf{c}} $; that is,
    it maximizes the probability $ \symbf{r} $ is received, given $ \symbf{c} $ was sent.
\end{Theorem}

We actually want to maximize $ \Prob{\symbf{r}\given \symbf{c}} $, but we will ignore that for now.

\begin{Proof}{\Cref{thm:imld}}{}
    Suppose $ \symbf{c}_1,\symbf{c}_2\in C $ with $ d(\symbf{c}_1,\symbf{r})=d_1 $
    and $ d(\symbf{c}_2,\symbf{r})=d_2 $.
    Suppose $ d_1>d_2 $. Now,

    $ \Prob{\symbf{r}\given \symbf{c}_1}=(1-p)^{n-d_1}\biggl(\dfrac{p}{q-1} \biggr)^{d_1} $ and
    $ \Prob{\symbf{r}\given \symbf{c}_2}=(1-p)^{n-d_2}\biggl( \dfrac{p}{q-1} \biggr)^{d_2} $.

    Hence,
    \begin{align*}
        \frac{\Prob{\symbf{r}\given \symbf{c}_1}}{\Prob{\symbf{r}\given \symbf{c}_2}}
         & =(1-p)^{d_2-d_1}\biggl(\frac{p}{q-1}  \biggr)^{d_1-d_2} \\
         & =\biggl[ \frac{p}{(1-p)(q-1)}  \biggr]^{d_1-d_2}
    \end{align*}
    Recall that, for a $ q $-ary channel, we can assume that $ p<\frac{q-1}{q} $. Thus,
    \begin{align*}
         & \implies pq < q-1                 \\
         & \implies 0 < q-1-pq               \\
         & \implies p < q-1-pq+p             \\
         & \implies p < (1-p)(q-1)           \\
         & \implies \frac{p}{(1-p)(q-1)} < 1
    \end{align*}
    Since $ d_1>d_2 $, we get $ \dfrac{\Prob{\symbf{r}\given \symbf{c}_1}}{\Prob{\symbf{r}\given \symbf{c}_2}} < 1 $, and so
    $ \Prob{\symbf{r}\given \symbf{c}_1}<\Prob{\symbf{r}\given \symbf{c}_2} $.
\end{Proof}
The ideal strategy is to correct $ \symbf{r} $ to $ \symbf{c}\in C $ such that
$ \Prob{\symbf{c}\given \symbf{r}} $ is maximized. This is \textbf{Minimum Error Decoding (MED)}.


\begin{Example}{IMLD $ \neq $ MED}{}
    Let $ C=\set{\underbrace{000}_{\symbf{c}_1},\underbrace{111}_{\symbf{c}_2}},\,
        \Prob{\symbf{c}_1}=0.1,\, \Prob{\symbf{c}_2}=0.9,\, p=1/4 $, and $ \symbf{r}=100 $.

    \textbf{IMLD} $ \symbf{r} $ is decoded to $ \symbf{c}_1 = 000 $.

    \textbf{MED}
    \begin{align*}
        \Prob{\symbf{c}_1\given \symbf{r}}
         & =\frac{\Prob{\symbf{r}\given \symbf{c}_1}\Prob{\symbf{c}_1}}{\Prob{\symbf{r}}} \\
         & =\frac{p(1-p)^2(0.1)}{\Prob{\symbf{r}}}                                        \\
         & =\frac{0.0140625}{\Prob{\symbf{r}}}
    \end{align*}
    \begin{align*}
        \Prob{\symbf{c}_2\given \symbf{r}}
         & =\frac{\Prob{\symbf{r}\given \symbf{c}_2}\Prob{\symbf{c}_2}}{\Prob{\symbf{r}}} \\
         & =\frac{p^2(1-p)(0.9)}{\Prob{\symbf{r}}}                                        \\
         & =\frac{0.0421875}{\Prob{\symbf{r}}}
    \end{align*}
    Since $ \Prob{\symbf{c}_1\given \symbf{r}}<\Prob{\symbf{c}_2\given \symbf{r}} $,
    $ \symbf{r} $ is decoded to $ \symbf{c}_2=111 $.
\end{Example}


\textbf{Notes}:
\begin{enumerate}[label=(\roman*)]
    \item IMLD selects $ \symbf{c} $ such that $ \Prob{\symbf{r}\given \symbf{c}} $ is maximum.
    \item MED selects $ \symbf{c} $ such that $ \Prob{\symbf{c}\given \symbf{r}} $ is maximum.
    \item MED has a drawback that it requires knowledge of $ \Prob{\symbf{c}_i} $ for
          each $ i\in [1,M] $.
    \item Suppose source messages are equally likely, so
          $ \Prob{\symbf{c}_i}=\frac{1}{M} $ for each $ i\in[1,M] $.
          Then,
          \[ \Prob{\symbf{r}\given \symbf{c}_i}=\frac{\Prob{\symbf{c}_i\given \symbf{r}}\Prob{\symbf{r}}}{\Prob{\symbf{c}_i}}
              =\Prob{\symbf{c}_i\given \symbf{r}} \underbrace{M \Prob{\symbf{r}}}_{\text{constant}}\]
          So, maximizing $ \Prob{\symbf{r}\given \symbf{c}_i} $ is the same as maximizing
          $ \Prob{\symbf{c}_i\given \symbf{r}} $. Thus, IMLD is the same as MED in this case.
\end{enumerate}
In the remainder of the course, we will use IMLD/CMLD\@.
