\section{2020-04-02: The Big Picture--Take 2}
\underline{Roadmap}
\begin{enumerate}[(i)]
    \item The big picture
    \item Two examples
\end{enumerate}
\underline{Example 1}: Check whether a die is fair
\begin{itemize}
    \item $ \theta_i=P(i^{\text{th}} \text{ face}) $ where $ i=1,\ldots ,6 $
    \item $ H_0 $: $ \theta_1=\theta_2=\cdots=\theta_6=\frac{1}{6} $
    \item $ \bm{\theta}=(\theta_1,\ldots ,\theta_6) $
\end{itemize}
If $ H_0 $ was true, then the expected frequency would be close
to the observed frequency.
\[
    \begin{array}{c|c|c|}
          & \text{Observed Frequency} & \text{Expected Frequency} \\
        \hline
        1 & 48                        & 50                        \\
        2 & 72                        & 50                        \\
        3 & 60                        & 50                        \\
        4 & 40                        & 50                        \\
        5 & 40                        & 50                        \\
        6 & 40                        & 50
    \end{array}
\]
The question we want to answer is how close is close enough?

\underline{Example 2}: $ W_1,\ldots ,W_n \thicksim Poi(\mu) $. $ H_0 $: $ W_i \thicksim Poi(\mu) $.
\[
    \begin{array}{c|c|c|}
                    & \text{Observed Frequency} & \text{Expected Frequency} \\
        \hline
        0           & y_0                       & e_0                       \\
        1           & y_1                       & e_1                       \\
        2           & y_2                       & e_2                       \\
        3           & y_3                       & e_3                       \\
        \geqslant 4 & y_4                       & e_4
    \end{array}
\]
where
\[ e_i=n\times \frac{e^{-\hat{\mu}}\hat{\mu}^i}{i!}  \]
\underline{Multinomial}
\begin{itemize}
    \item Extension to the Binomial
    \item Distribution function
    \item Likelihood function
    \item MLE
    \item LRTS
\end{itemize}
Distribution function and likelihood function:
\[ \frac{n!}{x_1!\cdots x_k!} \theta_1^{x_1}\cdots\theta_k^{x_k} \]
where $ x_1+\cdots+x_k=n $.

The MLE is
\[ \hat{\theta}_i=\frac{x_i}{n} \]
for each $ i\in[1,k] $.

LRTS: If $ n $ is large, we can construct a LRTS to test $ H_0 $.
\[ \Lambda(\theta) = -2\ln \left[ \frac{L(\theta)}{L(\tilde{\theta})} \right] \]
The particular form is,
\[ \Lambda = 2 \sum\limits_{i=1}^{n} \left[ Y_i \ln \left( \frac{Y_i}{E_i} \right) \right] \thicksim \chi^2_{k-\ell-1} \]
where
\begin{itemize}
    \item $ Y_i $ is the observed frequency,
    \item $ E_i $ is the expected frequency if $ H_0 $ was true,
    \item $ k $ is the number of categories, and
    \item $ \ell $ is the number of components of $ \theta $ we need to estimate under $ H_0 $.
\end{itemize}
\begin{exbox}
    \begin{example} $ H_0 $: $ \theta_1=\cdots=\theta_6=\frac{1}{6} $.
        \[
            \begin{array}{c|c|c|}
                  & \text{Observed Frequency} & \text{Expected Frequency} \\
                \hline
                1 & 48                        & 50                        \\
                2 & 72                        & 50                        \\
                3 & 60                        & 50                        \\
                4 & 40                        & 50                        \\
                5 & 40                        & 50                        \\
                6 & 40                        & 50
            \end{array}
        \]
        Calculate the $ p $-value.

        \textbf{Solution.}
        \[ \lambda=2 \sum\limits_{i=1}^{6} \left[ y_i \ln\left( \frac{y_i}{e_i}  \right) \right] \]
        Then, let $ n $ the number of categories and $ k $ be the number of parameters we estimate under $ H_0 $.
        So the degrees of freedom in our case is $ 6-k-1=5 $ where $ k=0 $ since we are given
        all of the $ \theta_i $'s.
        \begin{align*}
            p\text{-value}
             & = P(\Lambda\geqslant \lambda)  \\
             & = P(\chi^2_5\geqslant \lambda)
        \end{align*}
    \end{example}
\end{exbox}
\begin{remark}
    In the example we have different letters for the degrees of freedom compared to our derivation
    to match the course notes.
\end{remark}
