\section{2020-01-29}
\underline{Roadmap}:
\begin{itemize}
    \item 5 min recap
    \item Likelihood and the MLE for continuous distributions
    \item Invariance property of the MLE
    \item Parameter, Estimate, and Estimator
\end{itemize}

\begin{Definition}{}{}
    In many applications, the data $ \symbf{Y}=(Y_1,\ldots ,Y_n) $ are independent
    and identically distributed (iid) random variables each with probability function
    $ f(y;\theta) $ for $ \theta\in\Omega $. We refer to $ \symbf{Y} $
    as a random sample from the distribution $ f(y;\theta) $. In this case,
    the observed data are $ \symbf{y}=(y_1,\ldots ,y_n) $ and
    \[ L(\theta)=L(\theta;\symbf{y})=\prod_{i=1}^n f(y_i;\theta) \]
    for $ \theta\in\Omega $. Recall that if $ Y_1,\ldots ,Y_n $ are
    independent random variables, then their joint probability function is
    the product of their individual probability functions.
\end{Definition}



\begin{Proposition}{}{}
    Suppose the data $ \symbf{y}=(y_1,\ldots ,y_n) $ is independently
    drawn from a $ \poi(\theta) $ distribution, where $ \theta $ is unknown.
    The maximum likelihood estimate for $ \theta $ is given by
    \[ \hat{\theta}=\bar{y} \]
\end{Proposition}

\begin{Proof}{}{}
    The likelihood function is
    \begin{align*}
        L(\theta) & =
        \prod_{i=1}^n f(y_i;\theta)                                     \\
                  & =\prod_{i=1}^n \frac{\theta^{y_i}e^{-\theta}}{y_i!} \\
                  & =\left( \prod_{i=1}^n  \frac{1}{y_i!} \right)
        \theta^{\sum\limits_{i=1}^{n} y_i}e^{-n\theta}
    \end{align*}
    or more simply
    \[ L(\theta)=\theta^{n\bar{y}}e^{-n\theta} \]
    for $ \theta\geqslant 0 $. The log likelihood function is
    \[ \ell(\theta)=n\left[ \bar{y}\ln(\theta)-\theta \right] \]
    for $ \theta>0 $.
    \[ \frac{d\ell}{d\theta} =n\left( \frac{\bar{y}}{\theta}-1 \right)=\frac{n}{\theta}
        \left( \bar{y}-\theta \right):=0 \]
    \[ \implies \hat{\theta}=\bar{y} \]
\end{Proof}


\begin{Example}{}{}
    \begin{itemize}
        \item $ \mu= $ average time between two volcanic eruptions
        \item $ \symbf{y}=(y_1,\ldots ,y_n) $
        \item $ y_i= $ waiting time for the $ i^{\text{th}} $ eruption
    \end{itemize}
    \underline{Model}: $ Y_i \sim \exponential(\theta) $ iid
\end{Example}



\begin{Definition}{}{}
    If $ \symbf{y}=(y_1,\ldots ,y_n) $ are the observed values of a random sample from a distribution with
    probability distribution function $ f(y;\theta) $, then the \textbf{\emph{likelihood function}}
    is defined as
    \[ L(\theta)=L(\theta;\symbf{y})=\prod_{i=1}^n f(y_i;\theta) \]
    for $ \theta\in\Omega $.
\end{Definition}



\begin{Proposition}{}{}
    Suppose the data $ \symbf{y}=(y_1,\ldots ,y_n) $ is independently
    drawn from a $ \exponential(\theta) $ distribution, where $ \theta $ is unknown.
    The maximum likelihood estimate for $ \theta $ is given by
    \[ \hat{\theta}=\bar{y} \]
\end{Proposition}

\begin{Proof}{}{}
    The likelihood function is
    \begin{align*}
        L(\theta)
         & =\prod_{i=1}^n \frac{1}{\theta} e^{-y_i/\theta}                        \\
         & =\frac{1}{\theta^n} \exp\left(-\sum\limits_{i=1}^{n} y_i/\theta\right) \\
         & =\theta^{-n}e^{-n\bar{y}/\theta}
    \end{align*}
    for $ \theta>0 $. The log likelihood function is
    \[ \ell(\theta)=-n\left( \ln(\theta)+\frac{\bar{y}}{\theta} \right) \]
    for $ \theta>0 $.
    \[ \frac{d\ell}{d\theta} =-n\left( \frac{1}{\theta} -\frac{\bar{y}}{\theta^2} \right)=
        \frac{n}{\theta^2} \left( \bar{y}-\theta \right):=0 \]
    \[ \implies \hat{\theta}=\bar{y} \]
\end{Proof}


\begin{Example}{}{}
    \begin{itemize}
        \item $ \mu = $ average score in STAT 231
        \item $ \sigma^2= $ variance in STAT 231 scores
        \item $ \symbf{y}=(y_1,\ldots ,y_n) $
        \item $ y_i= $ STAT 231 score of the $ i^{\text{th}} $ student
    \end{itemize}
    \underline{Model}: $ Y_i \sim N(\mu,\sigma^2) $ iid
\end{Example}



\begin{Proposition}{}{}
    Suppose the data $ \symbf{y}=(y_1,\ldots ,y_n) $ is independently
    drawn from a $ N(\mu,\sigma^2) $ distribution,
    where $ \mu $ and $ \sigma $ are unknown. The maximum
    likelihood estimate for the pair $ (\mu,\sigma^2) $ is given by
    \[ \hat{\mu}=\bar{y}, \]
    \[ \hat{\sigma}^2=\frac{1}{n} \sum\limits_{i=1}^{n} \left( y_i-\bar{y} \right)^2 \]
\end{Proposition}



\begin{Theorem}{}{}
    If $ \hat{\symbf{\theta}}=(\hat{\theta}_1,\ldots ,\hat{\theta}_k) $ is the maximum likelihood
    estimate of $ \symbf{\theta}=(\theta_1,\ldots ,\theta_k) $, then
    $ g(\symbf{\hat{\theta}}) $ is the maximum likelihood estimate of $ g(\symbf{\theta}) $.
\end{Theorem}



\begin{Example}{}{}
    Suppose $ Y_1,\ldots ,Y_{25} \sim \poi(\mu) $ with $ \bar{y}=5 $.
    Find the MLE for $ P(Y=1) $.

    \textbf{Solution.}
    \[ P(Y=1)=\frac{e^{-\mu}\mu^y}{y!}=\frac{e^{-5}5^1}{1!}=\frac{5}{e^5} \]
\end{Example}

