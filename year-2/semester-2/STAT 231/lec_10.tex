\section{2020-01-27}
\subsection*{Roadmap}
\begin{itemize}
    \item Statistical Models.
    \item Likelihood and the MLE for discrete:
          \begin{itemize}
              \item Binomial.
              \item Poisson.
              \item Geometric.
          \end{itemize}
    \item Invariance property of the MLE.
    \item Relative likelihood function.
\end{itemize}

\begin{Definition}{}{}
    The \textbf{\emph{relative likelihood function}} is defined as
    \[ R(\theta)=\frac{\mathcal{L}(\theta)}{\mathcal{L}(\hat{\theta})} \]
    for $ \theta\in\Omega $. Note that $ 0\leqslant R(\theta)\leqslant 1 $
    for all $ \theta\in\Omega $.
\end{Definition}


\begin{Definition}{}{}
    The \textbf{\emph{log likelihood function}} is defined as
    \[ \ell(\theta)=\ln\bigl[\mathcal{L}(\theta) \bigr] \]
    for $ \theta\in\Omega $.
\end{Definition}

$ \dagger $ Why does maximizing $ \ell(\theta) $ also maximize $ \mathcal{L}(\theta) $?
Answer: $ \ln(\cdot) $ is an increasing function, in fact it will work for all increasing functions.

Let $ g:\mathbb{R}\to\mathbb{R} $ be a strictly increasing monotonic function;
that is $ t>s \iff g(t)>g(s) $. Suppose $ f(\hat{x}) $ is maximum for $ \hat{x} $.
That means $ f(\hat{x})>f(x) $ for all $ x $. Thus,
\[ g(f(\hat{x}))>g(f(x)) \]
Let $ t=f(\hat{x}) $ and $ s=f(x) $. The result now follows.


\begin{Proposition}{}{max_mle}
    If $ Y \sim \bin{n,\theta} $ with $ y $ successes, then the
    maximum likelihood estimate for $ \theta $ is given by
    \[ \hat{\theta}=\frac{y}{n} \]
\end{Proposition}

\begin{Proof}{\Cref{prop:max_mle}}{}
    If $ y=0 $, then
    \[ \mathcal{L}(\theta)=P(Y=0;\theta)=\binom{n}{0}\theta^0(1-\theta)^n=(1-\theta)^{n-0} \]
    for $ 0\leqslant \theta \leqslant 1 $. $ \mathcal{L}(\theta) $ is a decreasing function
    for $ \theta\in[0,1] $ and its maximum on the interval $ [0,1] $
    occurs at the endpoint $ \theta=0 $ and so $ \hat{\theta}=0=\frac{0}{n} $.

    If $ y=n $, then
    \[ \mathcal{L}(\theta)=P(Y=n;\theta)=\binom{n}{n}\theta^n(1-\theta)^{n-n}=\theta^n \]
    for $ 0\leqslant \theta \leqslant 1 $. $ \mathcal{L}(\theta) $ is an increasing function
    for $ \theta\in[0,1] $ and its maximum on the interval $ [0,1] $
    occurs at the endpoint $ \theta=1 $ and so $ \hat{\theta}=1=\frac{n}{n} $.

    If $ y\neq 0 $ and $ y\neq n $, then
    \[ \mathcal{L}(\theta)=P(Y=y;\theta)=\binom{n}{y}\theta^y(1-\theta)^{n-y} \]
    for $ 0\leqslant \theta\leqslant 1 $. Then,
    \[ \ell(\theta)=\ln\left[ \binom{n}{y} \right]+y\ln(\theta)+(n-y)\ln(1-\theta) \]
    for $ 0<\theta<1 $.
    \[ \frac{d\ell}{d\theta}=\frac{y}{\theta}-\frac{n-y}{1-\theta}=\frac{y-n\theta}{\theta(1-\theta)}:=0  \]
    \[ \implies \hat{\theta}=\frac{y}{n} \]
\end{Proof}

