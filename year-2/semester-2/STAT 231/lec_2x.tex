\section{Gaussian Response Models}
\subsection{Intro}

\begin{Example}{STAT 230 and 231 Final Grades}{} $ \; $

    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            \text{No.} & \text{S230} & \text{S231} \\
            \hline
            1          & 76          & 76          \\
            2          & 77          & 79          \\
            3          & 57          & 54          \\
            4          & 75          & 64          \\
            5          & 74          & 64          \\
            6          & 60          & 60          \\
            7          & 81          & 85          \\
            8          & 86          & 82          \\
            9          & 96          & 88          \\
            10         & 79          & 72          \\
            \hline
        \end{tabular} $ \qquad $
        \begin{tabular}{|c|c|c|}
            \hline
            \text{No.} & \text{S230} & \text{S231} \\
            \hline
            11         & 87          & 76          \\
            12         & 71          & 50          \\
            13         & 63          & 75          \\
            14         & 77          & 72          \\
            15         & 96          & 84          \\
            16         & 65          & 69          \\
            17         & 71          & 43          \\
            18         & 66          & 60          \\
            19         & 90          & 96          \\
            20         & 50          & 50          \\
            \hline
        \end{tabular} $ \qquad $
        \begin{tabular}{|c|c|c|}
            \hline
            \text{No.} & \text{S230} & \text{S231} \\
            \hline
            21         & 98          & 83          \\
            22         & 80          & 88          \\
            23         & 67          & 52          \\
            24         & 78          & 75          \\
            25         & 100         & 99          \\
            26         & 94          & 94          \\
            27         & 83          & 83          \\
            28         & 51          & 37          \\
            29         & 77          & 90          \\
            30         & 77          & 67          \\
            \hline
        \end{tabular}
    \end{center}

    \begin{itemize}
        \item Why might we be interested in collecting data such as these?
        \item What might be a reasonable choice for the target and study population?
        \item What are the variates? What type are they?
        \item What is the explanatory variate? What is the response variate?
        \item How do we summarize these data numerically and graphically?
        \item What model could we use to analyse these data?
    \end{itemize}

\end{Example}


\subsection{Sample Correlation}
Recall that the sample correlation is a numerical measure of the linear relationship between two variates.
It is defined as
\[r=\frac{S_{x y}}{\sqrt{S_{x x} S_{y y}}}\]
\[S_{x x}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\sum_{i=1}^{n} x_{i}^{2}-n(\bar{x})^{2}\]
\[S_{x y}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)=\sum_{i=1}^{n} x_{i} y_{i}-n \bar{x} \bar{y}\]
\[S_{y y}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}=\sum_{i=1}^{n} y_{i}^{2}-n(\bar{y})^{2}\]
Recall that $ -1\leqslant r\leqslant 1 $

\begin{Example}{Sample Correlation for STAT 230/231 Final Grades}{}
    Let $ x $ be the STAT 230 final grade, and $ y $ be the STAT 231 final grade.

    For these data, we have
    \[S_{x x}=5135.8667 \quad S_{x y}=5106.8667 \quad S_{y y}=7585.3667\]
    Thus,
    \[r=\frac{5106.8667}{\sqrt{(5135.8667)(7585.3667)}}=0.82\]
    Since $ r $ is close to $ 1 $, we would say that there is a strong positive linear
    relationship between STAT 230 and STAT 231 final grades.
\end{Example}


\subsection{Least Squares Estimates}
\underline{Fitting a Straight Line: Least Squares Approach}

To determine the fitted line $ y=\alpha+\beta x $, which minimizes the sum of the squares
of the distances between the observed points and the fitted line.

We need to find the values of $ \alpha $ and $ \beta $ which minimize
\[g(\alpha, \beta)=\sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}\]
These values are determined by simultaneously solving the equations
\[
    \begin{aligned}
        \frac{\partial g}{\partial \alpha} & =\frac{\partial}{\partial \alpha} \sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}=\sum_{i=1}^{n} 2\left(y_{i}-\alpha-\beta x_{i}\right)(-1)=0               \\
        \frac{\partial g}{\partial \beta}  & =\frac{\partial}{\partial \beta} \sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}=\sum_{i=1}^{n} 2\left(y_{i}-\alpha-\beta x_{i}\right)\left(-x_{i}\right)=0
    \end{aligned}
\]
These equations can be written as
\begin{equation}\tag{1}
    \bar{y}-\alpha-\beta \bar{x}=0
\end{equation}
\begin{equation}\tag{2}
    \sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)\left(x_{i}\right)=0
\end{equation}
From equation (1), we obtain $ \alpha=\bar{y}-\beta \bar{x} $ which
we can substitute into equation (2) to obtain
\[\sum_{i=1}^{n} x_{i}\left[y_{i}-\bar{y}-\beta\left(x_{i}-\bar{x}\right)\right]=0\]
or
\[\beta=
    \frac{\sum\limits_{i=1}^{n} x_{i}\left(y_{i}-\bar{y}\right)}
    {\sum\limits_{i=1}^{n} x_{i}\left(x_{i}-\bar{x}\right)}
    =\frac{\sum\limits_{i=1}^{n}\left(x_{i}-\bar{x}\right) y_{i}}
    {\sum\limits_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
    =\frac{\sum\limits_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}
    {\sum\limits_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
    =\frac{S_{x y}}{S_{x x}}\]
Therefore, the least squares estimates are
\[\alpha=\hat{\alpha}=\bar{y}-\hat{\beta} \bar{x}
    \quad \text{and} \quad \beta=\hat{\beta}=\frac{S_{x y}}{S_{x x}}\]
And, the equation of the fitted line is
\[y=\hat{\alpha}+\hat{\beta} x\]

\subsection{STAT 231 Versus STAT 230 Final Grades}


\begin{Example}{Fitted Line for STAT 230/231 Final Grades}{}
    For the STAT 230/231 data, we have the following
    \[\bar{x}=\frac{2302}{30}=76.7333 \quad \bar{y}=\frac{2167}{30}=72.2333\]
    \[S_{x x}=5135.8667 \quad S_{x y}=5106.8667 \quad S_{y y}=7585.3667\]
    \[\hat{\beta}=\frac{S_{x y}}{S_{x x}}=\frac{5106.8667}{5135.8667}=0.9944\]
    \[\hat{\alpha}=\bar{y}-\hat{\beta} \bar{x}=72.2333-(0.9944)(76.7333)=-4.0667\]
    The fitted line is
    \[y=-4.0667+0.9944 x\]
\end{Example}


\underline{Predicting STAT 231 Final Grade}

Given your STAT 230 Final Grade what is the best estimate of your STAT 231 final grade based on
these data?

If your final grade in STAT 230 was $ x=75 $, then the least squares estimate of your STAT 231
final grade is
\[\hat{y}=-4.0667+0.9944(75)=70.51\]
What can we say about the say about the uncertainty in this estimate?

We need a statistical model in order to obtain an interval estimate of your mark,
in particular we would like to construct a confidence interval.

We need a model which captures the fact that not everyone with a final grade of $ 75 $
in STAT 230 gets a final grade of $ 70.51 $ in STAT 231.

\underline{Determining a Model for STAT 230/231 Final Grades}

Let's begin by considering the population of students who obtained a final grade of $ x=75 $
in STAT 230.

Let $ Y = $ STAT 231 final grade of a student drawn at random from this population.

What distribution might we assume for $ Y $?

Since frequency histograms for marks often exhibit a bell shape, we might assume
$ Y\sim G(\mu,\sigma) $ where $ \mu $ represents the mean STAT 231 final grade for students
in the study population who obtained a final grade of $ x=75 $ in STAT 230.

We could then use this model along with the observed data to obtain interval
estimates for the mean $ \mu $.

\underline{Linear Relationship Between STAT 230/231 Final Grades}

In our sample of $ 30 $ students, we only observed one student with a final grade of $ 75 $
in STAT 230.

Does it make sense to do estimation with only one observation?

What to do? We do have $ 29 $ other observations.

Since the other $ 29 $ students had different STAT 230 final grades, they are
observations drawn from populations which have different means (and possibly different variances).

From the scatterplot however, the relationship between STAT 230 and STAT 231
final grades look very linear.


\underline{Linear Relationship Between STAT 230/231 Final Grades: Note}

Note we have assumed that the standard deviation $ \sigma $ does not depend
on $ x_i $.

We will look at ways of assessing whether this assumption is reasonable.

\underline{Model for STAT 230/231 Grades}

It seems reasonable to assume a model in which the mean STAT 231 final grade for
students in the study population who obtained a final grade of $ x $
in STAT 230 takes the form:
\[\mu(x)=\alpha+\beta x\]
For the data $ (x_i,y_i) $, $ i=1,2,\ldots ,n $, we assume the model
\[Y_{i} \sim G\left(\alpha+\beta x_{i}, \sigma\right)\]
for $ i=1,2,\ldots ,n $ independently, where $ x_i $ is assumed to be a known constant.

This model is usually referred to as a \textbf{simple linear regression model}.

\subsection{Simple Linear Regression Model}
In the model, defined by
\[Y_{i} \sim G\left(\alpha+\beta x_{i}, \sigma\right)\]
for $ i=1,\ldots ,n $ independently where $ x_i $ is assumed to be a known constant,
there are three unknown parameters: $ \alpha $, $ \beta $, and $ \sigma $.

For the STAT 230/231 data, the parameter $ \mu(x)=\alpha+\beta x $ represents
the mean STAT 231 final grade in the study population of students with a STAT 230 final
grade equal to $ x $.

\underline{Simple Linear Regression Model: Parameters $ \alpha $ and $ \beta $}

What does the parameter $ \beta $ represent?

The parameter $ \beta $ represents the change in the mean
$ \mu(x)=\alpha+\beta x $ for a unit increase in $ x $.

What does the parameter $ \alpha $ represent?

The parameter $ \alpha $ represents the mean in the study population of students
with a STAT 230 final grade equal to $ 0 $. (Note: In this example, the parameter
$ \alpha $ is not of interest since students with a final grade of $ 0 $ cannot
take STAT 231!)

\subsection{Maximum Likelihood Estimates}

Since our assumed model is
\[Y_{i} \sim G\left(\alpha+\beta x_{i}, \sigma\right), \quad \text{for } i=1, \ldots, n \text{ independently}\]
where $ x_i $ is assumed to be a known constant, the likelihood function for $ \alpha $
and $ \beta $ (assuming for the moment that $ \sigma $ is known) is
\[\begin{aligned}
        \mathcal{L}(\alpha, \beta)
         & =\prod_{i=1}^{n} \exp \left[-\frac{1}{2 \sigma^{2}}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}\right] \\
         & =\exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}\right]
    \end{aligned}
\]
where we have ignored terms not involving $ \alpha $ and $ \beta $.

To obtain the maximum likelihood estimates of $ \alpha $ and $ \beta $,
we would maximize
\[\mathcal{L}(\alpha, \beta)=\exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}\right],
    \alpha \in \mathbb{R}, \beta \in \mathbb{R}\]
with respect to $ \alpha $ and $ \beta $, or equivalently minimize
\[\sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}\]
with respect to $ \alpha $ and $ \beta $.

This is a problem we've already solved!

\underline{Fitting a Straight Line: Maximum Likelihood Approach}

\textbf{Important Result}

For the model
\[Y_{i} \sim G\left(\alpha+\beta x_{i}, \sigma\right),
    \text{for } i=1,2, \ldots, n \text{ independently}\]
where $ x_i $ is assumed to be a known constant, the maximum likelihood estimates of $ \alpha $
and $ \beta $ (usually called the \textbf{regression parameters}) are given by
\[\hat{\alpha}=\bar{y}-\hat{\beta} \bar{x} \quad \text{and} \quad \hat{\beta}=\frac{S_{x y}}{S_{x x}}\]
which are also the least square estimates of $ \alpha $ and $ \beta $.

So if the least squares estimates and the maximum likelihood estimates are the same,
why do we actually assume a probability model? Why complicate things?

We need a probability model to model the variability in the data. Remember,
not every student with a $ 75 $ in STAT 230 obtain the same STAT 231 final grade.

\subsection{Distribution of the Maximum Likelihood Estimator of the Slope, Beta}
In order to construct a confidence interval for your STAT 231 mark, we need to derive
some distributional results.

\textbf{Important Result}

The first of these is
\[\tilde{\beta} \sim G\left(\beta, \frac{\sigma}{\sqrt{S_{x x}}}\right)\]
where
\[\tilde{\beta}=\frac{S_{x y}}{S_{x x}}=\frac{1}{S_{x x}} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right) Y_{i}\]
The distribution of $ \tilde{\beta} $ is determined by the assumption
\[Y_{i} \sim G\left(\alpha+\beta x_{i}, \sigma\right), \quad \text{for } i=1,2, \dots, n \text{ independently}\]
where $ x_i $, $ i=1,2,\ldots ,n $, are assumed to be known constants.

Since
\[\tilde{\beta}=\frac{S_{x y}}{S_{x x}}=\frac{1}{S_{x x}} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right) Y_{i}\]
is a linear combination of independent normal random variables.

Thus, by a theorem learned in STAT 230, $ \tilde{\beta} $ has a normal distribution.

We only need to find $ \E{\tilde{\beta}} $ and $ \Var{\tilde{\beta}} $.
\[\begin{aligned}
        \symbf{\mathrm{E}}\Bigl[\tilde{\beta}\Bigr]
         & =\sum_{i=1}^{n} \frac{\left(x_{i}-\bar{x}\right)}{S_{x x}} \E{Y_{i}}                                                                                                                                                                                \\
         & =\sum_{i=1}^{n} \frac{\left(x_{i}-\bar{x}\right)}{S_{x x}}\left(\alpha+\beta x_{i}\right) & \quad & \text{since} \E{Y_{i}}=\alpha+\beta x_{i}                                                                                                       \\
         & =\beta \sum_{i=1}^{n} \frac{\left(x_{i}-\bar{x}\right)}{S_{x x}} x_{i}                    &       & \text{since} \sum_{i=1}^{n} \frac{\left(x_{i}-\bar{x}\right)}{S_{x x}} \alpha=\frac{\alpha}{S_{x x}} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)=0 \\
         & =\beta \frac{S_{x x}}{S_{x x}}                                                                                                                                                                                                                      \\
         & =\beta
    \end{aligned}
\]
Similarly,
\[\begin{aligned}
        \Var{\tilde{\beta}}
         & =\sum_{i=1}^{n} \frac{\left(x_{i}-\bar{x}\right)^{2}}{\left(S_{x x}\right)^{2}} \Var{Y_{i}}                                                                                                 \\
         & =\sum_{i=1}^{n} \frac{\left(x_{i}-\bar{x}\right)^{2}}{\left(S_{x x}\right)^{2}} \sigma^{2}                                                    & \quad & \text{since} \Var{Y_{i}}=\sigma^{2} \\
         & =\frac{\sigma^{2}}{\left(S_{x x}\right)^{2}} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\frac{\sigma^{2}}{\left(S_{x x}\right)^{2}} S_{x x}                                               \\
         & =\frac{\sigma^{2}}{S_{x x}}
    \end{aligned}\]
Thus,
\[\tilde{\beta} \sim G\left(\beta, \frac{\sigma}{\sqrt{S_{x x}}}\right)\]

\subsection{Distribution of the Variance Estimator}
\underline{Estimate of the Variance in Simple Linear Regression}

Since $ \sigma $ is usually unknown, we estimate it using
\[s_{e}^{2}=\frac{1}{n-2} \sum_{i=1}^{n}\left(y_{i}-\hat{\alpha}-\hat{\beta} x_{i}\right)^{2}
    =\frac{1}{n-2}\left(S_{y y}-\hat{\beta} S_{x y}\right)\]
Note: $\sum\limits_{i=1}^{n}\left(y_{i}-\hat{\alpha}-\hat{\beta} x_{i}\right)^{2}$ is usually called the
\textbf{sum of the squared errors} and $ s_e^2 $, the unbiased estimator,
is called the \textbf{mean squared error}. $ s_e^2 $ is not the maximum likelihood estimate of
$ \sigma^2 $. The maximum likelihood estimate is similar, but with a denominator of $ n $
rather than $ n-2 $.

We use $ S_e^2 $ as the estimator of $ \sigma^2 $ since it can be shown that
$ \E{S_e^2}=\sigma^2 $ where
\[ S_{e}^{2}=\frac{1}{n-2} \sum_{i=1}^{n}\left(Y_{i}-\tilde{\alpha}-\tilde{\beta} x_{i}\right)^{2}
    \quad \tilde{\beta}=\frac{S_{x y}}{S_{x x}}
    \quad \tilde{\alpha}=\bar{Y}-\tilde{\beta} \bar{x} \]

\underline{Distribution of the Estimator of the Variance}

It can be shown that
\[ \frac{(n-2) S_{e}^{2}}{\sigma^{2}}
    =\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(Y_{i}-\tilde{\alpha}-\tilde{\beta} x_{i}\right)^{2}
    \sim \chi^{2}(n-2) \]

The proof is beyond the scope of this course, but it is usually proved in a third year linear
regression course.

Note that there are $ n-2 $ degrees of freedom due to the two restrictions:
\begin{itemize}
    \item $\sum\limits_{i=1}^{n}\left(Y_{i}-\tilde{\alpha}-\tilde{\beta} x_{i}\right)=0$
    \item $\sum\limits_{i=1}^{n}\left(Y_{i}-\tilde{\alpha}-\tilde{\beta} x_{i}\right) x_{i}=0$
\end{itemize}
Recall that these $ 2 $ equations $ 2 $ unknowns determined the estimators
$ \tilde{\alpha} $ and $ \tilde{\beta} $.

\subsection{Constructing the Confidence Interval for $ \beta $}
Since
\[\frac{\tilde{\beta}-\beta}{\sigma / \sqrt{S_{x x}}} \sim G(0,1) \quad \text{and}
    \quad \frac{(n-2) S_{e}^{2}}{\sigma^{2}} \sim \chi^{2}(n-2)\]
independently, it follows that
\[ \frac{\tilde{\beta}-\beta}{S_e / \sqrt{S_{x x}}} \sim t(n-2)\]
This $ t $ pivotal quantity can be used to construct confidence intervals
and tests of hypotheses for $ \beta $.

\underline{$ 100p\% $ Confidence Interval and $ p $-Value for $ \beta $}

A $ 100p\% $ confidence interval for $ \beta $ is given by
\[\hat{\beta} \pm a \frac{s_{e}}{\sqrt{S_{x x}}}\]
where $ P(T\leqslant a)=\frac{(1+p)}{2} $ and $ T\sim t(n-2) $.

For testing $ H_0: \beta=\beta_0 $,
\[p \text{-value }=2\left[1-P\left(T \leq \frac{\left|\hat{\beta}-\beta_{0}\right|}{s_{e} / \sqrt{S_{x x}}}\right)\right], \text{where} T \sim t(n-2)\]

\subsection{Hypothesis of No Relationship}
Since $ \mu(x)=\alpha+\beta x $, a test of $ H_0: \beta=0 $ is a test of the Hypothesis
that the mean $ \mu(x) $ does not depend on $ x $.

This hypothesis is usually referred to as the hypothesis of no relationship
between variates $ Y $ and $ x $.


\begin{Example}{Estimates of $ \beta $ and $ \sigma $ for STAT 230/231 Final Grades}{}
    \[\bar{x}=\frac{2302}{30}=76.7333 \quad \bar{y}=\frac{2167}{30}=72.2333\]
    \[S_{x x}=5135.8667 \quad S_{x y}=5106.8667 \quad S_{y y}=7585.3667\]
    \[\hat{\beta}=\frac{S_{x y}}{S_{x x}}=\frac{5106.8667}{5135.8667}=0.9944\]
    \[s_{e}=\sqrt{\frac{1}{n-2}\left(S_{y y}-\hat{\beta} S_{x y}\right)}
        =\sqrt{\frac{1}{28}[7585.3667-(0.9944)(5106.8667)]}=9.4630\]
\end{Example}


\subsection{Interferences for the Slope}

\begin{Example}{$ 95\% $ Confidence Interval for $ \beta $ for STAT 230/231 Final Grades}{}
    Since
    \[P(T \leq 2.0484)=\frac{1+0.95}{2}=0.975 \quad \text{where} T \sim t(28)\]
    a $ 95\% $ confidence interval for $ \beta $ is
    \[\hat{\beta} \pm 2.0484 \frac{s_{e}}{\sqrt{S_{x x}}}
        =0.9944 \pm 2.0484 \frac{9.4630}{\sqrt{5135.8667}}\]
    or
    \[ \left[ 0.7239,1.2648 \right] \]
\end{Example}



\begin{Example}{Testing $ H_0:\beta=0 $ for STAT 230/231 Final Grades}{}
    Since the $ 95\% $ confidence interval, $ \left[ 0.7239,1.2648 \right] $,
    does not contain the value $ \beta=0 $, the $ p $-value for testing
    $ H_0:\beta=0 $ is smaller than $ 0.05 $.

    This means there is evidence against the hypothesis of no relationship
    between STAT 231 final grades and STAT 230 final grades.
\end{Example}



\begin{Example}{$ p $-Value for STAT 230/231 Final Grades}{}
    The actual $ p $-value for testing $ H_0:\beta=0 $ is
    \[\begin{aligned}
            p\text{-value }
             & =2\left[1-P\left(T \leq \frac{|\hat{\beta}-0|}{s_{e} / \sqrt{S_{x x}}}\right)\right]
            \quad \text{where}T \sim t(28)                                                          \\
             & =2\left[1-P\left(T \leq \frac{0.9944}{9.4630 / \sqrt{5135.8667}}\right)\right]       \\
             & =2[1-P(T \leq 7.5304)]                                                               \\
             & \approx 0
        \end{aligned}\]
    Therefore, there is very strong evidence against the hypothesis of no relationship
    between STAT 230 and STAT 231 final grades.
\end{Example}


What does the hypothesis $ H_0:\beta=1 $ represent?

Recall, that the slope $ \beta $ represents the change in STAT 231 final grade
for a unit change of $ 1 $ mark in the STAT 230 final grade in the study population.

\begin{Example}{Testing $ H_0:\beta=1 $ for STAT 230/231 Final Grades}{}
    Since the $ 95\% $ confidence interval, $ \left[ 0.7239,1.2648 \right] $,
    does contain the value $ \beta=1 $, the $ p $-value for testing
    $ H_0:\beta=1 $ is larger than $ 0.05 $ and there is no evidence against the hypothesis
    $ H_0:\beta=1 $.

    The actual $ p $-value for testing $ H_0:\beta=1 $ is
    \[
        \begin{aligned}
            p\text{-value}
             & =2\left[1-P\left(T \leq \frac{|\hat{\beta}-1|}{s_{e} / \sqrt{S_{x x}}}\right)\right]
            \quad \text{where} T \sim t(28)                                                         \\
             & =2\left[1-P\left(T \leq \frac{|0.9944-1|}{9.4630 / \sqrt{5135.8667}}\right)\right]   \\
             & =2[1-P(T \leq 0.0428)]
        \end{aligned}
    \]
    Since $ P(T\leqslant 0.2558)=0.6 $,
    \[ p\text{-value}\geqslant 2(1-0.6)=0.8 \]
\end{Example}


\subsection{Interferences for the Mean Response at $ x $}
Suppose we wanted a confidence interval for the mean STAT 231 final grade for students
who obtained a final grade of $ 75 $ in STAT 230; that is, we want a confidence
for $ \mu(75)=\alpha+\beta(75) $.

More generally we are often interested in a confidence interval for the mean
response $ \mu(x)=\alpha+\beta x $ for a specified value of $ x $.

The maximum likelihood estimator of $ \mu(x) $ is obtained by replacing the unknown
values of $ \alpha $ and $ \beta $ by their maximum likelihood estimators,
which gives
\[\tilde{\mu}(x)
    =\tilde{\alpha}+\tilde{\beta} x
    =\bar{Y}+\tilde{\beta}(x-\bar{x})\]
since $ \tilde{\alpha}=\bar{Y}-\tilde{\beta}\bar{x} $.

We will now show that
\[
    \begin{aligned}
        \tilde{\mu}(x)
         & =\tilde{\alpha}+\tilde{\beta}x                                      \\
         & =\bar{Y}+\tilde{\beta}\left( x-\bar{x} \right) \sim G\left( \mu(x),
        \sigma \sqrt{\frac{1}{n}+\frac{\left( x-\bar{x} \right)^2}{S_{xx}}} \right)
    \end{aligned}
\]

or
\[\tilde{\alpha}+\tilde{\beta} x \sim G\left(\alpha+\beta x,
    \sigma \sqrt{\frac{1}{n}+\frac{(x-\bar{x})^{2}}{S_{x x}}}\right)\]
We notice that the standard deviation is larger for values
of $ x $ which are not close to the center or mean of the $ x $ data.

\subsection{Distribution of the Estimator of the Mean Response at $ x $}
Recall our model assumption
\[Y_{i} \sim G\left(\alpha+\beta x_{i}, \sigma\right)
    \quad \text{for } i=1, \ldots, n \text{ independently}\]
where $ x_i $ is assumed to be a known constant, and the fact that
\[\tilde{\beta}=\frac{S_{x y}}{S_{x x}}=\sum_{i=1}^{n} \frac{\left(x_{i}-\bar{x}\right)}{S_{x x}} Y_{i}\]
Then,
\[\tilde{\mu}(x)=\bar{Y}+\tilde{\beta}(x-\bar{x})=\frac{1}{n} \sum_{i=1}^{n} Y_{i}+(x-\bar{x})
    \sum_{i=1}^{n} \frac{\left(x_{i}-\bar{x}\right)}{S_{x x}} Y_{i}=\sum_{i=1}^{n} a_{i} Y_{i}\]
where
\[a_{i}=\frac{1}{n}+(x-\bar{x}) \frac{\left(x_{i}-\bar{x}\right)}{S_{x x}}\]
Since $ \tilde{\mu}(x) $ is a linear combination of Gaussian random variables, it has a Gaussian
distribution. We only need to find $ \E{\tilde{\mu}(x)} $ and $ \Var{\tilde{\mu}(x)} $.
Facts:
\[ \sum\limits_{i=1}^{n} a_i=1 \]
\[ \sum\limits_{i=1}^{n} a_i x_i=\sum\limits_{i=1}^{n} \frac{1}{n} +
    \sum\limits_{i=1}^{n} x_i\left( x-\bar{x} \right)\frac{(x_i-\bar{x})}{S_{xx}}
    =\bar{x}+\left( x-\bar{x} \right)\sum\limits_{i=1}^{n} x_i \frac{\left( x_i-\bar{x} \right)}{S_{xx}}=x  \]
Thus,
\[
    \begin{aligned}
        \E{\tilde{\mu}(x)}
         & =\sum\limits_{i=1}^{n} a_i\E{Y_i}                                                   \\
         & =\sum\limits_{i=1}^{n} a_i\left( \alpha+\beta x_i \right)                           \\
         & =\alpha \sum\limits_{i=1}^{n} a_i+\beta\left( \sum\limits_{i=1}^{n} a_i x_i \right) \\
         & =\alpha+\beta x                                                                     \\
         & =\mu(x)
    \end{aligned}
\]
\[
    \begin{aligned}
        \Var{\tilde{\mu}(x)}                                                                                                  \\
         & =\sum\limits_{i=1}^{n} a_i\Var{Y_i}                                                                                \\
         & =\sigma^2 \sum\limits_{i=1}^{n} a_i^2                                                                              \\
         & =\sigma^{2} \sum_{i=1}^{n}\left[\frac{1}{n^{2}}+\frac{2}{n} \frac{(x-\bar{x})\left(x_{i}-\bar{x}\right)}{S_{x x}}+
        \frac{(x-\bar{x})^{2}\left(x_{i}-\bar{x}\right)^{2}}{\left(S_{x x}\right)^{2}}\right]                                 \\
         & =\sigma^2\left[ \frac{1}{n} +\frac{\left( x-\bar{x} \right)^2}{S_{xx}}  \right]
    \end{aligned}
\]
Therefore, we have that
\[\mu(x) \sim G\left(\mu(x), \sigma \sqrt{\frac{1}{n}+\frac{(x-\bar{x})^{2}}{S_{x x}}}\right)\]
\subsection{Interferences for the Mean}
Since
\[ \frac{\tilde{\mu}(x)-\mu(x)}{\sigma \sqrt{\frac{1}{n}+\frac{\left( x-\bar{x} \right)^2}{S_{xx}}}}\sim G(0,1)
    \quad\text{and}\quad \frac{(n-2)S_e^2}{\sigma^2}\sim \chi^2(n-2)   \]
independently, it follows that
\[ \frac{\tilde{\mu}(x)-\mu(x)}{S_e \sqrt{\frac{1}{n}+\frac{\left( x-\bar{x} \right)^2}{S_{xx}}}}\sim
    t(n-2) \]
This pivotal quantity can be used to construct confidence intervals for $ \mu(x) $
and to test hypotheses about $ \mu(x) $.

\underline{Confidence Interval for the Mean Response at $ x $}

A $ 100p\% $ confidence interval for $ \mu(x)=\alpha+\beta x $ is given by
\[ \hat{\mu}(x)\pm a s_e\sqrt{\frac{1}{n}+\frac{\left( x-\bar{x} \right)^2}{S_{xx}}}=
    \hat{\alpha}+\hat{\beta}x\pm as_e \sqrt{\frac{1}{n}+\frac{\left( x-\bar{x} \right)^2}{S_{xx}}}\]
where $ P(T\leqslant a)=\frac{1+p}{2} $ and $ T \sim t(n-2) $.


\begin{Example}{$ 95\% $ Confidence Interval for the Mean STAT 231 Final Grade}{}
    Since
    \[ P(T\leqslant 2.0484)=\frac{1+0.95}{2}=0.975\quad\text{where}\quad T\sim t(28) \]
    a $ 95\% $ confidence interval for the mean STAT 231 final grade for students who obtained
    a final grade of $ 75 $ in STAT 230 is
    \begin{align*}
        \hat{\alpha}+\hat{\beta} x \pm a s_{e} \sqrt{\frac{1}{n}+\frac{(x-\bar{x})^{2}}{S_{x x}}}
         & =-4.0667+0.9944(75) \pm 2.0484(9.4630) \sqrt{\frac{1}{30}+\frac{(75-76.7333)^{2}}{5135.8667}} \\
         & =70.51 \pm 3.5699
    \end{align*}
    or
    \[ \left[ 66.9,74.1 \right] \]
\end{Example}


\underline{Confidence Interval for the $ y $-Intercept, $ \alpha $}

Since $ \mu(0)=\alpha+\beta(0)=\alpha $, a $ 100p\% $ confidence interval for $ \alpha $,
is given by
\[ \hat{\alpha}\pm as_e\sqrt{\frac{1}{n} +\frac{\left( \bar{x} \right)^2}{S_{xx}} } \]
If $ \bar{x} $ is large in magnitude (which means the average $ x_i $ is large),
then the confidence interval for $ \alpha $ will be very wide.

This would be disturbing if the value $ x=0 $ is a value of interest, but it often not.

\underline{Confidence Interval for the Variance}

The pivotal quantity
\[ \frac{(n-2)S_e^2}{\sigma^2}\sim \chi^2(n-2)  \]
can be used to construct intervals for $ \sigma^2 $.

A $ 100p\% $ confidence interval for $ \sigma^2 $ is given by
\[ \left[ \frac{(n-2)s_e^2}{b} , \frac{(n-2)s_e^2}{a}  \right] \]
where
\[ P(U\leqslant a)=\frac{1-p}{2}=P(U>b),\quad\text{where}\quad U\sim \chi^2(n-2) \]
A $ 100p\% $ confidence interval for $ \sigma $ is given by
\[ \left[ \sqrt{\frac{(n-2)s_e^2}{b}} , \sqrt{\frac{(n-2)s_e^2}{a}}  \right] \]

\subsection{Interference for an Individual Response $ Y $ at $ x $}
Suppose we wanted an interval for $ Y= $ STAT 231 final grade for one student
who obtained a final grade of $ x=75 $ in STAT 230.

Now
\[\tilde{\mu}(x)=\tilde{\alpha}+\tilde{\beta} x \sim G\left(\alpha+\beta x,
    \sigma \sqrt{\frac{1}{n}+\frac{(x-\bar{x})^{2}}{S_{x x}}}\right)\]
and
\[ Y \sim G(\alpha+\beta x, \sigma) \]
independently.

To obtain a confidence interval for $ Y $, we first obtain the distribution
of the random variable $ Y-\tilde{\mu}(x) $ using its mean and variance.

Since
\[ \E{Y-\tilde{\mu}(x)}=0 \]
and
\begin{align*}
    \Var{Y-\tilde{\mu}(x)}
     & =\Var{Y}+\Var{\tilde{\mu}(x)}                                                    \\
     & =\sigma^2+\sigma^2\left[ \frac{1}{n} +\frac{(x-\bar{x})^2}{S_{xx}}  \right]      \\
     & =\sigma^2\left[ 1+\frac{1}{n} + \frac{\left( x-\bar{x} \right)}{S_{xx}}  \right]
\end{align*}
we have
\[ Y-\tilde{\mu}(x)\sim G\left( 0,\sigma\left[ 1+\frac{1}{n} + \frac{\left( x-\bar{x} \right)^2}{S_{xx}}  \right]
    ^{1/2} \right) \]
Since
\[ Y-\tilde{\mu}(x)\sim G\left( 0,\sigma\left[ 1+\frac{1}{n} + \frac{\left( x-\bar{x} \right)^2}{S_{xx}}  \right]
    ^{1/2} \right) \]
and
\[ \frac{(n-2)S_e^2}{\sigma^2}\sim \chi^2(n-2)  \]
independently, we have
\[ \frac{Y-\tilde{\mu}(x)}{S_{e} \sqrt{1+\frac{1}{n}+\frac{(x-\bar{x})^{2}}{S_{x x}}}} \sim t(n-2) \]
This pivotal quantity can be used to construct a prediction interval for $ Y $.

\subsection{A $ 100p\% $ Prediction Interval for a Future Response $ Y $}
The corresponding interval
\[\hat{\mu}(x) \pm a s_{e} \sqrt{1+\frac{1}{n}+\frac{(x-\bar{x})^{2}}{S_{x x}}}=\hat{\alpha}+\hat{\beta} x \pm a s_{e} \sqrt{1+\frac{1}{n}+\frac{(x-\bar{x})^{2}}{S_{x x}}}\]
where $ a $ is the value from the $ t $-table such that
\[P(T \leq a)=\frac{1+p}{2}, \quad \text{where} T \sim t(n-2)\]
We usually call such an interval a $ 100p\% $ prediction interval instead of a confidence interval,
since $ Y $ is not a parameter but a random variable.


\begin{Example}{Prediction Interval Example}{}
    A $ 95\% $ prediction interval for the STAT 231 final grade for a student who obtained
    a final grade of $ 75 $ in STAT 230 is
    \begin{align*}
        \hat{\alpha}+\hat{\beta} x \pm a s_{e} \sqrt{1+\frac{1}{n}+\frac{(x-\bar{x})^{2}}{S_{x x}}}
         & =-4.0667+0.9944(75) \pm 2.0484(9.4630) \sqrt{1+\frac{1}{30}+\frac{(75-76.7333)^{2}}{5135.8667}} \\
         & =70.51 \pm 19.7100
    \end{align*}
    or
    \[ \left[ 50.8,90.2 \right] \]
\end{Example}


\subsection{Gaussian Response Models}
The simple linear regression model we have just considered,
\[ Y_i \sim G(\alpha+\beta x_i,\sigma),\quad \text{for } i=1,2, \ldots, n \text{ independently} \]
where $ x_i $ is assumed to be a known constant, is a member of a larger family of models
called \textbf{Gaussian response models}.

The general form of a Gaussian response model is
\[Y_{i} \sim G\left(\mu\left(x_{i}\right), \sigma\right), \quad \text{for } i=1,2, \ldots, n \text{ independently}\]
where $ x_i $ is assumed to be a known constant, and where $ x_i $ can be a vector of explanatory
variates also called \textbf{covariates}.

Note that the mean of $ Y_i $ depends on $ x_i $, which may be a vector of variates (also called
covariates in the linear regression model) or a scalar. However, the standard deviation $ \sigma $
does not depend on $ x_i $.

The Gaussian response model
\[Y_{i} \sim G\left(\mu\left(x_{i}\right), \sigma\right), \quad \text{for } i=1,2, \ldots, n \text{ independently}\]
can also be written in the form
\[Y_{i}=\mu\left(x_{i}\right)+R_{i}, \quad \text{where } R_{i} \sim G(0, \sigma), i=1,2, \ldots, n \text{ independently}\]
$ Y_i $ is the sum of two componenets.

The first component, $ \mu(x_i) $, is a deterministic component (not a random variable),
and the second component $ R_i $ is a random component or random variable.

\underline{Linear Regression Models}

In many examples,
\[ \mu(x_i)=\beta_0+\sum\limits_{j=1}^{k} B_j x_{ij} \]
so the mean of $ Y_i $ is a linear function of $ x_i=(x_{i1},x_{i2},\ldots ,x_{ik}) $,
the vector of covariates for unit $ i $ and the unknown parameters $ \beta_0,\beta_1,\ldots ,\beta_k $.

These models are called \textbf{linear regression models}, and the $ B_j $'s are called the
\textbf{regression parameters.}

\subsection{Model Checking}
There are two main assumptions for Gaussian linear response models:
\begin{enumerate}
    \item $ Y_i $ (given covariates $ x_i $) is Gaussian with standard deviation $ \sigma $
          which does not depend on covariates.
    \item $ \E{Y_i}=\mu(x_i) $ is a linear combination of observed covariates with
          unknown coefficients.
\end{enumerate}
\begin{center}
    MODEL ASSUMPTIONS SHOULD ALWAYS BE CHECKED!!!
\end{center}
We will use graphical methods to do this.

\underline{Model Checking Method 1: Scatterplot with Fitted Line}

In a simple linear regression, a scatter plot of the data with the fitted line superimposed
shows us how well the model fits.

\underline{Model Checking Method 2: Residual Plots}

Residual plots are very useful for model checking when there are two or more explanatory
variates.

\begin{Definition}{}{}
    For the simple linear regression model, let
    \[ \hat{\mu}_i=\hat{\alpha}+\hat{\beta}x_i \]
    (often called the ``fitted'' response), and let
    \[ \hat{r}_i=y_i-\hat{\mu}_i,\quad i=1,2,\ldots ,n \]
    The $ \hat{r}_i $'s are \textbf{residuals} since $ \hat{r}_i $ represents
    what is ``left'' after the model has been ``fitted'' to the data.
\end{Definition}

The $ \hat{r}_i $'s can often be though of as ``observed'' $ R_i $'s in the model $ Y_i=\mu_i+R_i $,
where $ R_i\sim G(0,\sigma) $, $ i=1,2,\ldots ,n $ independently.

This isn't exactly correct since we are using $ \hat{\mu}_i $ instead of $ \mu_i $,
but if the model is correct, then the $ \hat{r}_i $'s should behave roughly like a random
sample from the $ G(0,\sigma) $ distribution.

Recall $ \hat{\alpha}=\bar{y}-\hat{\beta}\bar{x} $, which implies that $ \bar{y}-\hat{\alpha}-\hat{\beta}\bar{x}=0 $
or
\[ 0=\bar{y}-\hat{\alpha}-\hat{\beta}\bar{x}=\frac{1}{n} \sum\limits_{i=1}^{n} \left( \bar{y}_i-\hat{\alpha}-\hat{\beta}\bar{x}_i \right)
    =\frac{1}{n} \sum\limits_{i=1}^{n} \hat{r}_i \]
A plot of the points $ \left( x_i,\hat{r}_i \right) $, $ i=1,2,\ldots ,n $
should lie more or less within a horizontal band around the line $ \hat{r}_i=0 $.

\underline{Standardized Residual Plots}

\begin{Definition}{}{}
    Define the \textbf{standardized residuals} as
    \[\hat{r}_{i}^{*}=\frac{\hat{r}_{i}}{s_{e}}=\frac{y_{i}-\hat{\mu}_{i}}{s_{e}}, \quad i=1,2, \ldots, n\]
\end{Definition}

What is the only difference between a plot of points $ \left( x_i,\hat{r}_i \right) $, $ i=1,2,\ldots ,n $
and a plot of points $ \left( x_i,\hat{r}_i^* \right) $, $ i=1,2,\ldots ,n $.

If the model is correct, then the $ \hat{r}_i^* $ values will lie in the range of $ (-3,3) $. Why is this?

\underline{Model Checking Method 3: Plot of Residuals Versus Expected}

Another type of residual plot can be used to check the assumption about the form of the mean.
It consists of plotting the points $ \left( \hat{\mu}_i,\hat{r}_i \right) $, $ i=1,2,\ldots ,n $.

For the simple linear regression model, we are checking whether the assumed mean
\[ \E{Y_i}=\mu(x_i)=\alpha+\beta x_i \]
is reasonable.

if the assumed mean is reasonable, we should see approximately a horizontal band around the line
$ \hat{r}_i=0 $.

A plot of $ \left( \hat{\mu}_i,\hat{r}_i^* \right) $ looks like the plot of $ \left( \hat{\mu}_i,\hat{r}_i \right) $.
The only difference is that the vertical axis is rescaled.

\underline{Model Checking Method 4: Qqplot of Standardized Residuals}

To check the normality assumption, we use a qqplot of the standardized residuals.

Since our assumed model is
\[ \frac{Y_i-\mu_i}{\sigma}\sim G(0,1)  \]
the $ \hat{r}_i^* $'s should represent a sample (not quite random since $ \sum\limits_{i=1}^{n} \hat{r}_i^* =0 $)
from the $ G(0,1) $ distribution.

Therefore, a qqplot of the $ \hat{r}_i^* $ terms should give approximately a straight line if the normality
assumption holds.

\underline{Intepreting Residual Plots}

If a plot of the points $ (x_i,\hat{r}_i) $ or $ (x_i,\hat{r}_i^*) $, $ i=1,2,\ldots ,n $
shows a distinctive pattern, then this suggests the assumed form for $ \mu(x_i) $ may be inappropriate.

If a plot of the points $ \left( \hat{\mu}_i,\hat{r}_i \right) $, $ i=1,2,\ldots ,n $
indicates that the variability in the $ \hat{r}_i $'s is bigger for large values of $ \hat{\mu}_i $
than for small values of $ \hat{\mu}_i $ (or vice versa), thn there is evidence to suggest
that the assumption of constant variance $ \Var{Y_i}=\Var{R_i}=\sigma^2 $, $ i=1,2,\ldots ,n $
does not hold.

\underline{Intepreting Residual Plots: Warning}

Reading these plots takes practice. You should try not to read too much into plots especially
if the plots are based on a small number of points.

The following plots exhibit patterns.

\section{Comparing the Means of Two Populations}


\begin{Example}{Hand Span Example}{}
    Suppose we wanted to answer the question: Are the hand spans of females enrolled
    in STAT 231 in Winter 2015 different on average from the hand spans of males enrolled
    in STAT 231 in Winter 2015?

    To do this we test the hypothesis that there is no difference in mean hand spans
    between males and females enrolled in STAT 231 in Winter 2015.

    Let $ Y_{1i}= $ the hand span of the $ i $th male, $ i=1,2,\ldots ,78 $,
    and let $ Y_{2i}= $ the hand span of the $ i $th female, $ i=1,2,\ldots ,64 $.

    Based on these observations, a Gaussian model seems reasonable for both the $ Y_{1i} $'s
    and $ Y_{2i} $'s.

    Assume $ Y_{1i} $, $ i=1,2,\ldots ,78 $ is a random sample from a $ G(\mu_1,\sigma) $
    distribution, and independently $ Y_{2i} $, $ i=1,2,\ldots ,64 $ is a random
    sample from a $ G(\mu_2,\sigma) $ distribution.

    We call this a two sample Normal or Gaussian problem.

    Note that we have assumed both Gaussian populations have the same standard deviation,
    $ \sigma $.

    There are three unknown parameters in the model: $ \mu_1 $, $ \mu_2 $, and $ \sigma $.

    The parameter $ \mu_1 $ represents the mean hand span in centimeters for males
    enrolled in STAT 231 in Winter 2015 (the study population). Note that we are assuming
    there is no bias in the measurements due to the measurement system.

    The parameter $ \mu_2 $ represents the mean hand span in centimeters for females
    enrolled in STAT 231 in Winter 2015 (the study population).

    The hypothesis of interest is $ H_0:\mu_1=\mu_2 $, or $ H_0:\mu_1-\mu_2=0 $.
\end{Example}


\subsection{Special Case of the Gaussian Response Model}
By letting
\[\begin{aligned}
        Y_{i}=Y_{1 i},     & i=1,2, \ldots, n_{1} & \text{and} & Y_{n_{1}+i}=Y_{2 i},    & i=1,2, \ldots, n_{2} \\
        \E{Y_{i}}=\mu_{1}, & i=1,2, \ldots, n_{1} & \text{and} & E{Y_{n_{1}+i}}=\mu_{2}, & i=1,2, \ldots, n_{2}
    \end{aligned}\]
and
\[\Var{Y_{i}}=\sigma^{2}, \quad i=1,2, \ldots, n_{1}+n_{2}\]
we can see that this model is just a special case of the Gaussian response model.
\[ Y_i\sim G\left( \mu(x_i,\sigma) \right),\quad i=1,2,\ldots ,n\text{ independently} \]
where $ \mu(x_i)=\mu_1 $, $ i=1,2,\ldots ,n_1 $ and $ \mu(x_i)=\mu_2 $, $ i=n_1+1,2,\ldots ,n_1+n_2 $.

\underline{Likelihood Function for Random Samples from Two Gaussian Populations}

The likelihood function for $ \mu_1 $, $ \mu_2 $, and $ \sigma $ is
\[L\left(\mu_{1}, \mu_{2}, \sigma\right)=\prod_{i=1}^{n_{1}} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left[-\frac{1}{2}\left(\frac{y_{1 i}-\mu_{1}}{\sigma}\right)^{2}\right] \prod_{i=1}^{n_{2}} \frac{1}{\sqrt{2 \pi} \sigma}
    \exp \left[-\frac{1}{2}\left(\frac{y_{2 i}-\mu_{2}}{\sigma}\right)^{2}\right]\]
or more simply
\[L\left(\mu_{1}, \mu_{2}, \sigma\right)=\sigma^{-\left(n_{1}+n_{2}\right)} \exp \left[-\frac{1}{2} \sum_{i=1}^{n_{1}}\left(\frac{y_{1 i}-\mu_{1}}{\sigma}\right)^{2}\right]
    \exp \left[-\frac{1}{2} \sum_{i=1}^{n_{2}}\left(\frac{y_{2 i}-\mu_{2}}{\sigma}\right)^{2}\right]\]
$ \mu_1\in\mathbb{R} $, $ \mu_2\in\mathbb{R} $, and $ \sigma>0 $.

\underline{Maximum Likelihood Estimators}

The log likelihood function is
\[\ell\left(\mu_{1}, \mu_{2}, \sigma\right)=-\left(n_{1}+n_{2}\right) \log \sigma-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n_{1}}\left(y_{1 i}-\mu_{1}\right)^{2}
    -\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n_{2}}\left(y_{2 i}-\mu_{2}\right)^{2}\]
Maximization of this function gives the maximum likelihood estimators
\[ \tilde{\mu}_1=\frac{1}{n_1} \sum\limits_{i=1}^{n_1} Y_{1i}=\bar{Y}_1 \]
\[ \tilde{\mu}_2=\frac{1}{n_2} \sum\limits_{i=1}^{n_1} Y_{2i}=\bar{Y}_2 \]
\[ \tilde{\sigma}^2=\frac{1}{n_1+n_2}\left[ \sum\limits_{i=1}^{n_1}
        \left( Y_{1i}-\bar{Y}_1 \right)^2+\sum\limits_{i=1}^{n_2}
        \left( Y_{2i}-\bar{Y}_2 \right)^2 \right]  \]


\subsection{Pooled Estimator of Variance}
Define
\[S_{p}^{2}=\frac{1}{n_{1}+n_{2}-2}\left[\sum_{i=1}^{n_{1}}\left(Y_{1 i}-\bar{Y}_{1}\right)^{2}+\sum_{i=1}^{n_{2}}\left(Y_{2 i}-\bar{Y}_{2}\right)^{2}\right]\]
Note that
\[S_{p}^{2}=\left(\frac{n_{1}-1}{n_{1}+n_{2}-2}\right) S_{1}^{2}+\left(\frac{n_{2}-1}{n_{1}+n_{2}-2}\right) S_{2}^{2}\]
where
\[S_{1}^{2}=\frac{1}{n_{1}-1} \sum_{i=1}^{n_{1}}\left(Y_{1 i}-\bar{Y}_{1}\right)^{2}\]
is the sample variance for the data from population 1, and
\[S_{2}^{2}=\frac{1}{n_{2}-1} \sum_{i=1}^{n_{2}}\left(Y_{2 i}-\bar{Y}_{2}\right)^{2}\]
is the sample variance for the data from population 2.

\begin{Definition}{}{}
    \[\begin{aligned}
            S_{p}^{2} & =\frac{1}{n_{1}+n_{2}-2}\left[\sum_{i=1}^{n_{1}}\left(Y_{1 i}-\bar{Y}_{1}\right)^{2}+\sum_{i=1}^{n_{2}}\left(Y_{2 i}-\bar{Y}_{2}\right)^{2}\right] \\
                      & =\frac{\left(n_{1}-1\right) S_{1}^{2}+\left(n_{2}-1\right) S_{2}^{2}}{n_{1}+n_{2}-2}
        \end{aligned}\]
    $ S_p^2 $ is called the \textbf{pooled estimator of variance}, since it is obtained by
    ``pooling'' the estimators $ S_1^2 $ and $ S_2^2 $ of $ \sigma^2 $ from the two samples.
\end{Definition}

Why does this estimator make sense?

The degrees of freedom are $ n_1+n_2-2 $ because of the two restrictions
\[\sum_{i=1}^{n_{1}}\left(Y_{1 i}-\bar{Y}_{1}\right)=0 \quad \text{and} \quad \sum_{i=1}^{n_{2}}\left(Y_{2 i}-\bar{Y}_{2}\right)=0\]
It cal also be shown that
\[\frac{\left(n_{1}+n_{2}-2\right) S_{p}^{2}}{\sigma^{2}}=\frac{\sum_{i=1}^{n_{1}}\left(Y_{1 i}-\bar{Y}_{1}\right)^{2}+\sum_{i=1}^{n_{2}}
    \left(Y_{2 i}-\bar{Y}_{2}\right)^{2}}{\sigma^{2}} \sim \chi^{2}\left(n_{1}+n_{2}-2\right)\]


\subsection{Inferences for the Difference Between the Means}
To make inferences about the mean difference $ \mu_1-\mu_2 $,
we note that $ \tilde{\mu}_1=\bar{Y}_1 $ is a point estimator of $ \mu_1 $ and
$ \tilde{\mu}_2=\bar{Y}_2 $ is a point estimator of $ \mu_2 $, so that $ \tilde{\mu}_1-
    \tilde{\mu}_2=\bar{Y}_1-\bar{Y}_2 $ is a point estimator of $ \mu_1-\mu_2 $.

Since
\[\bar{Y}_{1} \sim N\left(\mu_{1}, \frac{\sigma^{2}}{n_{1}}\right) \quad \text{and } \quad \bar{Y}_{2} \sim N\left(\mu_{2}, \frac{\sigma^{2}}{n_{2}}\right) \quad \text{ independently}\]
we have
\[\tilde{\mu}_{1}-\tilde{\mu}_{2}=\bar{Y}_{1}-\bar{Y}_{2}
    \sim N\left(\mu_{1}-\mu_{2}, \sigma^{2}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)\right)\]

\underline{Pivotal Quantity for Confidence Interval for Mean Difference}

Since
\[\frac{\bar{Y}_{1}-\bar{Y}_{2}-\left(\mu_{1}-\mu_{2}\right)}{\sigma \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}} \sim G(0,1)\]
and
\[\frac{\left(n_{1}+n_{2}-2\right) S_{p}^{2}}{\sigma^{2}} \sim \chi^{2}\left(n_{1}+n_{2}-2\right)\]
independently, then
\[\frac{\bar{Y}_{1}-\bar{Y}_{2}-\left(\mu_{1}-\mu_{2}\right)}{S_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}} \sim t\left(n_{1}+n_{2}-2\right)\]
and this is the pivotal quantity that we can use to make inferences about the mean difference
$ \mu_1-\mu_2 $ when $ \sigma $ is unknown.

\underline{Confidence Interval for Difference in Means}

Since
\[\frac{\bar{Y}_{1}-\bar{Y}_{2}-\left(\mu_{1}-\mu_{2}\right)}{S_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}} \sim t\left(n_{1}+n_{2}-2\right)\]
is a $ 100p\% $ confidence interval for $ \mu_1-\mu_2 $ is given by
\[\bar{y}_{1}-\bar{y}_{2} \pm a s_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\]
where
\[P(T \leq a)=\frac{1+p}{2} \quad \text{and} \quad T \sim t\left(n_{1}+n_{2}-2\right)\]

\subsection{Test of Hypothesis for No Difference in Means}
Since
\[\frac{\bar{Y}_{1}-\bar{Y}_{2}-\left(\mu_{1}-\mu_{2}\right)}{S_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}} \sim t\left(n_{1}+n_{2}-2\right)\]
to test the hypothesis $ H_0:\mu_1-\mu_2=0 $,
we use the test statistic
\[D=\frac{\left|\bar{Y}_{1}-\bar{Y}_{2}-0\right|}{S_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}}\]
Let the observed value be
\[d=\frac{\left|\bar{y}_{1}-\bar{y}_{2}-0\right|}{s_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}}\]
then the $ p $-value is
\[p \text{-value }=2[1-P(T \leq d)], \quad \text{ where} T \sim t\left(n_{1}+n_{2}-2\right)\]

\begin{Example}{Hand Span Example: Test of Hypothesis for No Difference in Means}{}
    For the males:
    \[\hat{\mu}_{1}=\bar{y}_{1}=21.50 \quad \text{and} \quad s_{1}^{2}=3.4309\]
    For the females:
    \[\hat{\mu}_{2}=\bar{y}_{2}=19.37 \quad \text{and} \quad s_{2}^{2}=2.055\]
    with
    \[\hat{\mu}_{1}-\hat{\mu}_{2}=\bar{y}_{1}-\bar{y}_{2}=21.50-19.37=2.14\]
    and
    \[s_{p}^{2}=\frac{(77)(3.4309)+(63)(2.055)}{78+64-2}=2.8117 \quad \text{and} \quad s_{p}=\sqrt{2.8117}=1.6768\]

\end{Example}


\begin{Example}{Hand Span Example: $ 95\% $ Confidence Interval for $ \mu_1-\mu_2 $}{}
    Using R, we obtain $ P(T\leqslant 1.97705)=0.975 $, where $ T\sim t(140) $.
    A $ 95\% $ confidence interval for $ \mu_1-\mu_2 $ is given by
    \[\begin{aligned}
            \bar{y}_{1}-\bar{y}_{2} \pm a s_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}} & =2.14 \pm(1.97705)(1.6768) \sqrt{\frac{1}{78}+\frac{1}{64}} \\
                                                                                       & =2.14 \pm 0.5591                                            \\
                                                                                       & =[1.58,2.70]
        \end{aligned}\]
    Since $ \mu_1-\mu_2=0 $ is not contained in this $ 95\% $ confidence interval, we already
    know that the $ p $-value for testing $ H_0:\mu_1-\mu_2=0 $ is less than $ 0.05 $.

    \underline{Hand Span Example: $ p $-Value}

    Since
    \[d=\frac{|2.14-0|}{(1.67687) \sqrt{\frac{1}{78}+\frac{1}{64}}}=7.56\]
    the actual $ p $-value is
    \[\begin{aligned}
            p \text{-value } & =2[1-P(T \leq 7.56)], \quad \text{ where} T \sim t(140) \\
                             & \approx 0
        \end{aligned}\]
    Therefore, there is very strong evidence to contradict the hypothesis $ H_0:\mu_1-\mu_2=0 $
    based on the data.

    The difference is statistically significant. Is the difference of practical significance?
\end{Example}


\subsection{Comparison of Two Means, Unequal Variances}
Recall that the previous analysis depends on the assumptions
\begin{itemize}
    \item $ Y_{11},Y_{12},\ldots ,Y_{1n_1} $ is a random sample from $ G(\mu_1,\sigma_1) $
    \item $ Y_{21},Y_{22},\ldots ,Y_{2n_2} $ is a random sample from $ G(\mu_2,\sigma_2) $
    \item the two samples are independent
    \item $ \sigma_1=\sigma_2 $
\end{itemize}
What if $ \sigma_1=\sigma_2 $ is not a reasonable assumption?

Note: $ H_0:\sigma_1=\sigma_2 $ could be tested using a likelihood ratio test.

\underline{Approximate Pivotal Quantity, Unequal Unknown Variances}

If $ n_1 $ and $ n_2 $ are both large, then the approximate pivotal quantity is
\[\frac{\bar{Y}_{1}-\bar{Y}_{2}-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}} \sim G(0,1) \text{approximately}\]
This pivotal quantity can be used to construct confidence intervals and test hypotheses for the
mean difference $ \mu_1-\mu_2 $.


\begin{Example}{}{}
    For example, an approximate $ 95\% $ confidence interval for $ \mu_1-\mu_2 $ would be given
    by
    \[\bar{y}_{1}-\bar{y}_{2} \pm 1.96 \sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}\]
\end{Example}



\begin{Example}{Hand Span Example: $ 95\% $ Confidence Interval for $ \mu_1-\mu_2 $}{}
    For the males we have the following:
    \[n_{1}=78, \quad \hat{\mu}_{1}=\bar{y}_{1}=21.50, \quad \text{and} s_{1}^{2}=3.4309\]
    For the females we have the following:
    \[n_{2}=64, \quad \hat{\mu}_{2}=\bar{y}_{2}=19.37, \quad \text{and} s_{2}^{2}=2.055\]
    An approximate $ 95\% $ confidence interval for $ \mu_1-\mu_2 $ is given by
    \[\bar{y}_{1}-\bar{y}_{2} \pm 1.96 \sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}=21.50-19.37 \pm 1.96 \sqrt{\frac{3.4309}{78}+\frac{2.055}{64}}=[1.60,2.68]\]
    as compared to $ [1.58,2.70] $.
\end{Example}


\section{Gaussian Response Models}
\subsection{Bean Experiment}
In my Winter 2015 STAT 231 class, I conducted the following experiment.

Each student was given two small paper cups. One cup contained $30$ black beans, and
the other cup was empty.

Each student was also given a piece of paper on which two circles
(the size of the paper cup bottom) were drawn.

Students were asked to place one paper cup on each circle so the cups were the same distance
apart for each student, and then to use one hand to hold the cup containing the beans in place.
Using the other hand, they were asked to move as many black beans as possible to the empty
cup, one at a time, in a timed 15 -second interval. The students then performed the same
task using the opposite hands.

Students were randomized with respect to whether they moved the beans with
their dominant or nondominant hand first.

\underline{Bean Experiment Data}

Difference in the Number of Beans Moved by the Dominant and
Non-Dominant Hands
\[ \begin{array}{|c|c|}
        \hline
        \text{Difference } & \text{ Frequency} \\
        \hline
        -4                 & 2                 \\
        \hline
        -3                 & 1                 \\
        \hline
        -2                 & 1                 \\
        \hline
        -1                 & 4                 \\
        \hline
        0                  & 13                \\
        \hline
        1                  & 15                \\
        \hline
        2                  & 14                \\
        \hline
        3                  & 5                 \\
        \hline
        4                  & 2                 \\
        \hline
        \text{Total}       & 57                \\
        \hline
    \end{array} \]
\begin{itemize}
    \item Is there a difference in the mean number of beans moved in 15 seconds between the dominant and non-dominant hands?
    \item How do we analyze these data? What model should we assume? Is it a two sample problem?
    \item The assumptions for the two sample model are:
          \[Y_{11}, Y_{12}, \ldots, Y_{1 n_{1}} \text{is a random sample from} G\left(\mu_{1}, \sigma\right)\]
          and independently
          \[Y_{21}, Y_{22}, \ldots, Y_{2 n_{2}} \text{is a random sample from} G\left(\mu_{2}, \sigma\right)\]
\end{itemize}

\underline{Bean Experiment: Correlation in the Data}

Let $ Y_{1i}= $ number of beans moved using the dominant hand, and let $ Y_{2i}= $
number of beans moved using the non-dominant hand for the $ i $th student.

Does it seem reasonable to assume the $ Y_{1i} $'s are independent of the $ Y_{2i} $'s?

We could expect the observations on the $i$ th student
$\left(Y_{1 i}, Y_{2 i}\right)$ to be positively correlated.
That is, we would expect $\Cov{Y_{1 i}, Y_{2 i}}>0$.

\subsection{Paired Experiment}
In fact, the observations from the bean experiment have been deliberately paired to eliminate some factors
(e.g., finger size, agility, competitive spirit, etc.) which might otherwise affect conclusions about the parameter of interest, which is the mean difference $\mu_{1}-\mu_{2}$

The bean experiment is an example of a \textbf{paired experiment}.

For a paired experiment (can you show this?)
\[ \Var{\bar{Y}_{1}-\bar{Y}_{2}}
    =\frac{\sigma_{1}^{2}}{n}+\frac{\sigma_{2}^{2}}{n}-\frac{2}{n} \Cov{Y_{1 i}, Y_{2 i}} \]
If
$\Cov{Y_{1 i}, Y_{2 i}}>0 $,
then $\Var{\bar{Y}_{1}-\bar{Y}_{2}}$ is smaller than for an unpaired experiment.

\underline{Paired Experiment: Making Inferences about the Difference $ \mu=\mu_1-\mu_2 $}

To make inferences about $\mu=\mu_{1}-\mu_{2}$, we analyze the within-pair differences
\[Y_{i}=Y_{1 i}-Y_{2 i}, \quad i=1,2, \ldots, n\]
We assume
\[ Y_{i}=Y_{1 i}-Y_{2 i} \sim G\left(\mu_{1}-\mu_{2}, \sigma\right), \quad i=1,2, \ldots, n \]
independently.

We can now use the one-sample analysis that we used previously for analysing
a random sample from a $G(\mu, \sigma)$ distribution, with $\mu=\mu_{1}-\mu_{2}$.

\underline{Bean Experiment: Testing the Null Hypothesis}

For the bean data,
\[ \bar{y}=0.86 \quad \text{and} \quad s=\left[\frac{1}{56} \sum_{i=1}^{57}\left(y_{i}-\bar{y}\right)^{2}\right]^{1 / 2}=1.66 \]
To test $ H_0:\mu=0 $, we use the test statistic
\[D=\frac{|\bar{Y}-0|}{S / \sqrt{n}}\]
with the observed value
\[d=\frac{|\bar{y}-0|}{s / \sqrt{n}}=\frac{|0.86-0|}{1.66 / \sqrt{57}}=3.90\]
and $p$-value
\[p \text{-value }=2[1-P(T \leq 3.90)] \approx 0, \quad \text{where} T \sim t(56)\]
and there is strong evidence against $H_{0}: \mu=0$ based on the observed data.

\underline{Bean Experiment: Practical Versus Statistical Significance}

The difference is statistically significant.

Is the difference of practical significance?

\subsection{Examples of Experiments on Differences Between Means}
Examples in which the parameter of interest is the mean difference $\mu_{1}-\mu_{2}$
\begin{enumerate}
    \item Test for the difference in execution time between two algorithms
          $A$ and $B$ with randomly generated data.
    \item Test for a difference in the error rates or speeds of two algorithm
          designed for image resolution or character/speech recognition on many different scenarios/problems.
    \item Test whether one numerical method for nonlinear optimization is faster
          than another on a large population of potential test functions.
    \item Artificial intelligence: test whether one learning algorithm learns a task faster than another.
\end{enumerate}

\underline{Experimental Design I}

Generate or select $n_{1}$ random ``problems'' (for example, data sets to be sorted,
functions to be minimized, images to be resolved), and compute the mean execution time $\bar{Y}_{1}$ of
algorithm A.

Generate or select another $n_{2}$ ($n_{1}=n_{2}$ possibly)
random ``problems'', and compute the mean execution time $\bar{Y}_{2}$ of algorithm B.

Estimate the difference as $\bar{Y}_{1}-\bar{Y}_{2}$.

In this case, $\bar{Y}_{1}$ and $\bar{Y}_{2}$ are independent random variables, thus
\[\E{\bar{Y}_{1}-\bar{Y}_{2}}=\mu_{1}-\mu_{2}\]
and
\[\Var{\bar{Y}_{1}-\bar{Y}_{2}}
    =\Var{\bar{Y}_{1}}+\Var{\bar{Y}_{2}}
    =\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}
\]

\underline{Experimental Design II}

Generate or select $n$ random ``problems''
(for example, data sets to be sorted, functions to be minimized, images to be resolved),
and compute the mean execution time $\bar{Y}_{1}$ of algorithm A.

Compute the mean execution time $\bar{Y}_{2}$ of algorithm B on the same set of $n$ problems.

Estimate the difference as $\bar{Y}_{1}-\bar{Y}_{2}$.

In this case $\bar{Y}_{1}, \bar{Y}_{2}$ are dependent random variables, thus
\[\E{\bar{Y}_{1}-\bar{Y}_{2}}=\mu_{1}-\mu_{2}\]
and
\[\Var{\bar{Y}_{1}-\bar{Y}_{2}}
    =\frac{\sigma_{1}^{2}}{n}+
    \frac{\sigma_{2}^{2}}{n}-\frac{2}{n}
    \Cov{Y_{1 i}, Y_{2 i}}
\]
If $ \Cov{Y_{1 i}, Y_{2 i}}>0$,
then $ \Var{\bar{Y}_{1}-\bar{Y}_{2}}$ is smaller for Design II\@.

\subsection{Pairing as a Design Choice}

We expect that the covariance between the execution times of algorithms on the same problem to be
positively co rrelated (harder problems have longer execution times).

A sample of dependent pairs $\left(Y_{1 i}, Y_{2 i}\right)$ is better than two
independent random samples (one of $Y_{1 i}$'s and one of $Y_{2 i}$'s) for
estimating $\mu_{1}-\mu_{2}$, since the difference $\mu_{1}-\mu_{2}$,
can be estimated more accurately (shorter confidence intervals)
if $\Cov{Y_{1 i}, Y_{2 i}}>0$.

Note: If $\Cov{Y_{1 i}, Y_{2 i}}<0$,
then pairing is a bad idea since it increases the value of
$\Var{\bar{Y_{1}}-\bar{Y_{2}}}$.

In a paired experiment, we do not
assume that $Y_{1 i}$ and $Y_{2 i}$ are independent random variables.
We do, however, assume the differences $Y_{i}=Y_{1 i}-Y_{2 i}, i=1,2, \ldots, n$
are independent (all different problems).

\underline{Paired Versus Unpaired}

When you see data from a comparative study (i.e., one
whose objective is to compare two distributions, often through their means),
you have to determine whether it involves paired data or not.

Of course, a sample of $Y_{1 i}$'s and $Y_{2 i}$'s cannot be
from a paired study unless there are equal numbers of each, but
if there are equal numbers, the study might be either ``paired'' or
``unpaired''.

\section{Multinomial Models and Goodness of Fit}
\subsection{Multinomial Models and Goodness of Fit}
Is the distribution of colours uniform?

\underline{Smarties Data}
\[ \begin{array}{|c|c|c|}
        \hline
        \text{Colour } & \text{ Observed Number } & \text{ Expected Number}           \\
        \hline
        \text{Red}     & 80                       & 610\left(\frac{1}{8}\right)=76.25 \\
        \hline
        \text{Green}   & 73                       & 76.25                             \\
        \hline
        \text{Yellow}  & 77                       & 76.25                             \\
        \hline
        \text{Blue}    & 107                      & 76.25                             \\
        \hline
        \text{Purple}  & 61                       & 76.25                             \\
        \hline
        \text{Brown}   & 73                       & 76.25                             \\
        \hline
        \text{Orange}  & 72                       & 76.25                             \\
        \hline
        \text{Pink}    & 77                       & 76.25                             \\
        \hline
        \text{Total}   & 610                      & 610                               \\
        \hline
    \end{array} \]
How do we conduct a formal test of the hypothesis that the distribution of different
colours is uniform?

\underline{Model and Hypothesis}
\begin{align*}
    P\left(\text{observing the data } y_{1}, \ldots, y_{k} ; \theta_{1}, \ldots, \theta_{k}\right)
     & =P\left(Y_{1}=y_{1}, \ldots, Y_{k}=y_{k} ; \theta_{1}, \ldots, \theta_{k}\right) \\
     & =\frac{n !}{y_{1} ! \cdots y_{k} !} \theta_{1}^{y_{1}} \dots \theta_{k}^{y_{k}}
\end{align*}

where $0<\theta_{j}<1, \sum_{j=1}^{k} \theta_{j}=1, y_{j}=0,1, \ldots$, and $\sum_{j=1}^{k} y_{j}=n$.

For our example, $Y_{j}=$ number of Smarties of colour $j, j=1,2, \ldots, 8$ and $k=8$.

We want to test the hypothesis
\[ H_{0}: \theta_{j}=\frac{1}{k}, \quad \text{for all} j=1,2, \ldots, k \]

\subsection{Multinomial Likelihood Function}
The multinomial likelihood function is
\[
    L\left(\theta_{1}, \theta_{2}, \ldots, \theta_{k}\right)=\frac{n}{y_{1} ! y_{2} ! \cdots y_{k} !} \theta_{1}^{y_{1}} \theta_{2}^{y_{2}} \cdots \theta_{k}^{y_{k}}
\]
or more simply
\[
    L\left(\theta_{1}, \theta_{2}, \ldots, \theta_{k}\right)=\theta_{1}^{y_{1}} \theta_{2}^{y_{2}} \ldots \theta_{k}^{y_{k}}=\prod_{j=1}^{k} \theta_{j}^{y_{j}}
\]
The maximum likelihood estimates are
\[
    \hat{\symbf{\theta}}_{j}=\frac{\symbf{y}_{j}}{n}, \quad j=1,2, \ldots, k
\]
and the maximum likelihood estimators are
\[
    \tilde{\theta}_{j}=\frac{Y_{j}}{n}, \quad j=1,2, \ldots, k
\]

\underline{Multinomial Likelihood Ratio Test Statistic}

For testing $H_{0}: \theta=\theta_{0}=\left(\frac{1}{k}, \frac{1}{k}, \ldots, \frac{1}{k}\right)$, we use
\[
    \Lambda\left(\theta_{0}\right)=-2 \log \left[\frac{L\left(\theta_{0}\right)}{\mathcal{L}(\tilde{\theta})}\right]=2 l(\tilde{\theta})-2 l\left(\theta_{0}\right)
\]
where
\[
    \tilde{\theta}=\left(\frac{Y_{1}}{n}, \frac{Y_{2}}{n}, \ldots, \frac{Y_{k}}{n}\right)
\]
Now
\[
    \begin{aligned}
        \frac{L\left(\theta_{0}\right)}{\mathcal{L}(\tilde{\theta})} & =\left[\prod_{j=1}^{k}\left(\frac{1}{k}\right)^{Y_{j}}\right] \div\left[\prod_{j=1}^{k}\left(\frac{Y_{j}}{n}\right)^{Y_{j}}\right]                                               \\
                                                           & =\prod_{j=1}^{k}\left(\frac{n / k}{Y_{j}}\right)^{Y_{j}}=\prod_{j=1}^{k}\left(\frac{E_{j}}{Y_{j}}\right)^{Y_{j}}, \quad \text{where} E_{j}=n\left(\frac{1}{k}\right)=\frac{n}{k}
    \end{aligned}
\]
Note that $Y_{j}$ is the observed number, and $E_{j}$ is the expected number of observations in category $j$ if $H_{0}$ is true.

Therefore, the likelihood ratio test statistic for testing $H_{0}: \theta=\theta_{0}=\left(\frac{1}{k}, \frac{1}{k}, \ldots, \frac{1}{k}\right)$ is
\[
    \begin{aligned}
        \Lambda\left(\theta_{0}\right) & =-2 \log \left[\frac{L\left(\theta_{0}\right)}{\mathcal{L}(\tilde{\theta})}\right]=-2 \log \left[\prod_{j=1}^{k}\left(\frac{E_{j}}{Y_{j}}\right)^{Y_{j}}\right] \\
                                       & =2 \sum_{j=1}^{k} Y_{j} \log \left(\frac{Y_{j}}{E_{j}}\right)                                                                                         \\
                                       & =2 \sum_{j=1}^{k}(\text{observed frequency }) \times \log \left(\frac{\text{ observed frequency }}{\text{ expected frequency}}\right)
    \end{aligned}
\]

Why does this test statistic make sense? When does it take on large values?

When does it take on small values?

\underline{Behaviour of the Multinomial Test Statistic}

If all of the observed frequencies (the $Y_{j}$'s) equal the expected frequencies
under $H_{0}$ (the $E_{j}$'s), then $\log \left(\frac{Y_{j}}{E_{j}}\right)=0$,
and $\Lambda\left(\theta_{0}\right)=0 $; otherwise, $ \Lambda\left(\theta_{0}\right)>0$.

So large values of $\Lambda\left(\theta_{0}\right)$ give evidence against $H_{0}$.

\subsection{Distribution of the Multinomial Likelihood Ratio Test Statistic}
\underline{Multinomial Likelihood Ratio Test Statistic}

The likelihood ratio test statistic for testing $H_{0}: \theta_{j}=\frac{1}{8}, j=1,2, \ldots, 8$ is
\[
    \begin{aligned}
        \Lambda\left(\theta_{0}\right) & =-2 \log \left[\frac{L\left(\theta_{0}\right)}{\mathcal{L}(\tilde{\theta})}\right]                          \\
                                       & =2 \sum_{j=1}^{8} Y_{j} \log \left(\frac{Y_{j}}{E_{j}}\right)                                     \\
                                       & =2 \sum_{j=1}^{8}(\text{observed frequency }) \times \log \left(\frac{\text{ observed frequency}}
        {\text{expected frequency}}\right)
    \end{aligned}
\]
where $Y_{j}$ is the observed number and $E_{j}$ is the expected number of observations
in category $j$ if $H_{0}$ is true.

\underline{Distribution of the Multinomial Likelihood Ratio Test Statistic}

Recall that if $\theta$ is a scalar, then $\Lambda\left(\theta_{0}\right)$ has approximately
a $\chi^{2}(1)$ distribution for large $n$ if $H_{0}: \theta=\theta_{0}$ is true.

If $\theta$ is a vector, then $\Lambda\left(\theta_{0}\right)$ still has approximately a $\chi^{2}$
distribution for large $n$ if $H_{0}: \theta=\theta_{0}$ is true, but the degrees of freedom change.

The degrees of freedom in the multiparameter case depend on both how many parameters are unknown in
the original model, and how many parameters must be estimated under the null hypothesis.


The multinomial likelihood function $L\left(\theta_{1}, \theta_{2}, \ldots, \theta_{8}\right)$ is
a function of $8-1=7$ parameters.

Under the hypothesis $H_{0}: \theta_{j}=\frac{1}{8}, j=1,2, \ldots, 8$, there are no
unspecified parameters and no parameters needed to be estimated.

Therefore, $\Lambda\left(\theta_{0}\right)$ has approximately a $\chi^{2}(7)$ distribution if
$H_{0}: \theta_{j}=\frac{1}{8} $, $ j=1,2, \ldots, 8$ is true.

\underline{Approximate $ p $-Value for Smarties Data}

The observed value of the likelihood ratio test statistic is
\[
    \begin{aligned}
        \lambda\left(\theta_{0}\right) & =2 \sum_{j=1}^{8} y_{j} \log \left(\frac{y_{j}}{e_{j}}\right), \quad \text{where} e_{j}=610\left(\frac{1}{8}\right)=76.25 \\
                                       & =2\left[80 \log \left(\frac{80}{76.25}\right)+\cdots+77 \log \left(\frac{77}{76.25}\right)\right]=35.00
    \end{aligned}
\]
The approximate $p$ -value is
\[
    \begin{aligned}
        p\text{-value}
         & \approx P(W \geq 35), \quad \text{where} W \sim \chi^{2}(7) \\
         & \approx 0
    \end{aligned}
\]
and there is very strong evidence based on the observed data against the hypothesis of
an equal number of each colour.

Note: The Chi-squared approximation is good when $n$ is large and the expected
frequencies under $H_{0}$ are all at least five.

\subsection{Pearson's Chi-Squared Goodness of Fit Statistic}

An alternative test statistic that was developed historically before the likelihood ratio
test statistic is the Pearson Goodness of Fit Statistic:
\[
    D=\sum_{j=1}^{k} \frac{\left(Y_{j}-E_{j}\right)^{2}}{E_{j}}
\]
Note that when $Y_{j}=E_{j}$ for all $j$, then $D=0$.
Otherwise, $D \geq 0$.

For large $ n $, $ D $ and $ \Lambda $ are asymptotically equivalent and have the
same asymptotic Chi-squared distribution.

The Pearson Goodness of Fit Test Statistic is more popular than the Likelihood Ratio Test.

\section{Goodness of Fit Examples}
\subsection{Checking the Fit of the Model}
In order to study a data set, we typically assume a model in which $\left(Y_{1}, Y_{2}, \ldots, Y_{n}\right)$ is a random sample from a distribution which is a member of the family of models
\[
    f(y ; \theta), \quad \text{for} \theta \in \Omega
\]
It is important to check that the model adequately represents the variability in the data.
This can be done by comparing the observed data $\left(y_{1}, y_{2}, \ldots, y_{n}\right)$ with what we would expect to get using the model.

One way to do this is to compare the observed frequencies based on the data with the expected frequencies calculated using probabilities from the assumed model
If the model is suitable, then the observed and expected frequencies should be ``close''.

\underline{Discrete Data Example: Alpha-Particle Emissions}

Recall a previous example regarding the data collected by the scientists, Rutherford and Geiger on the number of alpha-particles emitted during a fixed time interval.

We examined the fit of the Poisson model to these data by comparing the observed frequencies with the expected frequencies calculated assuming a Poisson model with mean equal to the sample mean.

\underline{Observed and Expected Frequencies Under Assumed Poisson Model}

\[ \begin{array}{|c|c|c|}
        \hline \begin{array}{c}
            \text{Number of} \\
            \text{Alpha-Particles Detected}
        \end{array} & \begin{array}{c}
            \text{Freq.} \\
            f_{j}
        \end{array} & \begin{array}{c}
            \text{Expected Freq.} \\
            e_{j}
        \end{array} \\
        \hline 0                          & 57                         & 54.42                      \\
        \hline 1                          & 203                        & 210.42                     \\
        \hline 2                          & 383                        & 407.43                     \\
        \hline 3                          & 525                        & 525.54                     \\
        \hline 4                          & 532                        & 508.41                     \\
        \hline 5                          & 408                        & 393.47                     \\
        \hline 6                          & 273                        & 253.77                     \\
        \hline 7                          & 139                        & 140.28                     \\
        \hline 8                          & 45                         & 67.86                      \\
        \hline 9                          & 27                         & 29.18                      \\
        \hline 10                         & 10                         & 11.29                      \\
        \hline 11+                        & 6                          & 5.77                       \\
        \hline \text{Total}               & 2608                       & 2607.99                    \\
        \hline
    \end{array} \]


We decided based on the frequencies in this table that a Poisson model fits these data
reasonably well.

The drawback of this method is we don't have a good way of deciding how ``close'' is good enough.

We will now see how to do a formal test of the hypothesis that the Poisson model fits these data.

\underline{Alpha-Particle Emissions: Test of Fit of Poisson Model}

Our model is Multinomial $\left(n ; \theta_{0}, \theta_{1}, \ldots, \theta_{11}\right)$, and $\sum_{j=0}^{11} \theta_{j}=1$, which is a function of 11 parameters.
$H_{0}$: Data fit a Poisson model or, more specifically,
\[
    H_{0}: \theta_{j}=\frac{\theta^{j} e^{-\theta}}{j !}, \quad j=0,1, \ldots, 10
\]
Under $H_{0}$, there is one unknown parameter $\theta$ which must be estimated.
Therefore, the degrees of freedom for the Chi-squared approximation for the likelihood ratio test statistic are $11-1=10$

Note that the expected frequencies are all at least five, so we can use the Chi-squared approximation to obtain the $p$ -value.
The observed value of the likelihood ratio statistic is
\[
    \begin{aligned}
        2 \sum_{j=0}^{11} f_{j} \log \left(\frac{f_{j}}{e_{j}}\right) & =2\left[57 \log \left(\frac{57}{54.42}\right)+\cdots+6 \log \left(\frac{6}{5.77}\right)\right] \\
                                                                      & =14.01
    \end{aligned}
\]
with
\[
    p \text{-value } \approx P(W \geq 14.01)=0.17, \text{ where} W \sim \chi^{2}(10)
\]
and there is no evidence against the Poisson model.

The observed value of the Pearson Goodness of Fit Statistic is
\[
    \sum_{j=0}^{11} \frac{\left(f_{j}-e_{j}\right)^{2}}{e_{j}}=12.98
\]
with
\[
    p \text{-value } \approx P(W \geq 12.98)=0.22, \quad \text{ where} W \sim \chi^{2}(10)
\]
and there is no evidence against the Poisson model.

\underline{What to Do if Expected Frequencies Are Not All at Least $ 5 $?}

Suppose we have the following table of observed frequencies and expected frequencies calculated
under the null hypothesis.

\[ \begin{array}{|c|c|c|}
        \hline \text{Category } & \text{ Observed Number } & \text{ Expected Number} \\
        \hline 1                & 53                       & 50                      \\
        \hline 2                & 21                       & 25                      \\
        \hline 3                & 11                       & 12.5                    \\
        \hline 4                & 8                        & 6.25                    \\
        \hline 5                & 4                        & 3.125                   \\
        \hline 6                & 3                        & 3.125                   \\
        \hline \text{Total}     & 100                      & 100                     \\
        \hline
    \end{array} \]

The last two categories have expected frequencies less than five, so it may not be
appropriate to use the Chi-squared approximation.

Usually we collapse two or more adjacent categories with the smallest expected frequencies.

\[ \begin{array}{|c|c|c|}
        \hline \text{Category} & \begin{array}{c}
            \text{Observed} \\
            \text{Number}
        \end{array} & \begin{array}{c}
            \text{Expected} \\
            \text{Number}
        \end{array} \\
        \hline 1               & 53                         & 50                         \\
        \hline 2               & 21                         & 25                         \\
        \hline 3               & 11                         & 12.5                       \\
        \hline 4               & 8                          & 6.25                       \\
        \hline \geqslant{5}    & 7                          & 6.25                       \\
        \hline \text{Total}    & 100                        & 100                        \\
        \hline
    \end{array} \]

\subsection{Two-Way Tables and Testing for Independence of Two Variates}

\[ \begin{array}{|c|c|c|c|}
        \hline \begin{array}{c}
            \text{Program/} \\
            \text{Hometown}
        \end{array} & \begin{array}{c}
            \text{Canadian} \\
            \text{Hometown}
        \end{array} & \begin{array}{c}
            \text{Non-Canadian} \\
            \text{Hometown}
        \end{array} & \text{Total} \\
        \hline \text{Computer Science}    & 33                         & 14                         & 47           \\
        \begin{array}{c}
            \text{Non-Computer} \\
            \text{Science}
        \end{array}        & 39                         & 46                         & 85           \\
        \hline \text{Total}               & 72                         & 60                         & 132          \\
        \hline
    \end{array} \]
Is there a relationship between a student's program and their hometown?

\underline{Relative Risk}

Previously, we summarized these data using relative risk as a numerical summary.
Proportion of Computer Science students with Canadian hometown:
\[
    \frac{33}{47}=0.7021
\]
Proportion of non-Computer Science students with Canadian hometown:
\[
    \frac{39}{85}=0.4588
\]
Relative risk $=\frac{0.7021}{0.4588}=1.53$
Students who have a Canadian hometown are about one and a half times more likely to be in a Computer Science program.

How could we test the hypothesis that a student's program is independent of their hometown?

\underline{Two-Way Tables and Testing for Independence of Two Variates}

Suppose $ n $ individuals are classified according to two different variates which have two possible values.

\[ \begin{array}{|c|c|c|c|}
        \hline       & B                   & \overline{B} & \text{Total}        \\
        A            & y_{11}              & y_{12}       & r_{1}=y_{11}+y_{12} \\
        \overline{A} & y_{21}              & y_{22}       & n-r_{1}             \\
        \text{Total} & c_{1}=y_{11}+y_{21} & n-c_{1}      & n                   \\
        \hline
    \end{array} \]

Let $Y_{11}=$ number of $A \cap B$ outcomes, $Y_{12}=$ number of $A \cap \overline{B}$ outcomes $Y_{21}=$ number of $\overline{A} \cap B$ outcomes, and $Y_{22}=$ number of $\overline{A} \cap \overline{B}$ outcomes.
Then
\[
    \left(Y_{11}, Y_{12}, Y_{21}, Y_{22}\right) \sim \text{Multinomial}\left(n, \theta_{11}, \theta_{12}, \theta_{21}, \theta_{22}\right)
\]
The null hypothesis is that the variates $A$ and $B$ are independent, or
\[
    H_{0}: P(A \cap B)=P(A) P(B)
\]
Let $P(A)=\alpha$ and $P(B)=\beta$, then the null hypothesis may be written as
\[
    H_{0}: \theta_{11}=\alpha \beta
\]

\underline{Likelihood and Maximum Likelihood Estimators}

Since $\left(Y_{11}, Y_{12}, Y_{21}, Y_{22}\right) \sim$ Multinomial $\left(n, \theta_{11}, \theta_{12}, \theta_{21}, \theta_{22}\right)$, the likelihood function is
\[
    L\left(\theta_{11}, \theta_{12}, \theta_{21}, \theta_{22}\right)=\frac{n}{y_{11} ! y_{12} ! y_{21} ! y_{22} !} \theta_{11}^{y_{11}} \cdot \theta_{12}^{y_{12}} \cdot \theta_{21}^{y_{21}} \cdot \theta_{22}^{y_{22}}
\]
or more simply
\[
    L\left(\theta_{11}, \theta_{12}, \theta_{21}, \theta_{22}\right)=\theta_{11}^{y_{11}} \cdot \theta_{12}^{y_{12}} \cdot \theta_{21}^{y_{21}} \cdot \theta_{22}^{y_{22}}
\]
The maximum likelihood estimates are
\[
    \hat{\theta}_{i j}=\frac{y_{i j}}{n}, \quad i=1,2 \text{and} j=1,2
\]
and the maximum likelihood estimators are
\[
    \tilde{\theta}_{i j}=\frac{Y_{i j}}{n}, \quad i=1,2 \text{and} j=1,2
\]

\subsection{Parameter Estimation under the Null Hypothesis}

If $H_{0}: \theta_{11}=\alpha \beta$ is true, then the likelihood function under $H_{0}$ is
\[
    \begin{aligned}
        \mathcal{L}(\alpha, \beta) & =(\alpha \beta)^{y_{11}}[\alpha(1-\beta)]^{y_{12}}[(1-\alpha) \beta]^{y_{21}}[(1-\alpha)(1-\beta)]^{y_{22}}                               \\
                         & =\alpha^{y_{11}+y_{12}}(1-\alpha)^{y_{21}+y_{22}} \cdot \beta^{y_{11}+y_{21}}(1-\beta)^{y_{12}+y_{22}}, \quad 0<\alpha<1, \quad 0<\beta<1
    \end{aligned}
\]
which is maximized for
\[
    \hat{\alpha}=\frac{y_{11}+y_{12}}{n}=\frac{r_{1}}{n} \quad \text{and} \quad \hat{\beta}=\frac{y_{11}+y_{21}}{n}=\frac{c_{1}}{n}
\]
The maximum likelihood estimators for $\alpha$ and $\beta$ are
\[
    \tilde{\alpha}=\frac{Y_{11}+Y_{12}}{n} \quad \text{and} \quad \tilde{\beta}=\frac{Y_{11}+Y_{21}}{n}
\]

Why do these estimates make sense?

\underline{Likelihood Ratio Test Statistic}

\[
    \begin{array}{c}
        -2 \log \left[\frac{\mathcal{L}(\tilde{\alpha}, \tilde{\beta})}{L\left(\tilde{\theta}_{11}, \tilde{\theta}_{12}, \tilde{\theta}_{21}, \tilde{\theta}_{22}\right)}\right] \\
        =2\left[Y_{11} \log \left(\frac{Y_{11}}{E_{11}}\right)+Y_{12} \log \left(\frac{Y_{12}}{E_{12}}\right)+Y_{21} \log \left(\frac{Y_{21}}{E_{21}}\right)+Y_{22} \log \left(\frac{Y_{22}}{E_{22}}\right)\right]
    \end{array}
\]
which is of the form
\[
    2 \sum_{j=1}^{k}(\text{observed frequency }) \times \log \left(\frac{\text{ observed frequency }}{\text{ expected frequency}}\right)
\]

\underline{Observed Likelihood Ratio Test Statistic}

The observed value of the likelihood ratio test statistic is
\[
    \lambda=2\left[y_{11} \log \left(\frac{y_{11}}{e_{11}}\right)+y_{12} \log \left(\frac{y_{12}}{e_{12}}\right)+y_{21} \log \left(\frac{y_{21}}{e_{21}}\right)+y_{22} \log \left(\frac{y_{22}}{e_{22}}\right)\right]
\]
Note that
\[
    e_{11}=n\left(\frac{r_{1}}{n}\right)\left(\frac{c_{1}}{n}\right)=\frac{r_{1} c_{1}}{n}
\]
and the other expected frequencies can be obtained by subtraction.

\underline{Two-Way Table: Observed and [Expected]}

\[ \begin{array}{|c|c|c|c|}
        \hline       & B                                           & \bar{B}                            & \text{Total}        \\
        \hline
                     & y_{11}                                      & y_{12}                             &                     \\
        A            & {\left[e_{11}=\frac{r_{1} c_{1}}{n}\right]} & {\left[e_{12}=r_{1}-e_{11}\right]} & r_{1}=y_{11}+y_{12} \\
        \hline A     & y_{21}                                      & y_{22}                             & n-r_{1}             \\
                     & {\left[e_{21}=c_{1}-e_{11}\right]}          & {\left[e_{22}=r_{2}-e_{21}\right]} &                     \\
        \text{Total} & c_{1}=y_{11}+y_{21}                         & n-c_{1}                            & n                   \\
        \hline
    \end{array} \]

\underline{Degrees of Freedom for the Chi-Squared Approximation}

What are the degrees of freedom for the Chi-squared approximation?
How many parameters were there in the original model?
\[
    \left(Y_{11}, Y_{12}, Y_{21}, Y_{22}\right) \sim \text{Multinomial}\left(n ; \theta_{11}, \theta_{12}, \theta_{21}, \theta_{22}\right)
\]
How many parameters under $H_{0}: \theta_{11}=\alpha \beta $?
Why do the degrees of freedom make sense?

\underline{Approximate $ p $-Value}

\[ \begin{aligned}
        p \text{-value } & \approx P(W \geq \lambda), \quad \text{ where} W \sim \chi^{2}(1) \\
                         & =2[1-P(Z \leq \sqrt{\lambda})], \quad \text{where} Z \sim G(0,1)
    \end{aligned} \]


\begin{Example}{Program/Hometown Example: Two-Way Table}{}
    \[ \begin{array}{|c|c|c|c|}
            \hline \begin{array}{c}
                \text{Program /} \\
                \text{Hometown}
            \end{array} & \text{Canadian Hometown}               & \begin{array}{c}
                \text{Non-Canadian} \\
                \text{Hometown}
            \end{array} & \text{Total} \\
            \hline
            \text{Computer Science}           & e_{11}=\frac{47 \times{72}}{132}=25.64 & e_{12}=47-25.64=21.36      & 47           \\
            \text{Non-Computer Science}       & e_{21}=72-25.64=46.36                  & e_{22}=60-21.36=38.64      & 85           \\
            \text{Total}                      & 72                                     & 60                         & 132          \\
            \hline
        \end{array} \]

    \underline{Program/Hometown Example: $ p $-Value}

    \[
        \begin{aligned}
            \lambda=2\left[33 \log \left(\frac{33}{25.64}\right)+39 \log \left(\frac{39}{46.36}\right)+14 \log \left(\frac{14}{21.36}\right)+46 \log \left(\frac{46}{38.64}\right)\right] \\
            =7.38                                                                                                                                                                         \\
            p \text{-value}=2[1-P(Z \leq \sqrt{7.38})]=2[1-P(Z \leq 2.72)] \approx 0.0066
        \end{aligned}
    \]
    There is strong evidence based on these observed data against the hypothesis that a student's program is independent of their hometown.
\end{Example}


