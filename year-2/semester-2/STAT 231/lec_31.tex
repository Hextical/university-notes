\section{2020-03-23: MLE Regression}
\underline{Roadmap}:
\begin{enumerate}[(i)]
    \item $ 5 $ min recap
    \item MLE for $ \alpha,\,\beta,\,\sigma $
    \item Least Squares
    \item Example
\end{enumerate}

\underline{Recap}:

General: $ Y \thicksim N(\mu(x),r(x)) $

\underline{Assumptions for the Simple Linear Regression Model}
(Gauss Markov Assumptions)
\begin{enumerate}[(i)]
    \item One covariate (for the time being)
    \item Normality: $ Y_i $'s are Normal
    \item Linearity: $ E(Y)=\alpha+\beta x $
    \item Independence: $ Y_i $'s are all independent
    \item Homoscedasticity: $ \sigma^2=\sigma^2(x)=\sigma^2 $ for all $ x $
\end{enumerate}
We call it a Simple since $ x $ is the only
explanatory variate. If we used more than one explanatory variate,
we call it a multi-variable regression (not covered in this course).

\underline{MLE Calculation}

\[ Y_i \thicksim N(\alpha+\beta x_i, \sigma^2) \]
for each $ i\in[1,n] $
independent. We can also write
\[ Y_i=(\alpha+\beta x_i)+R_i \]
where $ R_i \thicksim N(0,\sigma^2) $ and $ R_i $'s independent.
We say $ \alpha+\beta x_i $ is the systematic part, and $ R_i $ is the random part.
\[ f(y_i)=\frac{1}{\sqrt{2 \pi}}e^{-\frac{1}{2\sigma^2}(y_i-(\alpha+\beta x_i))^2}  \]
\[
    L(\alpha, \beta, \sigma)=
    \frac{1}{(2 \pi)^{n / 2} \sigma^{n}}
    e^{-\frac{1}{2 \sigma^{2}}
            \sum\left[y_{i}-\left(\alpha+\beta x_i\right)\right]^{2}}
\]
so,
\[ \ell(\alpha,\beta,\sigma)=-\frac{n}{2} \ln(2\pi)-n\ln(\sigma)-
    \frac{1}{2\sigma^2}\sum\left[y_{i}-\left(\alpha+\beta x_i\right)\right]^{2}  \]
\[ \frac{\partial\ell}{\partial\alpha}=0\implies
    \hat{\alpha}=\overline{y}-\hat{B}\overline{x} \]
\[ \frac{\partial \ell}{\partial \beta}=0\implies
    \hat{\beta}=\frac{S_{xy}}{S_{xx}}=\frac{\sum(x_i-\overline{x})(y_i-\overline{y})}
    {\sum (x_i-\overline{x})^2}  \]
\[ \frac{\partial\ell}{\partial \sigma}=0\implies
    \hat{\sigma}^2=\frac{1}{n} \sum \left[ y_i-(\hat{\alpha}+\hat{\beta}x_i) \right]^2 \]

\begin{exbox}
    \begin{example}[Numerical Example]
        \[
            \begin{array}{|c|c|c|c|c|c|c|}
                \hline
                x & y & x-\overline{x} & y-\overline{y} & (x-\overline{x})^2 & (y-\overline{y})^2 & (x-\overline{x})(y-\overline{y}) \\
                \hline
                1 & 2 & -4             & -4             & 16                 & 16                 & 16                               \\
                3 & 3 & -2             & -3             & 4                  & 9                  & 6                                \\
                5 & 7 & 0              & 1              & 0                  & 1                  & 0                                \\
                7 & 9 & 2              & 3              & 4                  & 9                  & 6                                \\
                9 & 9 & 4              & 3              & 16                 & 16                 & 12                               \\
                \hline
                  &   & 0              & 0              & S_{xx}=40          & S_{yy}             & S_{xy}= 40                       \\
                \hline
            \end{array}
        \]
        \begin{itemize}
            \item $ \overline{x} = 5 $
            \item $ \overline{y} = 6 $
        \end{itemize}
        Find the regression equation.

        \textbf{Solution.}
        \[ \hat{\beta}=\frac{S_{xy}}{S_{xx}}=40/40=1  \]
        \[ \hat{\alpha}=\overline{y}-\hat{\beta}\overline{x}=6-(1)(5)=1 \]
        Thus, the regression equation is:
        \[ y=\hat{\alpha}+\hat{\beta}x=1+x \]
    \end{example}
\end{exbox}

\underline{Method of Least Squares}
\[ \text{minimize}\quad \sum\limits_{i=1}^{n} \left[ y_i-(\hat{\alpha}+\hat{\beta}x_i) \right]^2 \]
This is exactly the same as what we did previously. Sometimes we call
$ \hat{\alpha} $ and $ \hat{\beta} $ least square estimates.

\section{2020-03-23: Beta Properties and a Look Ahead}
\underline{Roadmap}:
\begin{enumerate}[(i)]
    \item Interpretation of SLRM and Recap
    \item An example
    \item Possible Questions
\end{enumerate}
\underline{What we know so far}:
\begin{itemize}
    \item $ Y_i = $ response variate $ = $ random variable where $ i=1,\ldots ,n $
    \item $ x_i = $ explanatory variable $ = $ given (known numbers)
\end{itemize}
Examples:
\begin{itemize}
    \item $ Y_i = $ STAT 231, $ x= $ STAT 230
    \item $ Y_i= $ stock price in month $ i $, $ x=P/E $
    \item $ Y_i= $ wage of UW graduate, $ x= $ major
\end{itemize}

\underline{Model}: $ Y_i  \thicksim N(\alpha+\beta x_i, \sigma^2) $ $ i\in[1,n] $ independent.
\[ Y_i=\alpha+\beta x_i+R_i \]
$ R_i =  $ residuals and $ R_i \thicksim N(0,\sigma^2) $.

\underline{Goal}: Extract the relationship between $ x $ and $ Y $.

\underline{Interpretation}:

$ E(Y_i)=\alpha+\beta x_i+0 $

$ \beta = $ change in $ E(Y) $ if $ x $ changes by $ 1 $ unit

Suppose $ x=0 $, then $ Y_i=\alpha+R_i $. So $ E(Y_i)=\alpha $.

\begin{exbox}
    \begin{example} $ \; $
        \begin{itemize}
            \item $ n=30 $
            \item $ \overline{x}=76.733 $
            \item $ \overline{y}=72.233 $
            \item $ S_{yy}=7585.3667 $
            \item $ S_{xx}=5135.8667 $
            \item $ S_{xy}=5106.8667 $
        \end{itemize}
        Find the regression equation.

        \textbf{Solution.}
        \begin{itemize}
            \item $ \hat{\beta}=\frac{S_{xy}}{S_{xx}}=\frac{5106.8667}{5135.8667}=0.9944 $
            \item $ \hat{\alpha}=\overline{y}-\hat{B}\overline{x}=72.233-(0.9944)(76.733)=-4.0677 $
        \end{itemize}
        Thus, the regression equation is:
        \[ y=-4.0677+0.9944x \]
    \end{example}
\end{exbox}
\underline{Note}: Suppose we have the data set $ \{(x_1,y_1),\ldots ,(x_{30},y_{30})\} $.
If $ x_{15}=75 $, we can predict $ y_{15} $ using the regression equation.
However, it may or may not lie on the line.

Given $ y=-4.0677+0.9944x  $., suppose $ \beta=0 $, this means that $ x $ has no effect on $ Y_i $ since
\[ Y_i \thicksim N(\alpha,\sigma^2) \]
\underline{Exercise}: $ \hat{\beta}=0\iff r_{xy}=0 $?

We could also figure out the following (next lecture):
\begin{itemize}
    \item Confidence interval for $ \beta $
    \item $ H_0 $: $ \beta=0 $ ($ x $ is uncorrelated to $ Y $)
    \item $ H_1 $: $ \beta\neq 0 $
\end{itemize}
