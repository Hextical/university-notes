\section{2020-01-31}
\underline{Roadmap}:
\begin{itemize}
    \item 5 min recap
    \item Likelihood function for multinomial
    \item Testing for the model
          \begin{itemize}
              \item Observed vs Expected frequencies
          \end{itemize}
    \item Likelihood function and the MLE for the uniform distribution
\end{itemize}


\begin{Example}{}{}
    The MLE of $ \theta $ for
    \[ f(y;\theta)=\frac{1}{\theta}e^{-y/\theta}  \]
    is $ \hat{\theta}=\bar{y} $. Find the corresponding MLE for
    $ \lambda $ for
    \[ f(y;\lambda)=\lambda e^{-\lambda y}. \]

    \textbf{Solution.} Since $ \lambda=\frac{1}{\theta} $, we have
    \[ \hat{\theta}=\bar{y}\implies \frac{1}{\lambda}=\bar{y} \]
    by the invariance property.
    Thus, the MLE for $ \lambda $ is
    \[ \hat{\lambda}=\frac{1}{\bar{y}}. \]
\end{Example}



\begin{Example}{}{}
    Suppose $ 4 $ people $ (A,B,C,D) $ run a 100 meter race every week.
    Let $ \theta_i $ be the probability person $ i $ wins a race for
    $ i\in\left\{ A,B,C,D\right\} $.
    Suppose also the following data is given to us.
    \begin{itemize}
        \item $ n=20 $
        \item $ y_A=8 $
        \item $ y_B=6 $
        \item $ y_C=4 $
        \item $ y_D=2 $
    \end{itemize}
    \underline{Model}: $ Y \sim \mult(n,\theta_A,\ldots ,\theta_D) $

    \textbf{Questions}:
    \begin{enumerate}[label=(\alph*)]
        \item What is the likelihood function?
        \item What are the MLEs?
    \end{enumerate}
    The likelihood function is given by
    \[ L(\theta_A,\ldots ,\theta_D)=\frac{20!}{8!6!4!2!} \theta_A^8\theta_B^6\theta_C^4\theta_D^2 \]
    Intuitively, the MLEs are given by
    \begin{itemize}
        \item $ \hat{\theta}_A=\frac{8}{20} $
        \item $ \hat{\theta}_B=\frac{6}{20} $
        \item $ \hat{\theta}_C=\frac{4}{20} $
        \item $ \hat{\theta}_D=\frac{2}{20} $
    \end{itemize}
\end{Example}


The Multinomial joint probability function is
\[ f(y_1,\ldots ,y_k;\symbf{\theta})=\frac{n!}{y_1!\cdots y_k!}\prod_{i=1}^k \theta_i^{y_i} \]
for $ y_i=0,1,\ldots $ where $ \sum\limits_{i=1}^{k} y_i = n $.
The likelihood function for $ \symbf{\theta}=\symbf(\theta_1,\ldots ,\theta_k) $ based on data
$ y_1,\ldots ,y_k $ is given by
\[ L(\symbf{\theta})=L(\theta_1,\ldots ,\theta_k)=\frac{n!}{y_1!\cdots y_k!} \prod_{i=1}^k
    \theta_i^{y_i} \]
or more simply
\[ L(\symbf{\theta})=\prod_{i=1}^k \theta_i^{y_i} \]
The log likelihood is
\[ \ell(\symbf{\theta})=\sum\limits_{i=1}^{k} \left[ y_i\ln(\theta_i) \right] \]
If $ y_i $ represents the number of times outcome $ i $ occurred in the $ n $ ``trials''
for $ i=1,\ldots ,k $, then the following result holds.

\begin{Proposition}{}{}
    Suppose $ Y \sim \mult(n,\theta_1,\ldots ,\theta_k) $, then the MLE for
    $ \symbf{\theta}=(\theta_1,\ldots ,\theta_k) $ is
    \[ \hat{\theta}_i=\frac{y_i}{n} \]
    for $ i=1,\ldots ,k $.
\end{Proposition}

\begin{Proof}{}{}
    Use Lagrange multiplier method for $ \ell(\symbf{\theta}) $ satisfying the linear
    constraint $ \sum\limits_{i=1}^{k} \theta_i=1 $.
\end{Proof}


\begin{Example}{}{}
    Let $ Y $ be a discrete random variable taking values in $ \{0,1,2,3\} $ and
    \[ P(Y=0)=\theta^3,\; P(Y=1)=3\theta(1-\theta)^2,\;P(Y=2)=3\theta^2(1-\theta),\;
        P(Y=3)=(1-\theta)^3 \]
    where $ \theta $ is an unknown parameter, with $ 0<\theta<1 $.
    We make a table of $ 80 $ independent observations from the distribution above.
    \[
        \begin{array}{c|c}
            Y & \text{Observed Frequency} \\
            \hline
            0 & 10                        \\
            1 & 30                        \\
            2 & 30                        \\
            3 & 10
        \end{array}
    \]
    (a) Determine the likelihood function, $ L(\theta) $.

    \textbf{Solution.}
    \begin{align*}
        L(\theta)
         & =\left( \theta^3 \right)^{10}\left[ 3\theta(1-\theta)^2 \right]^{30}
        \left[ 3\theta^2(1-\theta) \right]^{30}\left[ (1-\theta)^3 \right]^{10}                        \\
         & =3^{30}3^{30}\theta^{30}\theta^{30}\theta^{60}(1-\theta)^{60}(1-\theta)^{30}(1-\theta)^{30} \\
         & =3^{30}3^{30}\theta^{120}(1-\theta)^{120}
    \end{align*}
    or more simply
    \[ L(\theta)=\theta^{120}(1-\theta)^{120} \]

    (b) Determine the log likelihood function, $ \ell(\theta) $.

    \textbf{Solution.}
    \[ \ell(\theta)=120\ln(\theta)+120\ln(1-\theta) \]
    or more simply
    \[ \ell(\theta)=\ln(\theta)+\ln(1-\theta) \]

    (c) Using the function $ \ell(\theta) $ in (b) in order to derive the maximum
    likelihood estimate of $ \theta $.

    \textbf{Solution.}
    \[ \frac{d\ell}{d\theta}=\frac{1}{\theta}-\frac{1}{1-\theta}=\frac{1-2\theta}{\theta(1-\theta)}:=0 \]
    \[ \implies \hat{\theta}=\frac{1}{2}=0.5 \]
\end{Example}



\begin{Example}{Using the likelihood functions to test models}{}
    Suppose $ W_1,\ldots ,W_n $ are iid. We collect data $ \symbf{w}=(w_1,\ldots ,w_n) $.

    \underline{Model}: $ W_i \sim \poi(\theta) $
    \[
        \begin{array}{c|c|c|}
            W           & \text{Observed Frequency} & \text{Expected Frequency} \\
            \hline
            0           & y_0                       & e_1                       \\
            1           & y_1                       & e_2                       \\
            2           & y_2                       & e_3                       \\
            3           & y_3                       & e_4                       \\
            4           & y_4                       & e_5                       \\
            \geqslant 5 & y_5                       & e_6
        \end{array}
    \]
    To calculate the expected $ e_i $'s we use the formula
    \[ e_i=n\cdot p_i \]
    where
    \[ p_i=P(Y=i). \]
    for $ i\in[0,4] $ where $ n $ is the total number of observations (observed frequencies summed).
    For example, $ e_i $ would be the following.
    \[ e_i=n\cdot \left( \frac{e^{-\hat{\theta}}\cdot\hat{\theta}^{i}}{i!} \right) \]
    for $ j\in[0,4] $. Note that $ \hat{\theta}=\bar{y} $.
    To estimate $ e_5 $, we write
    \[ e_5=n\cdot P(Y\geqslant 5)=n\cdot \left( 1-\sum\limits_{i=0}^{4}P(Y=i) \right) \]
    Then, we compare the observed frequencies to the expected frequencies.
\end{Example}

