\chapter{Generalized Linear Models}
\makeheading{Lecture 21 | 2020-11-23}
\section{Beyond STAT 331:
  Generalized Linear Models and Logistic Regression}
We have a vector of responses $ \symbf{Y} $
which are independent, and predictors
$ \symbf{x}_1,\ldots,\symbf{x}_p $.
\begin{Definition}{Generalized Linear Model (GLM)}{}
    Three ingredients of a GLM\@:
    \begin{enumerate}
        \item Random component. Response $ Y_i $ is a random
              variable with a distribution that is a member of the
              \emph{exponential family}, e.g., Normal,
              Binomial, Poisson.
        \item Systematic component. Typically, a linear predictor
              based on $ x_{i1},\ldots,x_{ip} $ which we denote as
              \[ \eta_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p} \]
        \item Link function: A function $ g(\cdot) $ that defines
              a relationship between $ \E{Y_i} $ and $ \eta_i $;
              that is, $ \eta_i=g(\E{Y_i}) $.
    \end{enumerate}
\end{Definition}
\begin{Example}{Multiple Linear Regression}{}
    MLR\@: $ Y_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p}+\varepsilon $
    where $ \varepsilon_i\stackrel{\text{iid}}{\sim}\N{0,\sigma^2} $
    \begin{itemize}
        \item Random component: $ Y_i $ is Normal and $ \E{Y_i}=\eta_i $.
        \item Link function: $ g(\E{Y_i})=\E{Y_i} $ (identity).
    \end{itemize}
\end{Example}
\begin{Example}{Logistic Regression}{}
    Logistic regression: can be used when response $ Y_i $ is
    binary; that is,
    \[ Y_i =\begin{cases*}
            1 & success/on/yes/goose     \\
            0 & failure/off/no/non-goose
        \end{cases*} \]
    \begin{itemize}
        \item Random component: $ \Prob{Y_i=1}=\pi_i $ and
              $ \Prob{Y_i=0}=1-\pi_i $; that is,
              $ Y_i \sim \bin{1,\pi_i} $.
        \item Link function for logistic regression sets
              \[ \ln\biggl(\frac{\pi_i}{1-\pi_i} \biggr)=
                  \beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p}=\eta_i \]
    \end{itemize}
    Note: $ \E{Y_i}=1(\pi_i)+0(1-\pi_i)=\pi_i $ so logistic regression
    takes $ \displaystyle g(\pi_i)=\ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr) $.
    Say $ A $ is an event, then the \emph{odds} of event $ A $ is defined as
    \[ \frac{\Prob{A}}{1-\Prob{A}} \]
    Therefore, $ \displaystyle \frac{\pi_i}{1-\pi_i} $ is the odds that
    $ Y_i=1 $ and $ \displaystyle \ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr) $
    is the log-odds that $ Y_i=1 $. Since
    $ \displaystyle \eta_i=\ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr) $,
    inverting this function gives
    \[ e^{\eta_i}=\frac{\pi_i}{1-\pi_i}\iff \pi_i=\frac{e^{\eta_i}}{1+e^{\eta_i}} \]
    Plot $ \displaystyle y=\frac{e^x}{1+e^x} $.

    Note that $ \pi_i $ is bounded by $ 0 $ to $ 1 $, while
    $ \displaystyle \ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr) $
    can take any real number.
    \begin{itemize}
        \item Since $ \displaystyle \ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr)=
                  \beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p} $
              and $ \Prob{Y_i=1}=\pi_i $, then $ \beta_j $ is
              the expected \emph{additive change} in the log-odds of the event that $ Y_i=1 $
              for a unit increase in $ x_j $ (holding other variables constant).
              Also,
              \[ \frac{\pi_i}{1-\pi_i}=e^{\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p}}  \]
              would say that $ e^{\beta_j} $ is the expected \emph{multiplicative change}
              in odds of $ Y_i=1 $ or a unit increase in $ x_j $ (holding other variables constant).
        \item Model is fit using MLE (\texttt{glm()} function in R).
    \end{itemize}
\end{Example}
\begin{Example}{Logistic Regression (Music Analysis)---String Quartet Classification}{}
    Define
    \[ Y_i=\begin{cases*}
            1 & Haydn  \\
            0 & Mozart
        \end{cases*} \]
    The model (for illustration purposes, the actual model
    had $ 7 $ predictors) is given by:
    \[ \ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr)=\beta_0+\beta_1x_{i1}+\beta_2x_{i2} \]
    From training data (observation with $ Y_i $'s known and predictors given)
    we have
    \[ \symbf{\hat{\beta}}=(-1.12,-15.47,16.71)^\top \]
    Also,
    \begin{itemize}
        \item $ x_1= $ SD of note duration in 1st violin
              where $ x_1\in(0.05, 0.35) $.
        \item $ x_2= $ proportion descending pairwise intervals
              in 1st violin
              where $ x_2\in(0.2,0.5) $.
    \end{itemize}
    \begin{itemize}
        \item Higher values of $ x_1 $ are more likely to be Mozart
        \item Higher values of $ x_2 $ are more likely to be Haydn
    \end{itemize}
    Suppose we are given
    $ \symbf{x}_1=(0.05,0.2,0.35)^\top $ and $ \symbf{x}_2=(0.4, 0.4, 0.4)^\top $,
    then the higher estimated probability is Mozart since $ \hat{\beta}_1 $
    is negative.

    Estimate log odds of $ Y_i=1 $ is
    \[ -1.12-15.47(0.05)+16.71(0.4)=\hat{\eta}_i \]
    Estimate $ \pi_i $ is
    \[ \frac{e^{\hat{\eta}_i}}{1+e^{\hat{\eta}_i}}=0.992  \]
    which is the estimate probability an observation
    is Haydn for $ x_1=0.05 $ and $ x_2=0.4 $.
    \begin{itemize}
        \item Second probability: 0.992
        \item Third probability: 0.537
    \end{itemize}
    Interpretation for $ \hat{\beta}_1 $:
    \begin{itemize}
        \item For unit increase in SD of note duration in 1st violin,
              estimated expected change in log-odds of the piece being Haydn
              is -15.47.
    \end{itemize}
\end{Example}
