\makeheading{Lecture 21 | 2020-11-23}
\section{Beyond STAT 331:
  Generalized Linear Models and Logistic Regression}
We have a vector of responses $ \symbf{Y} $
which are independent, and predictors
$ \symbf{x}_1,\ldots,\symbf{x}_p $.

Three ingredients of a GLM\@:
\begin{enumerate}
    \item Random component. Response $ Y_i $ is a random
          variable with a distribution that is a member of the
          \emph{exponential family}, e.g.\ Normal,
          Binomial, Poisson.
    \item Systematic component. Typically, a linear predictor
          based on $ x_{i1},\ldots,x_{ip} $. Let's denote
          \[ \eta_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p} \]
    \item Link function: A function $ g(\cdot) $ that defines
          a relationship between $ \E{Y_i} $ and $ \eta_i $;
          that is, $ \eta_i=g(\E{Y_i}) $.
\end{enumerate}
\begin{Example}{Multiple Linear Regression}{}
    MLR\@: $ Y_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p}+\varepsilon $
    where $ \varepsilon_i\stackrel{\text{iid}}{\sim}\N{0,\sigma^2} $
    \begin{itemize}
        \item Random component: $ Y_i $ is Normal and $ \E{Y_i}=\eta_i $.
        \item Link function: $ g(\E{Y_i})=\E{Y_i} $ (identity).
    \end{itemize}
\end{Example}
\begin{Example}{Logistic Regression}{}
    Logistic regression: can be used when response $ Y_i $ is
    binary; that is,
    \[ Y_i =\begin{cases*}
            1 & success/on/yes/goose\\
            0 & failure/off/no/non-goose
        \end{cases*} \]
    \begin{itemize}
        \item Random component: $ \Prob{Y_i=1}=\pi_i $ and
              $ \Prob{Y_i=0}=1-\pi_i $; that is,
              $ Y_i \sim \bin{1,\pi_i} $.
        \item Link function for logistic regression sets
              \[ \ln\biggl(\frac{\pi_i}{1-\pi_i} \biggr)=
                  \beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p}=\eta_i \]
    \end{itemize}
    Note: $ \E{Y_i}=1(\pi_i)+0(1-\pi_i)=\pi_i $ so logistic regression
    takes $ \displaystyle g(\pi_i)=\ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr) $.
    Say $ A $ is an event, then the \emph{odds} of event $ A $ is defined as
    \[ \frac{\Prob{A}}{1-\Prob{A}} \]
    Therefore, $ \displaystyle \frac{\pi_i}{1-\pi_i} $ is the odds that
    $ Y_i=1 $ and $ \displaystyle \ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr) $
    is the log-odds that $ Y_i=1 $. Since
    $ \displaystyle \eta_i=\ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr) $,
    inverting this function gives
    \[ e^{\eta_i}=\frac{\pi_i}{1-\pi_i}\iff \pi_i=\frac{e^{\eta_i}}{1+e^{\eta_i}} \]
    Plot $ \displaystyle y=\frac{e^x}{1+e^x} $.

    Note that $ \pi_i $ is bounded by $ 0 $ to $ 1 $, while
    $ \displaystyle \ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr) $
    can take any real number.
    \begin{itemize}
        \item Since $ \displaystyle \ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr)=
                  \beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p} $
              and $ \Prob{Y_i=1}=\pi_i $, then $ \beta_j $ is
              the expected \emph{additive change} in the log-odds of the event that $ Y_i=1 $
              for a unit increase in $ x_j $ (holding other variables constant).
              Also,
              \[ \frac{\pi_i}{1-\pi_i}=e^{\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{i p}}  \]
              would say that $ e^{\beta_j} $ is the expected \emph{multiplicative change}
              in odds of $ Y_i=1 $ or a unit increase in $ x_j $ (holding other variables constant).
        \item Model is fit using MLE (glm function in R).
    \end{itemize}
\end{Example}
\begin{Example}{Numerical Example of Logistic Regression (Music)}{}
    Music analysis:
    \[ Y_i=\begin{cases*}
            1 & Haydn\\
            0 & Mozart
        \end{cases*} \]
    String quartet classification. Model (for illustration, the actual model
    had $ 7 $ predictors):
    \[ \ln\biggl(\frac{\pi_i}{1-\pi_i}\biggr)=\beta_0+\beta_1x_{i1}+\beta_2x_{i2} \]
    From training data (observation with $ Y_i $'s known and predictors given)
    MLEs are:
    \begin{itemize}
        \item $ \hat{\beta}_0=-1.12 $
        \item $ \hat{\beta}_1=-15.47 $
        \item $ \hat{\beta}_2=16.71 $
    \end{itemize}
    Also,
    \begin{itemize}
        \item $ x_1= $ SD of note duration in 1st violin
        \item $ x_2= $ proportion descending pairwise intervals
              in 1st violin
    \end{itemize}
    Range of predictor values
    \begin{itemize}
        \item 0.05 to 0.35
        \item 0.2 to 0.5
    \end{itemize}
    \begin{itemize}
        \item Higher values of $ x_1 $ are more likely to be Mozart
        \item Higher values of $ x_2 $ are more likely to be Haydn
    \end{itemize}
    If $ x_1 $ is 0.05; 0.2; 0.35 and $ x_2 $ is 0.4; 0.4; 0.4.
    Higher estimated probability is Mozart since $ \hat{\beta}_1 $
    is negative.

    Estimate log odds of $ Y_i=1 $ is
    \[ -1.12-15.47(0.05)+16.71(0.4)=\hat{\eta}_i \]
    Estimate $ \pi_i $ is
    \[ \frac{e^{\hat{\eta}_i}}{1+e^{\hat{\eta}_i}}=0.992  \]
    Estimate probability is Haydn for $ x_1=0.05 $ and $ x_2=0.4 $.
    \begin{itemize}
        \item Second probability: 0.992
        \item Third probability: 0.537
    \end{itemize}
    Interpretation for $ \hat{\beta}_1 $:
    \begin{itemize}
        \item For unit increase in SD of note duration in 1st violin,
              estimated expected change in log-odds of the piece being Haydn
              is -15.47.
    \end{itemize}
\end{Example}
