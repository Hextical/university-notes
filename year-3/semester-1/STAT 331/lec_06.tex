\makeheading{Lecture 6 | 2020-09-23}
\section{Inference}
Recall that last lecture, for multiple linear regression,
we have $ \symbf{Y}=X\symbf{B}+\symbf{\varepsilon} $
with the assumption that $ \varepsilon\stackrel{\text{iid}}{\sim}\N{0,\sigma^2} $.
Therefore, for a random vector $ \symbf{\varepsilon} $, we have
\[ \symbf{\varepsilon}\sim\Mvn*{
    \begin{bmatrix}
      0      \\
      0      \\
      \vdots \\
      0      \\
      0
    \end{bmatrix},
    \begin{bmatrix}
      \sigma^2 & 0        & \cdots & 0        & 0        \\
      0        & \sigma^2 & \cdots & 0        & 0        \\
      \vdots   & \vdots   & \ddots & \vdots   & \vdots   \\
      0        & 0        & \cdots & \sigma^2 & 0        \\
      0        & 0        & \cdots & 0        & \sigma^2 \\
    \end{bmatrix}}=
  \Mvn{\symbf{0}_{n\times 1},\sigma^2I_{n\times n}} \]
since $ \Cov{\varepsilon_i,\varepsilon_j}=0 $ for all $ i\ne j $ due to independence.

Thus, $ \symbf{Y}\sim \text{MVN}(X\symbf{B},\sigma^2 I) $.
\begin{Definition}{Least squares for MLR}{}
  We define the \textbf{least squares for a multiple linear regression model}
  as
  \[ S(\beta_0,\beta_1,\ldots,\beta_p)
    =\sum\limits_{i=1}^{n}
    (y_i-(\Uunderbracket{
      \beta_0+\beta_1 x_{i1}+\cdots+\beta_p x_{ip}
    }_{\E{Y_i}=\mu_i}))^2
  \]
\end{Definition}
\begin{Theorem}{Least Square Estimates (LSEs) for MLR}{lse_mlr}
  Minimizing $ S(\beta_0,\beta_1,\ldots,\beta_p) $, gives the least squares
  estimate $ \hat{\symbf{\beta}}=(X^\top X)^{-1}X^\top\symbf{y} $.
\end{Theorem}
\begin{Proof}{\ref{thm:lse_mlr}}{}
  The first partial is $ \frac{\partial S}{\partial \beta_0}=\sum\limits_{i=1}^{n} 2(y_i-\mu_u)(-1) $,
  and all other partials for $ j=1,\ldots,p $ are
  \[ \dfrac{\partial S}{\partial \beta_j}=
    \sum\limits_{i=1}^{n} 2(y_i-\mu_i)(-x_{i j}) \]
  Set $ \displaystyle \frac{\partial S}{\partial \beta_0}=0 $
  and $ \displaystyle \frac{\partial S}{\partial \beta_j}=0 $ for $ j=1,\ldots,p $
  to get
  \[ \begin{dcases}
      \sum\limits_{i=1}^{n} (y_i-\mu_i)=0\iff \symbf{1}^\top(\symbf{y}-\symbf{\mu})=0 \\
      \sum\limits_{i=1}^{n} (y_i-\mu_i)x_{i j}=0\iff \symbf{x}_{j}^\top
      (\symbf{y}-\symbf{\mu})=0 & j=1,\ldots,p
    \end{dcases} \]
  since we recall that
  \[ X=\begin{bmatrix}
      1      & x_{11} & \cdots & x_{1(p-1)} & x_{1p}  \\
      \vdots & \vdots & \ddots & \vdots     & \vdots  \\
      1      & x_{n1} & \cdots & x_{n(p-1)} & x_{n p}
    \end{bmatrix}=
    \begin{bmatrix}
      \symbf{1} & \symbf{x}_1 & \cdots & \symbf{x}_{p-1} & \symbf{x}_p
    \end{bmatrix} \]
  Therefore,
  \[ X^\top(\symbf{y}-X\symbf{\beta})=0\iff
    X^\top \symbf{y}-X^\top X\symbf{\beta}=0\iff
    X^\top X \symbf{\beta}=X^\top \symbf{y}\iff
    \symbf{\beta}=(X^\top X)^{-1}X^\top \symbf{y} \]
  assuming $ X^\top X $ is invertible; that is,
  $ \rank(X^\top X)=p+1 $. So, the LS solution for $ \symbf{\beta} $ is given by
  $ \hat{\symbf{\beta}}=(X^\top X)^{-1}X^\top \symbf{y} $.
\end{Proof}
\begin{Definition}{Residuals for MLR}{}
  The \textbf{residuals} for a multiple linear regression model is defined
  as
  \[ e_i=y_i-(\Uunderbracket{\hat{\beta}_0+\hat{\beta}_1x_{i1}+\cdots\hat{\beta}_p x_{ip}
    }_{\text{fitted value }\mu_i}) \]
  or equivalently, $ \hat{\symbf{\mu}}=X\hat{\symbf{\beta}} $ and
  $ \symbf{e}=\symbf{y}-\hat{\symbf{\mu}} $.
\end{Definition}
The estimate $ \sigma^2 $ based on $ e_i $'s is
\[ \hat{\sigma}^2=\frac{\SS{Res}}{n-(p+1)}=
  \frac{\sum_{i=1}^{n} e_i^2}{n-p-1}=\frac{\symbf{e}^\top\symbf{e}}{n-p-1}  \]
since d.f.\ is $ n-(\text{number of estimated parameters}) $. When viewed
as a random variable,
\[ \frac{(n-p-1)\hat{\sigma}^2}{\sigma^2}\sim \chi^2(n-p-1)  \]
Inference for $ \hat{\symbf{\beta}}=(\hat{\beta}_0,\ldots,\hat{\beta}_p)^\top
  =(X^\top X)^{-1}X^\top \symbf{Y} $.

Note that $ \hat{\symbf{\beta}} $ is a matrix of constants and
$ \symbf{Y} $ is a random vector, and
$ \symbf{Y}\sim \text{MVN}(X\symbf{\beta},\sigma^2 I) $, so
\begin{align*}
  \E{\hat{\symbf{\beta}}}
   & =\E{(X^\top X)^{-1}X^\top \symbf{Y}}    \\
   & =(X^\top X)^{-1}X^\top\E{\symbf{Y}}     \\
   & =(X^\top X)^{-1}(X^\top X)\symbf{\beta} \\
   & =\symbf{\beta}
\end{align*}
That is, $ \E{\hat{\beta}_0},\ldots,\E{\hat{\beta}_p}=\beta_p $
all unbiased.
\begin{align*}
  \Var{(X^\top X)^{-1}X^\top\symbf{Y}}
   & =(X^\top X)^{-1}X^{\top}\Var{\symbf{Y}}\bigl[ (X^\top X)^{-1}X^\top \bigr]^\top                              \\
   & =(X^\top X)^{-1}X^\top \sigma^2 I(X^\top)^\top\bigl[ (X^\top X)^{-1} \bigr]^\top & X^\top X\text{ symmetric} \\
   & =\sigma^2(X^\top X)^{-1}(X^\top X)(X^\top X)^{-1}
\end{align*}
Since $ \hat{\symbf{\beta}} $ is a linear transformation of $ \symbf{Y} $
we have
$ \hat{\symbf{\beta}} \sim \text{MVN}(\symbf{\beta},\sigma^2
  \Uunderbracket{(X^\top X)^{-1}}_{V}) $. We proved the following theorem.
\begin{Theorem}{Distribution of $ \hat{\beta}_j $}{}
  The distribution of a given $ \hat{\beta}_j $ is
  \[ \hat{\beta}_j \sim \N{\beta_j,\sigma^2 V_{jj}}\qquad j=0,1,\ldots,p \]
\end{Theorem}
\[ Z=\frac{\hat{\beta}_j-\beta_j}{\sigma\sqrt{V_{jj}}} \sim \N{0,1}\quad
  \text{ and }\quad
  T=\frac{\hat{\beta}_j-\beta_j}{\hat{\sigma}\sqrt{V_{jj}}} \sim t(n-p-1)\qquad
  j=0,1,\ldots,p   \]
\begin{Definition}{Standard error for $ \hat{\beta}_j $}{}
  We define the \textbf{standard error} of $ \hat{\beta}_j $ as
  \[ \Se*{\hat{\beta}_j}=\hat{\sigma}\sqrt{V_{jj}}\qquad j=0,1,\ldots,p  \]
\end{Definition}
So, a $ (1-\alpha) $ confidence interval for $ \beta_j $
is
\[ \hat{\beta}_j\pm c\Se*{\hat{\beta}_j} \]
where $ c $ is $ (1-(\alpha/2)) $ quantile of $ t(n-p-1) $.

To test $ H_0 $: $ \beta_j=0 $ vs $ H_A $: $ \beta_j\neq 0 $,
calculate $ t $-statistic
$ t=\dfrac{\hat{\beta}_j}{\Se*{\hat{\beta}_j}} $
reject at level $ \alpha $ if $ \abs{t}>c $ and
$ p $-value is $ 2P(T\geqslant \abs{t}) $ where $ T \sim t(n-p-1) $.

Interpretation of $ \hat{\symbf{\beta}} $: fitted linear
regression model says $ \widehat{\E{Y}} $
(estimate of the expected response) is
\[ \hat{\beta}_0+\hat{\beta}_1x_1+\cdots+\hat{\beta}_p x_p \]
\begin{itemize}
  \item $ \hat{\beta}_0 $ is the estimate of expected response
        when all explanatory variables are equal to 0.
  \item $ \hat{\beta}_j $ is the estimated change
        in expected response for a unit increase in $ x_j $
        when holding all other explanatory variables constant.
        \[ \hat{\beta}_0+\hat{\beta}_1(x_1+1)+\cdots+\hat{\beta}_p x_p
          -(\hat{\beta}_0+\hat{\beta}_1 x_1+\cdots+\hat{\beta}_p x_p)=\hat{\beta}_1 \]
\end{itemize}

\begin{Remark}{}{}
  When it's written $ V_{jj} $, that means the $ j+1^{\text{th}} $
  column and $ j+1^{\text{th}} $ row since we start from index $ 0 $
  for these matrices. Some unfortunate events may have happened
  on the quiz to me due to this.
\end{Remark}
\section{Application}
\begin{Example}{Rocket MLR}{}
  Let $ n=12 $, $ \hat{\symbf{\beta}}=(473.6, 16.7,-1.09)^\top
    =(\hat{\beta}_0,\hat{\beta}_1,\hat{\beta}_2)^\top $.
  \begin{itemize}
    \item $ x_1 $: nozzle area ($ 1 = L,0=S $)
    \item $ x_2 $: propellant ratio
    \item $ Y $: thrust
  \end{itemize}
  \[ \hat{\sigma}=\sqrt{\frac{\sum\limits_{i=1}^{12} e_i^2}{12-1-2}}=
    \sqrt{\frac{\symbf{e}^\top \symbf{e}}{9}}=
    2.655 \]
  Interpretation of $ \hat{\symbf{\beta}} $:
  \begin{itemize}
    \item $ \hat{\beta}_1 $ estimated change in expected thrust is 16.7
          when changing small to large nozzle while holding other variables
          (propellant ratio) constant.
    \item $ \hat{\beta}_2 $ estimated thrust to decrease by 1.09 on average
          for a unit increase in propellant ratio while holding other
          variables (nozzle area) constant.
  \end{itemize}
  Given $ \Se*{\hat{\beta}_2}=0.94 $,
  we compute the $ t $-statistic for $ H_0 $: $ \beta_2=0 $ vs $ H_A $: $ \beta_2\neq 0 $
  which is $ t=-1.09/0.94=-1.16 $.
  \[ p\text{-value}=2P(T\geqslant 1.16)=0.275\text{ from R where } T \sim t(9)\]
  Do not reject $ H_0 $ (e.g., $ \alpha=0.05 $), therefore
  propellant ratio does not significantly influence thrust.
\end{Example}
