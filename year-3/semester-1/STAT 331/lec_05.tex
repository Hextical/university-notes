\chapter{Multiple Linear Regression}
\makeheading{Lecture 5 | 2020-09-21}
\section{Random Vectors}
\begin{Definition}{Multiple linear regression}{}
    A \textbf{multiple linear regression} (MLR) model is defined as
    \[ Y=\beta_0+\beta_1 x_1+\cdots+\beta_p x_p+\varepsilon \]
    which links a response variable $ y $ to several
    independent explanatory variables $ x_1,x_2,\ldots,x_p $.
\end{Definition}
\begin{Example}{Rocket MLR}{}
    \begin{itemize}
        \item $ x_1 $: nozzle area (large or small, 0 or 1)
        \item $ x_2 $: mixture in propellant, ratio oxidized fuel
        \item $ Y $: thrust
    \end{itemize}
    Want to develop linear relationship between response $ y $
    and $ x_1,x_2 $; that is, we want to develop a linear relationship
    between thrust and both nozzle area and mixture in propellant.
\end{Example}

In multiple linear regression, there are $ n $ observations, where each consists of
$ p $ response variables $ (y_i) $, and $ p $ explanatory variables $ (x_{i1},x_{i2},\ldots,x_{ip}) $.
Then,
\[ Y_i \sim N(
    \underbrace{\beta_0+\beta_1x_{i1}+
        \beta_2x_{i2}+\cdots +\beta_p x_{ip}}_{\E{Y_i}=
        \mu_i},\sigma^2) \]
or $ Y_i=\mu_i+\varepsilon_i $ where $ \varepsilon_i \stackrel{\text{iid}}{\sim}
    \N{0,\sigma^2} $.
We can write in vector/matrix form
\[ \begin{bmatrix}
        Y_1    \\
        Y_2    \\
        \vdots \\
        Y_n
    \end{bmatrix}_{n\times 1}=
    \begin{bmatrix}
        \beta_0+\beta_1x_{11}+\cdots+\beta_p x_{1p} \\
        \beta_0+\beta_1x_{21}+\cdots+\beta_p x_{2p} \\
        \vdots                                      \\
        \beta_0+\beta_1x_{n1}+\cdots+\beta_p x_{n p}
    \end{bmatrix}+
    \begin{bmatrix}
        \varepsilon_1 \\
        \varepsilon_2 \\
        \vdots        \\
        \varepsilon_n
    \end{bmatrix}_{n\times 1} \]
Which we can more commonly write as $ \symbf{Y}=X\symbf{\beta}+\symbf{\varepsilon} $ where
\[ \symbf{Y}=\begin{bmatrix}
        Y_1    \\
        Y_2    \\
        \vdots \\
        Y_n
    \end{bmatrix}_{n\times 1}\,
    X=
    \begin{bmatrix}
        1      & x_{11}     & x_{12}     & \cdots & x_{1(p-1)}     & x_{1p}     \\
        1      & x_{21}     & x_{22}     & \cdots & x_{2(p-1)}     & x_{2p}     \\
        \vdots &            &            & \ddots &                & \vdots     \\
        1      & x_{(n-1)1} & x_{(n-1)2} & \cdots & x_{(n-1)(p-1)} & x_{(n-1)p} \\
        1      & x_{n1}     & x_{n2}     & \cdots & x_{n(p-1)}     & x_{n p}    \\
    \end{bmatrix}_{n\times(p+1)}\,
    \symbf{\beta}=
    \begin{bmatrix}
        \beta_0 \\
        \vdots  \\
        \beta_p
    \end{bmatrix}_{(p+1)\times 1}\,
    \symbf{\varepsilon}=\begin{bmatrix}
        \varepsilon_1 \\
        \varepsilon_2 \\
        \vdots        \\
        \varepsilon_n
    \end{bmatrix}_{n\times 1} \]
\begin{Definition}{Random vector}{}
    We call $ \symbf{Y}=(Y_1,Y_2,\ldots,Y_{n})^\top $
    a \textbf{random vector}.
\end{Definition}
\begin{Definition}{Mean vector}{}
    The \textbf{mean vector} of $ \symbf{Y} $ is defined as
    $ \E{\symbf{Y}}=(\E{Y_1},\E{Y_2},\ldots,\E{Y_n})^\top $.
\end{Definition}
\begin{Definition}{Covariance matrix}{}
    The \textbf{covariance matrix} (or \textbf{variance-covariance matrix}) of $ \symbf{Y} $ is defined as
    \[ \Var{\symbf{Y}}=\begin{bmatrix}
            \Var{Y_1}         & \Cov{Y_1,Y_2}     & \cdots & \Cov{Y_1,Y_{n-1}} & \Cov{Y_1,Y_n}     \\
            \Cov{Y_2,Y_1}     & \Var{Y_2}         & \cdots & \Cov{Y_2,Y_{n-1}} & \Cov{Y_2,Y_n}     \\
            \vdots            & \vdots            & \ddots & \vdots            & \vdots            \\
            \Cov{Y_{n-1},Y_1} & \Cov{Y_{n-1},Y_2} & \cdots & \Var{Y_{n-1}}     & \Cov{Y_{n-1},Y_n} \\
            \Cov{Y_n,Y_1}     & \Cov{Y_n,Y_2}     & \cdots & \Cov{Y_n,Y_{n-1}} & \Var{Y_n}
        \end{bmatrix}_{n\times n} \]
\end{Definition}
\begin{Proposition}{Properties of Covariance Matrix}{cov_prop}
    Let $ \symbf{Y} $ be a random vector and $ \symbf{a}\in\mathbb{R}^n $,
    then the covariance matrix has the following properties.
    \begin{enumerate}[label=(\arabic*)]
        \item Symmetric since $ \Cov{Y_i,Y_j}=\Cov{Y_j,Y_i} $; that is $ \Var{\symbf{Y}}^\top=\Var{\symbf{Y}} $.
        \item Positive semi-definite since
              $ \symbf{a}^\top\Var{\symbf{Y}}\symbf{a}\geqslant 0 $
              for all $ \symbf{a}\in\mathbb{R}^n $.
        \item $ \Var{\symbf{Y}}=
                  \E*{\bigl( \symbf{Y}-\E{\symbf{Y}} \bigr)\bigl( \symbf{Y}-\E{\symbf{Y}} \bigr)^\top} $
    \end{enumerate}
\end{Proposition}
\begin{Proof}{\ref{prop:cov_prop}}{}
    Trivial.
\end{Proof}
\begin{Proposition}{Properties of Random Vector}{randomvec_prop}
    Let $ \symbf{a} $ be a $ 1\times n $ matrix (row vector)
    of constants and $ A $ be an $ n\times n $ matrix of constants, then
    the random vector has the following properties.
    \begin{enumerate}[label=(\arabic*)]
        \item $ \E{\symbf{a}\symbf{Y}}=\symbf{a}\E{\symbf{Y}} $
        \item $ \E{A\symbf{Y}}=A\E{\symbf{Y}} $
        \item $ \Var{\symbf{a}\symbf{Y}}=\symbf{a}\Var{\symbf{Y}}\symbf{a}^\top $
        \item $ \Var{A\symbf{Y}}=A\Var{\symbf{Y}}A^\top $
    \end{enumerate}
\end{Proposition}
\begin{Proof}{\ref{prop:randomvec_prop}}{}
    We prove property (4) only.
    \begin{align*}
        \Var{A\symbf{Y}}
         & =\E*{\bigl(A\symbf{Y}-\E{A\symbf{Y}}\bigr)
        \bigl(A\symbf{Y}-\E{A\symbf{Y}}\bigr)^\top}                                                     \\
         & =\E*{\bigl( A\symbf{Y}-A\E{\symbf{Y}} \bigr)
        \bigl( A\symbf{Y}-A\E{\symbf{Y}} \bigr)^\top}                                                   \\
         & =\E*{A\bigl( \symbf{Y}-\E{\symbf{Y}} \bigr)
        \bigl( A\bigl( \symbf{Y}-\E{\symbf{Y}} \bigr) \bigr)^\top}                                      \\
         & =\E*{A\bigl( \symbf{Y}-\E{\symbf{Y}} \bigr)
        \bigl( \symbf{Y}-\E{\symbf{Y}} \bigr)^\top A^\top}                                              \\
         & =A\E*{\bigl( \symbf{Y}-\E{\symbf{Y}} \bigr)\bigl( \symbf{Y}-\E{\symbf{Y}} \bigr)^\top}A^\top \\
         & =A\Var{\symbf{Y}}A^\top
    \end{align*}
\end{Proof}
\begin{Example}{Calculations with MLR Varaibles}{}
    Let $ \symbf{Y}=(Y_1,Y_2,Y_3)^\top $. Suppose
    $ \E{\symbf{Y}}= (3,1,2)^\top $. Let
    $ \Var{\symbf{Y}}=\begin{bmatrix}
            4   & 1/2 & -2 \\
            1/2 & 1   & 0  \\
            -2  & 0   & 3
        \end{bmatrix} $
    and
    $ \symbf{a}=(1,-1,2) $
    and
    $ A=\begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6 \\
            7 & 8 & 9
        \end{bmatrix} $. Note that $ \symbf{a} $ is a
    $ 1\times 3 $ row vector. Compute the following.

    \begin{enumerate}[label=(\roman*)]
        \item $ \E{\symbf{a}\symbf{Y}} $
        \item $ \Var{\symbf{a}\symbf{Y}} $
        \item $ \E{A\symbf{Y}} $
        \item $ \Var{A\symbf{Y}} $
    \end{enumerate}

    \textbf{Solution.} We do the first two and leave the rest as an exercise.
    \begin{enumerate}[label=(\roman*)]
        \item $ \E{\symbf{a}\symbf{Y}}=\symbf{a}\E{\symbf{Y}}=
                  \begin{bmatrix}
                      1 & -1 & 2
                  \end{bmatrix}\begin{bmatrix}
                      3 \\
                      1 \\
                      2
                  \end{bmatrix}=1(3)-1(1)+2(2)=6 $.
        \item \begin{align*}
                  \Var{\symbf{a}\symbf{Y}}
                   & = \symbf{a}\Var{\symbf{Y}}\symbf{a}^\top \\
                   & =
                  \begin{bmatrix}
                      1 & -1 & 2
                  \end{bmatrix}
                  \begin{bmatrix}
                      4   & 1/2 & -2 \\
                      1/2 & 1   & 0  \\
                      -2  & 0   & 3
                  \end{bmatrix}
                  \begin{bmatrix}
                      1  \\
                      -1 \\
                      2
                  \end{bmatrix}                  \\
                   & =
                  \begin{bmatrix}
                      1 & -1 & 2
                  \end{bmatrix}
                  \begin{bmatrix}
                      4(1)+(1/2)(-1)-2(2) \\
                      (1/2)(1)+1(-1)+0(2) \\
                      -2(1)+0(-1)+3(2)
                  \end{bmatrix}                  \\
                   & =\begin{bmatrix}
                      1 & -1 & 2
                  \end{bmatrix}
                  \begin{bmatrix}
                      -1/2 \\
                      -1/2 \\
                      4
                  \end{bmatrix}                  \\
                   & =1(-1/2)-1(-1/2)+2(4)                    \\
                   & =8
              \end{align*}
    \end{enumerate}
\end{Example}
\section{Multivariate Normal Distribution}
\begin{Definition}{Multivariate normal distribution}{}
    Let $ \symbf{Y}=(Y_1,\ldots,Y_n)^\top $
    be a random vector. We say
    that $ Y \sim \text{MVN}(\mu,\sigma) $; that is,
    $ Y $ follows a \textbf{multivariate normal distribution} (MVN) when
    \[ f(\symbf{y};\symbf{\mu},\Sigma)=
        \frac{1}{(2\pi)^{n/2}\abs{\Sigma}^{1/2}}
        \expon*{-\frac{1}{2}(\symbf{y}-\symbf{\mu})^\top
            \Sigma^{-1}(\symbf{y}-\symbf{\mu})}  \]
    where $ \symbf{\mu} $ is defined as the \textbf{mean vector},
    and $ \Sigma $ is defined as the \textbf{covariance matrix}.
    Note that $ \Sigma^{-1} $ is the inverse of the covariance matrix
    and $ \abs{\Sigma} $ is the determinant of $ \Sigma $.
\end{Definition}

\begin{Theorem}{Properties of Multivariate Normal Distribution}{}
    Let $ \symbf{Y}
        =(Y_1,Y_2,\ldots,Y_n)^\top \sim \text{MVN}(\symbf{\mu},\Sigma) $.
    \begin{enumerate}[label=(\arabic*)]
        \item Any subset of $ Y_1,Y_2,\ldots,Y_n $
              also has a multivariate normal distribution and in particular
              \[ Y_i \sim \N{\mu_i,\Sigma_{ii}}\qquad i=1,2,\ldots,n \]
        \item Let $ \symbf{a}=(a_1,a_2\ldots,a_n) $ be a
              non-zero row vector ($ 1\times n $) of constants, then
              \[ \symbf{a}\symbf{Y}\sim
                  \N{\symbf{a}\symbf{\mu},\symbf{a}
                      \Sigma\symbf{a}^\top} \]
        \item Let $ A $ be an $ n\times n $ matrix of rank $ n $, then
              \[ A\symbf{Y} \sim \text{MVN}(A\symbf{\mu},A\Sigma A^\top) \]
        \item The conditional distribution of any
              subset of $ (Y_1,Y_2,\ldots,Y_n) $ given any other coordinates
              that are not in the subset
              is a multivariate normal distribution.
        \item $ Y_i $ and $ Y_j $ are independent
              random variables if and only if $ \Sigma_{ij}=\Cov{Y_i,Y_j}=0 $.
    \end{enumerate}
\end{Theorem}
