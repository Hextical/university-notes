\makeheading{Lecture 9 | 2020-10-05}
Analysis of variance (ANOVA): how well
does our regression model fit our
response variable?

Variability in response can be measured by
``total sum of squares:''
\[ \Ss{Total} =
    \sum\limits_{i=1}^{n} (y_i-\bar{y})^2 \]
as seen in HW1, it's closely related to
sample variance of $ y_1,\ldots,y_n $,
which is $ \Ss{Total}/(n-1) $.

ANOVA decomposes $ \Ss{Total}=\Ss{Reg}+\Ss{Res} $
where $ \Ss{Reg} $ is the regression sum of squares
and $ \Ss{Res} $ is the residual sum of squares.

The regression sum of squares is variation explained by
the model
and the residual sum of squares is the variation not
explained by the regression model.

Using the fact that
\[ y_i-\bar{y}=y_i-\hat{\mu}_i+\hat{\mu}_i-\bar{y} \]
When regression fits data well,
the observations $ y_i $ tend to be much closer to $ \hat{\mu}_i $.
Note that $ \bar{y} $ is line a regression line with $ \beta_1=0 $.

Mathematically,
\[ \Uunderbracket{\sum\limits_{i=1}^{n} (y_i-\bar{y})^2}_{\Ss{Total}}
    =\Uunderbracket{\sum\limits_{i=1}^{n} (\hat{\mu}_i-\bar{y})^2}_{\Ss{Reg}}+
    \Uunderbracket{\sum\limits_{i=1}^{n} (y_i-\hat{\mu}_i)^2}_{\Ss{Res}} \]
since we showed that $ \sum\limits_{i=1}^{n}(\hat{\mu}_i-\bar{y})
    \Uunderbracket{(y_i-\hat{\mu}_i)}_{e_i}=0 $ in HW1 for SLR\@.
It's also true for MLR since
\[ \sum\limits_{i=1}^{n} (\hat{\mu}_i-\bar{y})e_i=
    \sum\limits_{i=1}^{n} (e_i\hat{\mu}_i)-\bar{y}\sum\limits_{i=1}^{n} e_i
    =\hat{\symbf{\mu}}^\top\symbf{e}-\bar{y}\symbf{1}^\top\symbf{e}=0 \]
Recall: $ \symbf{1}^\top\symbf{e}=0 $ is one of LS equations,
and $ \hat{\symbf{\mu}}=X\hat{\symbf{\beta}} $ is in $ \Span{X} $,
so $ \symbf{e} $ is orthogonal to $ \Span{X} $,
so $ \hat{\symbf{\mu}}^\top\symbf{e}=0 $.

\begin{table}[H]
    \centering
    \caption{ANOVA Table}
    \begin{tabu} to 1\textwidth {Y[0.5]YYY[2]Y}
        \toprule
        Source     & d.f.      & $ SS $         & Mean Square                         & $ F $                 \\
        \midrule
        Regression & $ p $     & $ \Ss{Reg} $   & $ \Ss{Reg}/p $                      & $ \Ms{Reg}/\Ms{Res} $ \\
        Residual   & $ n-p-1 $ & $ \Ss{Res} $   & $ \Ss{Res}/(n-p-1)=\hat{\sigma}^2 $                         \\
        \midrule
        Total      & $ n-1 $   & $ \Ss{Total} $                                                               \\
        \bottomrule
    \end{tabu}
\end{table}
$ F $ is used to test the overall significance of regression (later).

We call the \textbf{coefficient of determination}
$ R^2 = \Ss{Reg}/\Ss{Total}=1-\Ss{Res}/\Ss{Total} $.
clearly, $ 0\leqslant R^2\leqslant 1 $. It is the proportion
of variation (in our response variable) that is explained
by the regression model. Larger $ R^2 $ means
the fitted values are closer to the observations $ y_i $,
which means the residuals are small; that is, smaller $ \Ss{Res} $.
Note that (HW1) in SLR, $ R^2 $ is equivalent to the square of
the sample correlation between $ x $ and $ y $
based on $ (x_1,y_1),\ldots,(x_n,y_n) $.

\begin{table}[H]
    \centering
    \caption{Rocket ANOVA Table}
    \begin{tabu} to 0.9\textwidth {Y[0.5]YYY[2]Y}
        \toprule
        Source     & d.f.   & $ SS $     & Mean Square & $ F $  \\
        \midrule
        Regression & $ 2 $  & $ 846.2 $  & $ 423.1 $   & $ 60 $ \\
        Residual   & $ 9 $  & $ 63.42 $  & $ 7.05 $    &        \\
        \midrule
        Total      & $ 11 $ & $ 909.62 $                        \\
        \bottomrule
    \end{tabu}
\end{table}
Response thrust
$ R^2=846.2/909.62 \approx 0.93 $. $ R^2 $
interpretation: regression model with nozzle size and propellant ratio
explains $ 93\% $ of variation in thrust (response).
