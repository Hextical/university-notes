\section{Simple Linear Regression}
\makeheading{Lecture 2 | 2020-09-09}
\begin{definition}
    A \textbf{simple linear regression}
    is a linear model that uses only one
    explanatory variable; that is,
    $ Y=\beta_0+\beta_1x+\varepsilon $.
    The \textbf{data}
    in a simple linear regression consists of pairs $ (x_i,y_i) $
    where $ i=1,\ldots,n $.
\end{definition}
\begin{remark}
    Before fitting any model, we might want
    to make a scatterplot to visualize if there
    is a linear relationship between $ x $ and $ y $,
    or calculate the \emph{correlation}.
\end{remark}
\begin{definition}
    The \textbf{correlation} of
    random variables $ X $ and $ Y $ is
    $ \rho_{XY}=\dfrac{\Cov{X,Y}}{\Sd{X}\Sd{Y}} $.
\end{definition}
\begin{definition}
    The \textbf{sample correlation} of
    all pairs $ (x_i,y_i) $ is
    \begin{align*}
        r
         & =\frac{\frac{1}{n-1} \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}
        {\sqrt{\frac{1}{n-1}\sum_{i=1}^{n} (x_i-\bar{x})^2}
        \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(y_i-\bar{y})}}                  \\
         & =\frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})}{
            \sqrt{\sum_{i=1}^{n} (x_i-\bar{x})^2}
            \sqrt{\sum_{i=1}^{n} (y_i-\bar{y})^2}
        }                                                                 \\
         & =\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}
    \end{align*}
\end{definition}
\begin{remark}
    The sample correlation measures the strength and direction of
    the linear relationship between $ X $ and $ Y $. Note
    that $ -1\leqslant r\leqslant 1 $.
    If $ \abs{r}\approx 1 $, then
    there is a strong linear relationship, and if
    $ \abs{r}\approx 0 $
    then there is a lack of linear relationship. Also, if $ r>0 $,
    then there is a positive relationship, and if $ r<0 $
    then there is a negative relationship.
    It does not tell us how to predict $ Y $ from $ X $. To do so,
    we need to estimate $ \beta_0 $ and $ \beta_1 $.
\end{remark}

\begin{definition}
    For data $ (x_i,y_i) $ for $ i=1,\ldots,n $, the
    \textbf{simple linear regression model} is
    $ Y_i=\beta_0+\beta_1x_i+\varepsilon_i $
    with the assumption that
    $ \varepsilon_i\stackrel{\text{iid}}{\sim}N(0,\sigma^2) $.
    Therefore,
    $ Y_i\sim N(\mu_i=\beta_0+\beta_1x_i,\sigma^2) $.
\end{definition}
\begin{definition}
    The method of estimating $ \beta_0 $
    and $ \beta_1 $ by minimizing
    $ S(\beta_0,\beta_1)=\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_i))^2 $
    is referred to as the \textbf{method of least squares}.
\end{definition}
\begin{remark}
    The least squares is equivalent to maximum likelihood estimate
    when $ \varepsilon_i\stackrel{\text{iid}}{\sim}N(0,\sigma^2) $.
\end{remark}
\begin{theorem}[Least Square Estimates (LSEs)]
    Minimizing $ S(\beta_0,\beta_1) $, gives the
    least square estimates
    \[ \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}\quad\text{and}
        \quad
        \hat{\beta}_1=
        \frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})}{
            \sum_{i=1}^{n} (x_i-\bar{x})^2
        }=\frac{S_{xy}}{S_{xx}}   \]
\end{theorem}
\begin{proof}
    $ \displaystyle \frac{\partial S}{\partial\beta_0}=2
        \sum\limits_{i=1}^{n} \left[ y_i-(\beta_0+\beta_1x_i) \right](-1)
    $ and $ \displaystyle
        \frac{\partial S}{\partial\beta_1}=2
        \sum\limits_{i=1}^{n} \left[ y_i-(\beta_0+\beta_1x_i) \right](-x_i) $.

    Now,
    \[ \frac{dS}{d\beta_0}\coloneq 0
        \iff \sum\limits_{i=1}^{n}y_i-n\beta_0-
        \beta_1 \sum\limits_{i=1}^{n} x_i=0
        \iff \beta_0=\bar{y}-\beta_1\bar{x} \]
    \begin{align*}
        \frac{dS}{d\beta_1}\coloneq 0
         & \stackrel{\text{plug }\beta_0}{\iff}
        \sum\limits_{i=1}^{n} \left[ y_i-\bar{y}+\beta_1\bar{x}-\beta_1 x_i \right]x_i=0 \\
         & \iff \sum\limits_{i=1}^{n} x_i(y_i-\bar{y})-\beta_1
        \sum\limits_{i=1}^{n} x_i(x_i-\bar{x})=0                                         \\
         & \iff \beta_1=\frac{\sum_{i=1}^{n} x_i(y_i-\bar{y})}{\sum_{i=1}^{n}
            x_i
            (x_i-\bar{x})}
    \end{align*}
\end{proof}
\begin{remark}
    We use a hat on the $ \beta $'s to show that they are estimates.
\end{remark}
\begin{definition}
    We call
    $ \hat{\mu}_i=\hat{\beta}_0+\hat{\beta}_1x_i $ the \textbf{fitted values},
    and $ e_i=y_i-\hat{\mu}_i $ the \textbf{residuals}.
\end{definition}
