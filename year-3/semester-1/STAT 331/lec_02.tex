\chapter{Simple Linear Regression}
\makeheading{Lecture 2 | 2020-09-09}
\section{Introduction}
\begin{Definition}{Simple linear regression}{}
    A \textbf{simple linear regression}
    is a linear model that uses only one
    explanatory variable; that is,
    $ Y=\beta_0+\beta_1x+\varepsilon $.
    The \textbf{data}
    in a simple linear regression consists of pairs $ (x_i,y_i) $
    where $ i=1,\ldots,n $.
\end{Definition}
\begin{Remark}{}{}
    Before fitting any model, we might want
    to make a scatter plot to visualize if there
    is a linear relationship between $ x $ and $ y $,
    or calculate the \emph{correlation}.
\end{Remark}
\begin{Definition}{Correlation}{}
    The \textbf{correlation} of
    random variables $ X $ and $ Y $ is
    $ \rho_{XY}=\dfrac{\Cov{X,Y}}{\Sd{X}\Sd{Y}} $.
\end{Definition}
\begin{Definition}{Sample correlation}{}
    The \textbf{sample correlation} of
    all pairs $ (x_i,y_i) $ is
    \begin{align*}
        r
         & =\frac{\frac{1}{n-1} \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}
        {\sqrt{\frac{1}{n-1}\sum_{i=1}^{n} (x_i-\bar{x})^2}
        \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(y_i-\bar{y})^2}}                \\
         & =\frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})}{
            \sqrt{\sum_{i=1}^{n} (x_i-\bar{x})^2}
            \sqrt{\sum_{i=1}^{n} (y_i-\bar{y})^2}
        }                                                                 \\
         & =\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}
    \end{align*}
\end{Definition}
\begin{Remark}{}{}
    The sample correlation measures the strength and direction of
    the linear relationship between $ x $ and $ y $. Note
    that $ -1\leqslant r\leqslant 1 $.
    If $ \abs{r}\approx 1 $, then
    there is a strong linear relationship, and if
    $ \abs{r}\approx 0 $
    then there is a lack of linear relationship. Also, if $ r>0 $,
    then there is a positive relationship, and if $ r<0 $
    then there is a negative relationship.
    It does not tell us how to predict $ y $ from $ x $. To do so,
    we need to estimate $ \beta_0 $ and $ \beta_1 $.
\end{Remark}

\begin{Definition}{Simple linear regression model}{}
    For data $ (x_i,y_i) $ for $ i=1,\ldots,n $, the
    \textbf{simple linear regression model} is
    $ Y_i=\beta_0+\beta_1x_i+\varepsilon_i $
    with the assumption that
    $ \varepsilon_i\stackrel{\text{iid}}{\sim}\N{0,\sigma^2} $.
    Therefore,
    $ Y_i\sim \N{\mu_i=\beta_0+\beta_1x_i,\sigma^2} $.
\end{Definition}
\begin{Definition}{Least squares}{}
    The method of estimating $ \beta_0 $
    and $ \beta_1 $ by minimizing
    $ S(\beta_0,\beta_1)=\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_i))^2 $
    is referred to as the \textbf{method of the least squares}.
\end{Definition}
\begin{Remark}{}{}
    The least squares is equivalent to maximum likelihood estimate
    when $ \varepsilon_i\stackrel{\text{iid}}{\sim}\N{0,\sigma^2} $.
\end{Remark}
\begin{Theorem}{Least Square Estimates (LSEs) for SLR}{lse_slr}
    Minimizing $ S(\beta_0,\beta_1) $, gives the
    least square estimates
    \[ \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}\quad\text{and}
        \quad
        \hat{\beta}_1=
        \frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})}{
            \sum_{i=1}^{n} (x_i-\bar{x})^2
        }=\frac{S_{xy}}{S_{xx}}   \]
\end{Theorem}
\begin{Proof}{\ref{thm:lse_slr}}{}
    $ \displaystyle \frac{\partial S}{\partial\beta_0}=2
        \sum\limits_{i=1}^{n} \left[ y_i-(\beta_0+\beta_1x_i) \right](-1)
    $ and $ \displaystyle
        \frac{\partial S}{\partial\beta_1}=2
        \sum\limits_{i=1}^{n} \left[ y_i-(\beta_0+\beta_1x_i) \right](-x_i) $.

    Now,
    \[ \frac{dS}{d\beta_0}\coloneq 0
        \iff \sum\limits_{i=1}^{n}y_i-n\beta_0-
        \beta_1 \sum\limits_{i=1}^{n} x_i=0
        \iff \beta_0=\bar{y}-\beta_1\bar{x} \]
    \begin{align*}
        \frac{dS}{d\beta_1}\coloneq 0
         & \stackrel{\text{plug }\beta_0}{\iff}
        \sum\limits_{i=1}^{n} \left[ y_i-\bar{y}+\beta_1\bar{x}-\beta_1 x_i \right]x_i=0 \\
         & \iff \sum\limits_{i=1}^{n} x_i(y_i-\bar{y})-\beta_1
        \sum\limits_{i=1}^{n} x_i(x_i-\bar{x})=0                                         \\
         & \iff \beta_1=\frac{\sum_{i=1}^{n} x_i(y_i-\bar{y})}{\sum_{i=1}^{n}
            x_i
            (x_i-\bar{x})}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})
            (y_i-\bar{y})}{
            \sum_{i=1}^{n} (x_i-\bar{x})^2
        }=\frac{S_{xy}}{S_{xx}}
    \end{align*}
\end{Proof}
\begin{Remark}{}{}
    We use a hat on the $ \beta $'s to show that they are estimates.
\end{Remark}
\begin{Definition}{Fitted value, Residual}{}
    The expression
    $ \hat{\mu}_i=\hat{\beta}_0+\hat{\beta}_1x_i $
    is called the \textbf{fitted value}
    that corresponds to the $ i^{\text{th}} $ observation
    with $ x_i $ as the explanatory variable.
    The difference between $ y_i $ and $ \hat{\mu}_i $,
    and $ e_i=y_i-\hat{\mu}_i $ is referred
    to as the \textbf{residual}. It is
    the vertical distance between the observation $ y_i $
    and the estimated line $ \hat{\mu}_i $
    evaluated at $ x_i $.
\end{Definition}
