\makeheading{Lecture 8 | 2020-09-27}
\section{Joint Expectation}
This section: extend the definition of expectation
from univariate to bivariate cases.
\begin{Definition}{Joint exepectation}{}
    Suppose $ h(x,y) $ is a real-valued function.

    If $ X $ and $ Y $
    are discrete random variables with joint
    probability function $ f(x,y) $ and support set $ A $ then
    \[ \E{h(X,Y)}=\sum_{(x,y)\in A}h(x,y)f(x,y)  \]
    provided the joint sum converges absolutely.

    If $ X $ and $ Y $
    are continuous random variables with joint
    probability density function $ f(x,y) $ then
    \[ \E{h(X,Y)}=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} h(x,y)f(x,y)\, d{x}\, d{y}   \]
    provided the joint integral converges absolutely.
\end{Definition}
\begin{Example}{}{}
    \[ \E{XY}=\begin{dcases}
            \sum_{x}\sum_{y}xy f(x,y)                                                 & X,Y\text{ are joint discrete}   \\
            \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x y f(x,y)\, d{x} \, d{y} & X,Y\text{ are joint continuous}
        \end{dcases} \]
    \[ \E{X}=\begin{dcases}
            \sum_{x}\sum_{y}x f(x,y)                                                & X,Y\text{ are joint discrete}   \\
            \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f(x,y)\, d{x} \, d{y} & X,Y\text{ are joint continuous}
        \end{dcases} \]
    Alternatively,
    \[ \E{X}=\sum_{x}x f_1(x)=\sum_{x}x\biggl[\sum_{y}f(x,y) \biggr]   \]
\end{Example}
\begin{Proposition}{Linearity Property}{}
    Suppose $ X $ and $ Y $ are random variables
    with joint probability (density) function $ f(x,y) $,
    $ a $ and $ b $ are constants, and $ g(x,y) $
    and $ h(x,y) $ are real-valued functions. Then
    \[ \E{ag(X,Y)+bh(X,Y)}=a\E{g(X,Y)}+b\E{h(X,Y)} \]
\end{Proposition}
\begin{Corollary}{}{}
    If $ X_1,\ldots,X_n $ are random variables
    and $ a_1,\ldots,a_n $ are real constants then
    \[ \E[\bigg]{\sum_{i=1}^{n} a_i X_i}=
        \sum_{i=1}^{n} a_i \E{X_i} \]
\end{Corollary}
\begin{Theorem}{Expectation and Independence}{exp_indep}
    \begin{enumerate}[label=(\arabic*)]
        \item If $ X $ and $ Y $ are independent
              random variables and $ g(x) $ and $ h(y) $
              are real-valued functions then
              \[ \E{g(X)h(Y)}=\E{g(X)}\E{h(Y)} \]
        \item More generally, if $ X_1,\ldots,X_n $ are independent
              random variables and $ h $
              is a real-valued function then
              \[ \E[\bigg]{\prod_{i=1}^n h(X_i)}=
                  \prod_{i=1}^n \E{h(X_i)} \]
    \end{enumerate}
\end{Theorem}
\begin{Definition}{Covariance}{}
    The \textbf{covariance} of
    random variables $ X $ and $ Y $ is defined by
    \[ \Cov{X,Y}=\E{(X-\mu_X)(Y-\mu_Y)} \]
    where $ \mu_X=\E{X} $ and $ \mu_Y=\E{Y} $.
\end{Definition}
\begin{Theorem}{Covariance and Independence}{cov_indep}
    If $ X $ and $ Y $ are random variables then
    \[ \Cov{X,Y}=\E{XY}-\mu_X\mu_Y \]
    If $ X $ and $ Y $ are independent then $ \Cov{X,Y}=0 $.
\end{Theorem}
\begin{Proof}{\Cref{thm:cov_indep}}{}
    \begin{align*}
        \Cov{X,Y}
         & =\E{(X-\mu_X)(Y-\mu_Y)}                  \\
         & =\E{XY-\mu_X Y - \mu_Y X + \mu_X\mu_Y}   \\
         & =\E{XY}-\mu_X\E{Y}-\mu_Y\E{X}+\mu_X\mu_Y \\
         & =\E{XY}-\E{X}\E{Y}-\E{Y}\E{X}+\E{X}\E{Y} \\
         & =\E{XY}-\E{X}\E{Y}
    \end{align*}
    Now, if $ X $ and $ Y $ are independent, then
    by~\Cref{thm:exp_indep}, $ \E{XY}=\E{X}\E{Y}=0 $. Thus,
    $ \Cov{X,Y}=0 $.
\end{Proof}
\begin{Theorem}{Results for Covariance}{}
    \begin{enumerate}[label=(\arabic*)]
        \item $ \Cov{X,X}=\E{(X-\mu_X)(X-\mu_X)}=\E{(X-\mu_X)^2}=\Var{X} $
        \item $ \Cov{X+Y,Z}=\Cov{X,Z}+\Cov{Y,Z} $
    \end{enumerate}
\end{Theorem}
\begin{Theorem}{Variance of a Linear Combination}{}
    \begin{enumerate}[label=(\arabic*)]
        \item Suppose $ X $ and $ Y $ are random variables
              and $ a $ and $ b $ are real constants then
              \[ \Var{aX+bY}=a^2\Var{X}=b^2\Var{Y}+2ab\Cov{X,Y} \]
        \item Suppose $ X_1,\ldots,X_n $ are random variables
              and $ a_1,\ldots,a_n $ are real constants then
              \[ \Var[\bigg]{\sum_{i=1}^{n} a_i X_i}=
                  \sum_{i=1}^{n} a_i^2 \Var{X_i}+\Uunderbracket{\sum_{i\neq j}a_i a_j\Cov{X_i,X_j}}_{\binom{n}{2}\text{ terms}}=
                  \sum_{i=1}^{n} a_i^2\Var{X_i}
                  +\Uunderbracket{2\sum_{i<j}a_i a_j\Cov{X_i,X_j}}_{\binom{n}{2}\text{ terms}} \]
        \item If $ X_1,\ldots,X_n $ are random variables
              and $ a_1,\ldots,a_n $ are real constants then
              \[ \Var[\bigg]{\sum_{i=1}^{n} a_i X_i}=\sum_{i=1}^{n} a_i^2\Var{X_i} \]
    \end{enumerate}
\end{Theorem}
\begin{Example}{}{}
    Suppose the joint p.f.\ of $ X $ and $ Y $ is
    $ \displaystyle f(x,y)=\frac{\theta^{x+y}e^{-2\theta}}{x!y!} $,
    where $ 0\le x,y\in\mathbf{Z} $.
    Find $ \Var{2X+3Y} $.

    \textbf{Solution.}
    \[ f(x,y)=\Uunderbracket{\biggl( \frac{\theta^x e^{-\theta}}{x!} \biggr)}_{g(x)}
        \Uunderbracket{\biggl( \frac{\theta^y e^{-\theta}}{y!} \biggr)}_{h(y)} \]
    Thus, the range of $ X $ does not depend on $ Y $. Therefore,
    $ X $ and $ Y $ are independent. In other words, we can write
    \[ f_1(x)=C \frac{\theta^x e^{-\theta}}{x!} \quad 0\le x\in\mathbf{Z} \]
    Since $ \displaystyle \sum_{x=0}^{\infty} \frac{\theta^x e^{-\theta}}{x!} =1 $
    as it is Poisson we get that $ C=1 $. Also,
    \[ f_2(y)=\frac{\theta^y e^{-\theta}}{y!}\quad 0\le y\in\mathbf{Z} \]
    Thus, $ \Var{X}=\theta $ and $ \Var{Y}=\theta $. Finally,
    \[ \Var{2X+3Y}=4\Var{X}+9\Var{Y}=13\theta \]
\end{Example}
\begin{Example}{}{}
    The joint p.d.f.\ of $ X $ and $ Y $ is
    $ f(x,y)=\begin{cases}
            x+y & x,y\in\interval{0}{1} \\
            0   & \text{otherwise}
        \end{cases} $.

    Find $ \Var{X+Y} $.

    \textbf{Solution.} We know $ \Var{X+Y}=\Var{X}+\Var{Y}+2\Cov{X,Y} $. Recall
    that
    \[ f_1(x)=\begin{dcases}
            x+\frac{1}{2} & x\in\interval{0}{1} \\
            0             & \text{otherwise}
        \end{dcases}\quad
        f_2(y)=\begin{dcases}
            y+\frac{1}{2} & y\in\interval{0}{1} \\
            0             & \text{otherwise}
        \end{dcases} \]
    \[ \E{X}=\int_{0}^{1} x\biggl( x+\frac{1}{2}  \biggr)\, d{x} =
        \frac{7}{12} \]
    \[ \E{X^2}=\int_{0}^{1} x^2\biggl( x+\frac{1}{2}  \biggr)\, d{x}=
        \frac{5}{12} \]
    \[ \implies\Var{X}=\E{X^2}-\mu_X^2=\frac{5}{12}-\biggl( \frac{7}{12} \biggr)^{\!2}=\frac{11}{144}  \]
    We know that $ \E{Y}=7/12 $, $ \Var{Y}=11/144 $. Now,
    \[ \E{XY}=
        \int_{0}^{1} \int_{0}^{1} x y(x+y)\, d{y}\, d{x}=\frac{1}{3}  \]
    \[ \implies \Cov{X,Y}=\E{XY}-\mu_X\mu_Y=
        \frac{1}{3} -\biggl( \frac{7}{12}  \biggr)\biggl( \frac{7}{12}  \biggr)=
        -\frac{1}{144}  \]
    Hence,
    \[ \Var{X+Y}=\Var{X}+\Var{Y}+2\Cov{X,Y}=
        \frac{11}{144} +\frac{11}{144} -\frac{2}{144} =\frac{20}{144}=\frac{5}{36} \]
\end{Example}
\begin{Definition}{Correlation coefficient}{}
    The \textbf{correlation coefficient} of
    random variables $ X $ and $ Y $ is defined by
    \[ \rho(X,Y)=\frac{\Cov{X,Y}}{\sqrt{\Var{X}}\sqrt{\Var{Y}}}  \]
\end{Definition}
\begin{Remark}{}{}
    $ \rho(X,Y) $ can only be used to characterize \textbf{linear} association
    between $ X $ and $ Y $. For example, there might exist some quadratic
    relationship between $ X $ and $ Y $ but $ \rho(X,Y)\to 0 $.
\end{Remark}
\begin{Example}{}{}
    $ Y=X^2 $ and $ X \sim \N{0,1} $. Note that $ \rho(X,Y)=0 $,
    but obviously there is some relationship between $ X $ and $ Y $.
\end{Example}
\begin{Theorem}{}{}
    If $ \rho(X,Y) $ is the correlation coefficient
    of random variables $ X $ and $ Y $, then
    $ -1\le \rho(X,Y)\le 1 $
    \begin{enumerate}[label=(\arabic*)]
        \item $ \rho(X,Y)=1\iff Y=aX+b $ with $ a>0 $.
        \item $ \rho(X,Y)=-1\iff Y=aX+b $ with $ a<0 $.
    \end{enumerate}
\end{Theorem}
\begin{Example}{}{}
    Let $ \displaystyle f(x,y)=\begin{cases}
            x+y & x,y\in\interval{0}{1} \\
            0   & \text{otherwise}
        \end{cases} $.
    Find $ \rho(X,Y) $.

    \textbf{Solution.} Recall that $ \Var{X}=\Var{Y}=11/144 $
    and $ \Cov{X,Y}=-1/144 $. So,
    \[ \rho(X,Y)=\frac{-1/144}{\sqrt{11/144}\sqrt{11/144}} =-\frac{1}{11}  \]

\end{Example}
