\makeheading{Lecture 3 | 2020-09-13}
We first introduce a function that will be used.

\begin{Definition}{Gamma function}{}
    The \textbf{gamma function},
    denoted $ \Gamma(\alpha) $ for all $ \alpha>0 $, is given by
    \[ \Gamma(\alpha)=\int_{0}^{\infty} x^{\alpha-1}e^{-x}\, d{x}  \]
\end{Definition}
\begin{Proposition}{Properties of the Gamma Function}{prop_gamma}
    \begin{enumerate}[label=(\arabic*)]
        \item\label{gamma_prop_1}$ \Gamma(\alpha)=(\alpha-1)\Gamma(\alpha-1) $ for $ \alpha>1 $
        \item\label{gamma_prop_2} $ \Gamma(n)=(n-1)! $ when $ n\geqslant 1 $ is a positive integer
        \item\label{gamma_prop_3} $ \displaystyle \Gamma\left( \frac{1}{2} \right)=\sqrt{\pi} $
    \end{enumerate}
\end{Proposition}
We don't need to know the following proof, but I checked it out for fun. Content not found in the
syllabus is usually labelled with a dagger ($ \dagger $).
\begin{Proof}{$ \dagger $~\ref{prop:prop_gamma}}{}
    Proof of~\ref{gamma_prop_1}. Suppose $ \alpha>1 $.
    \[ \Gamma(\alpha)=\int_{0}^{\infty} x^{\alpha-1}e^{-x}\, d{x}  \]
    Let $ u=x^{\alpha-1} \implies du=(\alpha-1)x^{\alpha-2}\,dx $ and $ dv=e^{-x}\,dx\implies v=-e^{-x} $. Now,
    recall from MATH 138:
    \[ \int u\, d{v} =uv-\int v\, d{u} \]
    So,
    \begin{align*}
        \Gamma(\alpha)
         & = \left[ (\alpha-1)x^{\alpha-2}\left( -e^{-x} \right) \right]_0^\infty-\int_{0}^{\infty}\left(-e^{-x}\right)
        (\alpha-1)x^{\alpha-2}\, d{x}                                                                                   \\
         & =0+(\alpha-1)\int_{0}^{\infty}e^{-x} x^{\alpha-2}\, d{x}                                                     \\
         & =(\alpha-1)\Gamma(\alpha)
    \end{align*}
    Proof of~\ref{gamma_prop_2}. Using~\ref{gamma_prop_1}:
    \begin{align*}
        \Gamma(\alpha)
         & =(\alpha-1)\Gamma(\alpha-1)                    \\
         & =(\alpha-1)(\alpha-2)\Gamma(\alpha-3)          \\
         & =(\alpha-1)(\alpha-2)\cdots (3)(2)(1)\Gamma(1) \\
    \end{align*}
    We know that $ \Gamma(1)=1 $ by using the definition (trivial), therefore the result now follows.

    Proof of~\ref{gamma_prop_3}. Sketch:
    \begin{itemize}
        \item Let $ u=x^2 $, so $ du=2x\,dx $. Let $ \alpha=\dfrac{1}{2} $, so the integral looks like:
              \[ \Gamma\left( \frac{1}{2}  \right)=2\int_{0}^{\infty} e^{-u^2}\, d{u}  \]
        \item Compute $ \left[ \Gamma \left( \frac{1}{2}  \right) \right]^2 $. Using polar coordinates,
              compute the following double integral.
              \[ 4 \int_{0}^{\infty} \int_{0}^{\infty} e^{-(u^2+v^2)}\, d{v} \, d{u}  \]
              One will have to compute the Jacobian Matrix.
        \item Solve for $ \Gamma\left( \dfrac{1}{2} \right) $ explicitly now.
    \end{itemize}
    \underline{Author's note}: This was covered in MATH 237 when I took it (F19).
\end{Proof}

\begin{Example}{}{}
    The p.d.f.\ is given by
    \[ f(x)=\begin{dcases}
            \frac{x^{\alpha-1}e^{-x/\beta}}{\Gamma(\alpha)\beta^\alpha} & x>0          \\
            0                                                           & x\leqslant 0
        \end{dcases}
    \]
    when $ \alpha>0 $ and $ \beta>0 $. We say that $ X \sim \gam{\alpha,\beta} $.

    We also say that $ \alpha $ is the scale parameter and $ \beta $ is the shape parameter
    for this distribution.

    Verify that $ f(x) $ is a p.d.f.

    \textbf{Solution.} Showing $ f(x)\geqslant 0 $ is trivial. Now,
    \[ \int_{-\infty}^{\infty} f(x)\, d{x} =
        \int_{0}^{\infty} \frac{x^{\alpha-1}e^{-x/\beta}}{\Gamma(\alpha)
        \beta^{\alpha}} \, d{x}  \]
    Let $ y=x/\beta\implies x=y\beta $ and $ dx=\beta\,dy $.
    Therefore,
    \[\int_{-\infty}^{\infty} f(x)\, d{x} =\int_{0}^{\infty}
        \frac{y^{\alpha-1}\beta^{\alpha-1}e^{-y}}{\Gamma(\alpha)\beta^\alpha}(\beta)  \, d{y}
        =\frac{1}{\Gamma(\alpha)}\int_{0}^{\infty} y^{\alpha-1}e^{-y}\, d{y}=1    \]
\end{Example}

\begin{Example}{}{}
    Suppose the p.d.f.\ is given by
    \[ f(x)=\begin{dcases}
            \frac{\beta}{\theta^\beta}x^{\beta-1}e^{-\left( x/\theta  \right)^\beta} & x>0          \\
            0                                                                        & x\leqslant 0
        \end{dcases} \]
    with $ \alpha>0 $ and $ \beta>0 $.
    Then, $ X \sim \weib{\theta,\beta} $.
    Verify that $ f(x) $ is a p.d.f.

    \textbf{Solution.} $ f(x)\geqslant 0 $ for every $ x\in\mathbb{R} $. Now,
    \[ \int_{-\infty}^{\infty} f(x)\, d{x} =
        \int_{0}^{\infty} \frac{\beta}{\theta^\beta}x^{\beta-1}e^{-\left( x/\theta  \right)^\beta} \, d{x} \]
    Let $ y=(x/\theta)^\beta \implies
        x=\theta y^{1/\beta} $ and $ dx=(\theta/\beta) y^{(1/\beta)-1}\,dy $.
    Therefore,
    \[ \int_{-\infty}^{\infty} f(x)\, d{x}=\int_{0}^{\infty} \frac{\beta}{\theta^\beta} \theta^{\beta-1}
        y^{(\beta-1)/\beta}e^{-y}\frac{\theta}{\beta} y^{(1/\beta)-1}\, d{y}
        =\int_{0}^{\infty} e^{-y}\, d{y}=\Gamma(1)=1  \]
\end{Example}

\begin{Example}{Normal}{}
    The p.d.f.\ is given by
    \[ f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{(x-\mu)^2}{2\sigma^2} \right\}  \]
    for $ x\in\mathbb{R} $, $ \mu\in\mathbb{R} $, $ \sigma^2>0 $.
    Verify that $ f(x) $ is a p.d.f.

    \textbf{Solution.}

    $ f(x)\geqslant 0 $ obviously.

    \underline{Case 1}: $ \mu=0 $ and $ \sigma^2=1 $, then
    we say $ X $ follows a \textbf{standard normal} distribution.
    We want to show that
    \[ \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{x^2}{2} \right\} \, d{x}=1  \]
    Since the function is symmetrical around 0, we have the following equivalent integral.
    \[ 2\int_{0}^{\infty} \frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{x^2}{2} \right\} \, d{x}  \]
    Let $ y=x^2/2\implies x=\sqrt{2y} $
    and $ dx=\dfrac{\sqrt{2}}{2} y^{-1/2}\,dy $. Therefore,
    \[ =\frac{2}{\sqrt{2\pi}}\int_{0}^{\infty} e^{-y}\frac{\sqrt{2}}{2} y^{-1/2}\, d{y}
        =\frac{1}{\sqrt{\pi}}\int_{0}^{\infty} y^{1/2-1}e^{-y}\, d{y}=
        \left( \frac{1}{\sqrt{\pi}}  \right)\Gamma\left( \frac{1}{2} \right)=1    \]
    \underline{Case 2}: For general $ \mu $ and $ \sigma^2 $,
    \[ \int_{-\infty}^{\infty}  \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{(x-\mu)^2}{2\sigma^2} \right\} \, d{x} \]
    Let $ z=\dfrac{x-\mu}{\sigma}\implies x=\mu+\sigma z  $
    and $ dx=\sigma\,dz $. Therefore,
    \[ =\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}\sigma}
        \exp\left\{ -\frac{z^2}{2}\right\}\sigma \, d{z}=
        \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{z^2}{2} \right\} \, d{z}=1   \]
    using Case 1.
\end{Example}
\section{Expectation}
\begin{Definition}{Expectation of discrete random variable}{}
    Suppose $ X $ is a discrete random variable with support
    $ A $ and p.f. $ f(x) $. Then,
    \[ \E{X}=\sum\limits_{x\in A}x f(x)  \]
    if $ \sum\limits_{x\in A}\abs{x}f(x) < \infty $ (finite).
    If $ \sum\limits_{x\in A}\abs{x}f(x) =\infty $ (infinite), then
    $ \E{X} $ does not exist.
\end{Definition}

\begin{Definition}{Expectation of continuous random variable}{}
    Suppose $ X $ is a continuous random variable with support $ A $
    and p.d.f. $ f(x) $. Then,
    \[ \E{X}=\int_{-\infty}^{\infty} x f(x)\, d{x}  \]
    if $ \displaystyle \int_{-\infty}^{\infty} \abs{x}f(x)\, d{x} <\infty $
    (finite). Similarly,
    if $ \displaystyle \int_{-\infty}^{\infty} \abs{x}f(x)\, d{x} =\infty $
    (infinite),
    then $ \E{X} $ does not exist.
\end{Definition}

\begin{Example}{Discrete}{}
    Suppose
    \[ f(x)=\frac{1}{x(x+1)}=\frac{1}{x} -\frac{1}{x+1}  \]
    for $ x=1,2,\ldots $. The support set is $ A=\set{1,2,\ldots} $.
    We note that $ f(x) $ is a p.f.\ since
    $ f(x)\geqslant 0 $ and
    \[ \sum\limits_{x\in A}f(x)=\sum\limits_{x=1}^{\infty}
        \left( \frac{1}{x} -\frac{1}{x+1} \right)=
        1-\frac{1}{2} +\frac{1}{2} -\frac{1}{3} +\cdots=1  \]
    Find $ \E{X} $.

    \textbf{Solution.}
    \[ \sum\limits_{x\in A}\abs{x}f(x)=\sum\limits_{x=1}^{\infty}
        x\left( \frac{1}{x} -\frac{1}{x+1} \right)
        =\sum\limits_{x=1}^{\infty} \frac{1}{x+1} =\infty  \]
    Therefore, $ \E{X} $ does not exist!
\end{Example}

\begin{Example}{Continuous}{}
    Let the p.d.f.\ be defined as $ f(x)=\dfrac{1}{x^2+1} $ for
    $ x\in\mathbb{R} $. This is known as the Cauchy distribution
    (or Student's T-distribution with 1 degree of freedom). Find $ \E{X} $.

    \textbf{Solution.}
    \[ \int_{-\infty}^{\infty} \abs{x}f(x)\, d{x} =
        \int_{-\infty}^{\infty} \abs{x}\frac{1}{x^2+1} \, d{x}=
        2 \int_{0}^{\infty} \frac{x}{x^2+1} \, d{x} =
        \bigl[\ln\abs{x^2+1}\bigr]_0^\infty=\infty \]
    $ \E{X} $ does not exist! The following is \underline{\textbf{wrong}}:
    \[ \E{X}=\int_{-\infty}^{\infty}x f(x)\, d{x}=
        \int_{-\infty}^{\infty} \frac{x}{1+x^2} \, d{x}=0  \]
    since the integral above with $ \abs{x} $ is infinite. You must
    always remember to check that the $ \E{X} $ is finite
    (using $ \abs{X} $) for both the discrete and continuous case.
\end{Example}

\begin{Example}{Bernoulli and Binomial Random Variable}{}
    Suppose $ X \sim \bern{p} $.
    \[ \Prob{X=1}=p\quad\text{ and }\quad \Prob{X=0}=1-p \]
    We know $ \E{X}=(1)\Prob{X=1}+(0)\Prob{X=0}=p $

    Now suppose
    $ X \sim \bin{n,p} $. Find $ \E{X} $.

    \textbf{Solution.}
    \[ \E{X}=\sum\limits_{x\in A}x f(x)=\sum\limits_{x=0}^{n} x
        \binom{n}{x}p^x(1-p)^{n-x}  \]
    This is hard to do. But, we know we can use the
    relationship between the Binomial and Bernoulli random variable
    so,
    \[ X=\sum\limits_{i=1}^{n} X_i \]
    Therefore,
    \[ \E{X}=\E[\bigg]{\sum\limits_{i=1}^{n} X_i}=\sum\limits_{i=1}^{n}
        \E{X_i}=np \]
\end{Example}
\begin{Example}{}{}
    Suppose for a random variable $ X $ the p.d.f.\ is given by
    $ f(x)=\dfrac{\theta}{x^{\theta+1}} $
    for $ x\geqslant 1 $ and $ 0 $ when $ x<1 $. Assume $ \theta>0 $.
    Find $ \E{X} $ and for what values of $ \theta $,
    does $ \E{X} $ exist.

    \textbf{Solution.}
    \[ \int_{-\infty}^{\infty} \abs{x}f(x)\, d{x}=
        \int_{1}^{\infty} (x) \frac{\theta}{x^{\theta+1}} \, d{x}
        =\theta \int_{1}^{\infty} \frac{1}{x^{\theta}} \, d{x} <\infty
        \iff \theta>1 \]
    from MATH 138. So, if $ \theta>1 $ then $ \E{X} $ exists. Also,
    \[ \E{X}=\int_{-\infty}^{\infty} xf(x)\, d{x}=
        \int_{1}^{\infty} \frac{\theta x}{x^{\theta+1}} \, d{x}
        =\theta \int_{1}^{\infty} \frac{1}{x^{\theta}} \, d{x}
        =\frac{\theta}{\theta-1}   \]

\end{Example}

\begin{Definition}{Expectation (Discrete)}{}
    If $ X $ is a discrete random variable with probability
    function $ f(x) $ and support set $ A $,
    then the \textbf{expectation} of the random variable $ g(X) $
    is defined by
    \[ \E{g(X)}=\sum\limits_{x\in A}g(x)f(x) \]
    provided the sum converges absolutely; that is, provided
    \[ \sum\limits_{x\in A}\abs{g(x)}f(x)<\infty \]
\end{Definition}

\begin{Definition}{Expectation (Continuous)}{}
    If $ X $ is a continuous random variable with p.d.f.
    $ f(x) $ and support set $ A $,
    then the \textbf{expectation} of the random variable $ g(X) $
    is defined by
    \[ \E{g(X)}=\int_{-\infty}^{\infty} g(x)f(x)\, d{x} \]
    provided the integral converges absolutely; that is, provided
    \[ \int_{-\infty}^{\infty} \abs{g(x)}f(x)\, d{x} <\infty \]
\end{Definition}

\begin{Theorem}{Expectation is a Linear Operator}{exp_linear_op}
    Suppose $ X $ is a random variable with probability (density)
    function $ f(x) $, and $ a $ and $ b $ are real constants,
    and $ g(x) $ and $ h(x) $ are real-valued functions. Then,
    \[ \E{aX+b}=a\E{X}+b \]
    \[ \E{a g(X)+b h(X)}=a\E{g(X)}+b\E{h(X)} \]
\end{Theorem}
\begin{Proof}{\ref{thm:exp_linear_op}}{}
    Omitted from the lecture and hence these notes.
    See Course Notes for most of the proof.
\end{Proof}
\begin{Definition}{Variance}{}
    The variance of a random variable is defined as
    \[ \sigma^2=\Var{X}=\E{(X-\mu)^2} \]
    where $ \mu=\E{X} $.
\end{Definition}
\begin{Definition}{Special Expectations}{}
    \begin{enumerate}[label=(\Roman*)]
        \item The $ k $th moment (about the origin) of a random variable
              \[ \E{X^k} \]
        \item The $ k $th moment about the mean of a random variable
              \[ \E{(X-\mu)^k} \]
    \end{enumerate}
\end{Definition}
\begin{Theorem}{Properties of Variance}{prop_var}
    If $ X $ is a random variable, then
    \[ \Var{X}=\E{X^2}-\mu^2 \]
    where $ \mu=\E{X} $. Note that the
    variance of $ X $ exists if $ \E{X^2}<\infty $.
\end{Theorem}
\begin{Proof}{\ref{thm:prop_var}}{}
    Omitted from the lecture and hence these notes.
    See Course Notes for most of the proof.
\end{Proof}

\begin{Example}{}{}
    Suppose $ X \sim \poi{\theta} $, the p.f.\ is defined as
    $ f(x)=\dfrac{\theta^x}{x!} e^{-\theta} $
    for $ x=0,1,2,\ldots $. Find $ \E{X} $ and $ \Var{X} $.

    \textbf{Solution.} The support is non-negative, so $ \abs{x}=x $.
    Therefore,
    \[ \E{X}=\sum\limits_{x=0}^{\infty} x \frac{\theta^x}{x!} e^{-\theta}
        =\sum\limits_{x=1}^{\infty} \frac{x}{x!} \theta^x e^{-\theta}
        =\theta \sum\limits_{x=1}^{\infty} \frac{\theta^{x-1}}{(x-1)!}
        e^{-\theta}  \]
    Let $ y=x-1 $, then
    \[ \E{X}= \theta\sum\limits_{y=0}^{\infty} \frac{\theta^y}{y!} e^{-\theta} \]
    We know $\displaystyle  e^{\theta}=\sum\limits_{y=0}^{\infty} \frac{\theta^y}{y!} $,
    so $ \E{X}= \theta $.

    \[ \Var{X}=\E{X^2}-\mu^2 \]
    Let's find $ \E{X^2} $:
    \begin{align*}
        \E{X^2}
         & =\sum\limits_{x=0}^{\infty} x^2 \frac{\theta^x}{x!} e^{-\theta}         \\
         & =\sum\limits_{x=1}^{\infty} \frac{x}{(x-1)!}\theta^x e^{-\theta}        \\
         & =\sum\limits_{x=1}^{\infty} \frac{(x-1)+1}{(x-1)!} \theta^x e^{-\theta} \\
         & =\sum\limits_{x=1}^{\infty} \frac{x-1}{(x-1)!}\theta^x e^{-\theta}+
        \sum\limits_{x=1}^{\infty} \frac{1}{(x-1)!}\theta^x e^{-\theta}
    \end{align*}
    Looking at the first sum (since the second sum was computed before):
    \[ \sum\limits_{x=2}^{\infty} \frac{\theta^2}{(x-2)!} \theta^{x-2}e^{-\theta}+\theta \]
    Let $ y=x-2 $:
    \[ \E{X^2}=\sum\limits_{y=0}^{\infty}\frac{\theta^2\theta^y}{y!}e^{-\theta}+\theta=
        \theta^2+\theta   \]
    Therefore,
    \[ \Var{X}=\E{X^2}-\mu^2=(\theta^2+\theta)-\theta^2=\theta \]
\end{Example}
