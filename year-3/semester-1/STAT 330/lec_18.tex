\makeheading{Lecture 18 | 2020-11-08}
\section{Convergence in Probability}
\begin{Definition}{Converges in Probability}{}
    Let $ X_1,\ldots,X_n $ be a sequence of random
    variables such that $ X_n $ has
    c.d.f.\ $ F_n(x) $. Let $ X $ be a random variable
    with c.d.f.\ $ F(x) $. If for any $ \varepsilon>0 $,
    \[ \lim\limits_{{n} \to {\infty}}
        \Prob*{\abs*{X_n-X}\ge \varepsilon}=0 \]
    or
    \[ \lim\limits_{{n} \to {\infty}}
        \Prob*{\abs*{X_n-X}<\varepsilon}=1 \]
    then we say $ X_n $ \textbf{converges in probability}
    to $ X $, and write $ X_n\stackrel{\mathbb{P}}{\to}X $.
\end{Definition}
\begin{Remark}{}{}
    \begin{enumerate}[label=(\roman*)]
        \item Here it's the convergence or limit for
              a probability, therefore it's called
              \underline{convergence in probability}.
        \item ``Meaning'' of $ X_n\stackrel{\mathbb{P}}{\to}X $.
              As $ n\to\infty $, $ X_n $ cannot be ``$ \varepsilon $''
              away from $ X $. That is, $ X_n $ becomes very
              close to $ X $ as $ n\to\infty $. Because of that,
              we expect that $ F_n(x) $ becomes very close to $ F(x) $.
    \end{enumerate}
\end{Remark}
\begin{Theorem}{Convergnece in Probability Implies Convergence in Distribution}{}
    If $ X_n\stackrel{\mathbb{P}}{\to}X $, then $ X_n\stackrel{\text{d}}{\to} X $.
\end{Theorem}
We consider a special case.
\begin{Definition}{Convergence in Probability to a Constant}{}
    Let $ X_1,\ldots,X_n $ be a sequence of random variables,
    and $ b $ be a constant. If $ \lim\limits_{{n} \to {\infty}}
        \Prob*{\abs*{X_n-b}\ge \varepsilon}=0 $
    for any $ \varepsilon>0 $. We say $ X_n $ \textbf{converges
        in probability} to $ b $, and write $ X_n\stackrel{\mathbb{P}}{\to}b $.
\end{Definition}
\begin{Theorem}{}{lim_dist_thm}
    Let $ X_1,\ldots,X_n $ be a sequence of random variables
    such that $ X_n $ has c.d.f.\ $ F_n(x) $. If
    \[ \lim\limits_{{n} \to {\infty}} F_n(x)=\begin{cases}
            0 & x<b \\
            1 & x>b
        \end{cases} \]
    or limiting distribution of $ X_n $ is
    \[ F(x)=\begin{cases}
            0 & x<b    \\
            1 & x\ge b
        \end{cases} \]
    (c.d.f.\ of $ X $, which satisfies $ \Prob{X=b}=1 $), then
    $ X_n\stackrel{\mathbb{P}}{\to}b $ and write
    $ X_n\stackrel{\text{d}}{\to}b $.

    In other words, $ X_n\stackrel{\text{d}}{\to}b $ implies
    $ X_n\stackrel{\mathbb{P}}{\to}b $. Therefore,
    \[ X_n\stackrel{\text{d}}{\to}b\iff X_n\stackrel{\mathbb{P}}{\to}b \]
\end{Theorem}
\begin{Proof}{\Cref{thm:lim_dist_thm}}{}
    For any $ \varepsilon>0 $, $ \Prob{\abs{X_n-b}\ge \varepsilon}\to 0 $
    as $ n\to\infty $.
    \begin{enumerate}[label=(\roman*)]
        \item Lower bound: $ \Prob{\abs{X_n-b}\ge \varepsilon}\ge 0 $
        \item Upper bound:
              \begin{align*}
                  \Prob{\abs{X_n-b}\ge \varepsilon}
                   & =\Prob{(X_n\ge b+\varepsilon)\cup (X_n\le b-\varepsilon)}       \\
                   & =1-\Prob{X_n<b+\varepsilon}+
                  \Uunderbracket{\Prob{X_n\le b-\varepsilon}}_{F_n(b-\varepsilon)}   \\
                   & \le 1-\Prob*{X_n\le b+\frac{\varepsilon}{2}}+F_n(b-\varepsilon) \\
                   & =1-F_n\biggl(b+\frac{\varepsilon}{2}\biggr)+F_n(b-\varepsilon)
              \end{align*}
              as $ n\to\infty $, $ \displaystyle F_n\biggl(b+\frac{\varepsilon}{2}\biggr)\ge 1 $
              and $ \lim\limits_{{n} \to {\infty}} F_n(b-\varepsilon)=0 $,
              so the upper bound will be $ 1-1+0=0 $, and hence
              \[ 0\le \lim\limits_{{n} \to {\infty}} \Prob{\abs{X_n-b}\ge \varepsilon}
                  \le 0 \]
              and hence
              \[ X_n\stackrel{\text{d}}{\to}b\iff X_n\stackrel{\mathbb{P}}{\to}b \]
    \end{enumerate}
\end{Proof}
\begin{Example}{}{}
    $ X_1,\ldots,X_n \stackrel{\text{iid}}{\sim}\uniform{0,1} $.
    In~\Cref{ex:lec_17ex1}, we showed that
    \[ \lim\limits_{{n} \to {\infty}} \Prob{X_{(1)}\le x}=
        \begin{cases}
            0 & x<0 \\
            1 & x>0
        \end{cases}\implies X_{(1)}\stackrel{\text{d}}{\to}0
        \implies X_{(1)}\stackrel{\mathbb{P}}{\to}0 \]
    \[ \lim\limits_{{n} \to {\infty}} \Prob{X_{(n)}\le x}=
        \begin{cases}
            0 & x<1 \\
            1 & x>1
        \end{cases}\implies X_{(n)}\stackrel{\text{d}}{\to}1\implies
        X_{(n)}\stackrel{\mathbb{P}}{\to}1 \]
\end{Example}
\begin{Example}{}{}
    $ X_1,\ldots,X_n $ are i.i.d with p.d.f.\
    $ f(x)=e^{-(x-\theta)} $, $ x>\theta $. Show that
    $ X_{(1)}\stackrel{\mathbb{P}}{\to}\theta $.

    \textbf{Solution 1.} Only need to show that $ X_{(1)}\stackrel{\text{d}}{\to}\theta $;
    that is,
    \[ \lim\limits_{{n} \to {\infty}} \Prob{X_{(1)}\le x}
        =\begin{cases}
            0 & x<\theta \\
            1 & x>\theta
        \end{cases} \]
    or limiting distribution of $ X_{(1)} $ is
    \[ F(x)=\begin{cases}
            0 & x< \theta   \\
            1 & x\ge \theta
        \end{cases} \]
    Support $ X_{(1)} $ is $ (\theta,\infty) $.
    $ \Prob{X_{(1)}\le x}=F_n(x) $.
    \begin{itemize}
        \item $ x\le \theta $, $ F_n(x)=0 $
        \item $ x>\theta $,
              \begin{align*}
                  \Prob{X_{(1)}\le x}
                   & =1-[\Prob{X_1>x}]^n \\
                   & =1-e^{-n(x-\theta)}
              \end{align*}
              since
              $ \displaystyle  \Prob{X_1>x}=\int_{x}^{\infty} e^{-(t-\theta)}\, d{t}=
                  e^{-(x-\theta)}  $. Therefore,
              \[ F_n(x)=\begin{cases}
                      0                  & x\le \theta \\
                      1-e^{-n(x-\theta)} & x>\theta
                  \end{cases}\implies
                  \lim\limits_{{n} \to {\infty}} F_n(x)=
                  \begin{cases}
                      0 & x<\theta \\
                      1 & x>\theta
                  \end{cases} \]
              So, $ X_{(1)}\stackrel{\text{d}}{\to}\theta\implies
                  X_{(1)}\stackrel{\mathbb{P}}{\to}\theta $.
    \end{itemize}
    \textbf{Solution 2.} By definition, for any $ \varepsilon>0 $,
    \begin{itemize}
        \item Lower bound: $ \Prob{\abs{X_{(1)}-\theta}\ge \varepsilon}\ge 0 $
        \item Upper bound:
              \begin{align*}
                  \Prob{\abs{X_{(1)}-\theta}\ge \varepsilon}
                   & =\Prob{(X_{(1)}\ge \theta+\varepsilon)\cup
                  (X_{(1)}\le \theta-\varepsilon)}                \\
                   & =\Prob{X_{(1)}\ge \theta+\varepsilon}+
                  \Prob{X_{(1)}\le \theta-\varepsilon}            \\
                   & =[\Prob{X_1>\theta+\varepsilon}]^n           \\
                   & =e^{-n(\theta+\varepsilon-\theta)}           \\
                   & =e^{-n\varepsilon}\to 0\text{ as }n\to\infty
              \end{align*}
              Therefore, $ \Prob{\abs{X_{(1)}-\theta}\ge \varepsilon}=0 $
              as $ n\to\infty $ which implies $ X_{(1)}\stackrel{\mathbb{P}}{\to}\theta $
              by definition.
    \end{itemize}
\end{Example}
Brief Summary:
\begin{itemize}
    \item Convergence in distribution.
    \item Convergence in probability.
    \item Special case. Convergence in probability
          to a \underline{constant} if and only if convergence to distribution.
    \item $ X_{(1)}=\min_{1\le i\le n}X_i $
          and $ X_{(n)}=\max_{1\le i\le n}X_i $.
\end{itemize}
Next, our main job is to study convergence in distribution
and probability $ \bar{X}_n=\frac{1}{n} \sum_{i=1}^{n} X_i $.

Sequence of results:
\begin{itemize}
    \item Convergence in probability of $ \bar{X}_n $,
          WLLN\@.
    \item Convergence in distribution of $ \sqrt{n}(\bar{X}_n-\mu) $. CLT\@.
    \item Combine them together: Slutsky's Theorem, Delta Method.
\end{itemize}
\begin{Theorem}{Markov's Inequality}{}
    Suppose that $ X $ is a random variable. For any $ k>0 $,
    $ c>0 $, we have
    \[ \Prob{\abs{X}\ge c}\le \frac{\E*{\abs{X}^k}}{c^k}  \]
\end{Theorem}
Markov's Inequality relates probability to moments.

In most situations, we take $ k=2 $; that is, we consider
\[ \Prob{\abs{X}\ge c}\le \frac{\E{X^2}}{c^2} \]
\begin{Example}{Weak Law of Large Numbers}{}
    Suppose $ X_1,\ldots,X_n $ are i.i.d.\ random variables
    with $ \E{X_i}=\mu $ and $ \Var{X_i}=\sigma^2<\infty $, then
    \[ \bar{X}_n=\frac{1}{n} \sum_{i=1}^{n} X_i\stackrel{\mathbb{P}}{\to}\mu \]
    \textbf{Solution.} By definition, we only need to show that
    for any $ \varepsilon>0 $,
    \[ \lim\limits_{{n} \to {\infty}} \Prob*{\abs*{\bar{X}_n-\mu}\ge \varepsilon}=0 \]
    Lower bound: $ \Prob*{\abs*{\bar{X}_n-\mu}\ge \varepsilon}\ge 0 $.

    Upper bound:
    \[ \Prob*{\abs*{\bar{X}_n-\mu}\ge \varepsilon}\le
        \frac{\E{(\bar{X}_n-\mu)^2}}{\varepsilon^2}  \]
    Aside: $ \E{\bar{X}_n}=\mu $, $ \Var{X_n}=\frac{\sigma^2}{n} $, so
    \[ \frac{\E{(\bar{X}_n-\mu)^2}}{\varepsilon^2}=
        \frac{\Var{\bar{X}_n}}{\varepsilon^2}=
        \frac{\sigma^2}{n\varepsilon^2}\to 0\text{ as }n\to\infty   \]
    By Squeeze Theorem, $ \Prob*{\abs*{\bar{X}_n-\mu}\ge \varepsilon}=0 $.
\end{Example}
\begin{Exercise}{Markov's Inequality}{}
    If $ X_1,\ldots,X_n $ are independent. $ \E{X_i}=\mu $
    and $ \Var{X_i}=\sigma_i^2 $ for $ i=1,\ldots,n $.
    $ \max_{1\le i\le n}\sigma_i^2\le c $.
    Show that
    \[ \bar{X}_n\stackrel{\mathbb{P}}{\to}\mu \]
\end{Exercise}
\begin{Example}{}{}
    If $ X_1,\ldots,X_n\stackrel{\text{iid}}{\sim}\chi^2(1) $,
    then $ \bar{X}_n\stackrel{\mathbb{P}}{\to}1 $.

    \textbf{Solution.} $ \E{X_1}=\cdots=\E{X_n}=1 $.

    From term test 1,
    $ \chi^2(1) $ m.g.f.\ is $ (1-2t)^{-1/2} $
    which is $ \text{Gamma}(\alpha=1/2,\beta=2) $,
    so $ \Var{\chi^2(1)}=\alpha\beta^2=(1/2)(2)^2=2 $. By
    WLLN, $ \bar{X}_n\stackrel{\mathbb{P}}{\to}\mu=1 $.
\end{Example}
\begin{Example}{}{}
    If $ Y_n \sim \chi^2(n) $, then $ \displaystyle \frac{Y_n}{n}
        \stackrel{\mathbb{P}}{\to} 1 $.

    \textbf{Solution.} We can write
    $ Y_n=\sum_{i=1}^{n} X_i $ where $ X_i\stackrel{\text{iid}}{\sim}\chi^2(1) $,
    then
    \[ \frac{Y_n}{n}=\frac{1}{n} \sum_{i=1}^{n} X_i=\bar{X}_n
        \stackrel{\mathbb{P}}{\to}1 \]
\end{Example}
\begin{Example}{}{}
    If $ X_1,\ldots,X_n\stackrel{\text{iid}}{\sim}\poi{\mu} $, then
    $ \bar{X}_n\stackrel{\mathbb{P}}{\to}\mu $.

    \textbf{Solution.} $ \E{X_i}=\mu<\infty $ and $ \Var{X_i}=\mu<\infty $,
    so by WLLN, $ \bar{X}_n\stackrel{\mathbb{P}}{\to}\mu  $.
\end{Example}
\begin{Exercise}{}{}
    If $ Y_n \sim \poi{n} $, then
    \[ \frac{Y_n}{n} \stackrel{\mathbb{P}}{\to}1 \]
    \textbf{Solution.} $ Y_n=\sum_{i=1}^{n} X_i $
    where $ X_i\stackrel{\text{iid}}{\sim}\poi{1} $,
    so by WLLN,
    \[ \frac{Y_n}{n} =\frac{1}{n}\sum_{i=1}^{n} X_i=
        \bar{X}_n \stackrel{\mathbb{P}}{\to}1 \]
\end{Exercise}
