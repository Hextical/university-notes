\makeheading{Lecture 2 | 2020-09-09}
\section{Discrete Random Variables}
\begin{Definition}{Discrete random variable}{}
    If a random variable $ X $ can only take
    finite or countable values, $ X $
    is a \textbf{discrete random variable}.
\end{Definition}
In this case, $ F(x) $ is a right-continuous
step function.
\begin{Remark}{}{}
    When we say \textbf{countable}, we mean
    something you can enumerate such as $ \mathbb{Z} $ or
    $ \mathbb{N}^+ $.
\end{Remark}

\begin{Definition}{Probability function}{}
    If $ X $ is a discrete random variable,
    then the \textbf{probability function} (p.f.) of $ X $ is given by
    \[ f(x)=
        \begin{dcases}
            P(X=x)=
            F(x)-\lim\limits_{{\varepsilon} \to {0^+}}
            F(x-\varepsilon) & \text{ if } X\text{ can take value }x    \\
            0                & \text{ if } X\text{ cannot take value }x
        \end{dcases} \]
\end{Definition}

\begin{Definition}{Support set}{}
    The set $ A=\set{x:f(x)>0} $ is called the \textbf{support set}
    of $ X $. These are all the positive values $ X $ can take.
\end{Definition}

\begin{Proposition}{Properties of the Probability Function}{ds_prob_prop}
    \begin{enumerate}[label=(\arabic*)]
        \item $ f(x)\geqslant 0 $ for $ x\in\mathbb{R} $
        \item $ \sum\limits_{x\in A}f(x)=1 $
    \end{enumerate}
\end{Proposition}

Review some commonly used discrete random variables:
\begin{itemize}
    \item \textbf{Bernoulli.} $ X \sim \text{Bernoulli}(p) $
          where $ X $ can only take two possible values 0 (failure) or 1 (success).
          Let $ p $ be the probability of a success for a single trial. So,
          \[ P(X=1)=p\text{ and }P(X=0)=1-p \]
          Therefore,
          \[ f(x)=P(X=x)=p^x(1-p)^{1-x} \]
          Example: Toss a coin twice. Let $ X $ be the number of heads.
          Then $ X \sim \text{Bernoulli}(p) $
    \item \textbf{Binomial.} $ X \sim \bin(n,p) $. Suppose
          we have \textbf{Bernoulli Trials}:
          \begin{itemize}
              \item We run $ n $ trials
              \item Each trial is independent of each other
              \item Each trial has two possible outcomes: 0 (failure), 1 (success)
          \end{itemize}
          \[ P(X=1)=p \]
          Let $ X $ be the number of success across these $ n $ trials
          and $ p $ be the success probability for a single trial.
          \[ X=\sum\limits_{i=1}^{n} X_i \]
          $ X_i $ is the outcome of the $ i $th trial.
          \[ P(X_i=1)=p \]
          where $ X_i \sim \text{Bernoulli}(p) $. Therefore,
          \[ f(x)=P(X=x)=\binom{n}{x}p^x(1-p)^{n-x} \]
    \item \textbf{Geometric.} $ X \sim \geo(p) $.
          Let $ X $ be the number of failures before the first success.
          $ X $ can take values $ 0,1,2,\ldots $.
          \[ f(x)=P(X=x)=(1-p)^x p \]
          Example. $ X = $ number of tails before you get the first
          head.
    \item \textbf{Negative Binomial.} $ X \sim \nb(r,p) $.
          Let $ X $ be the number of failures before you get $ r $ success.
          $ X $ can take values $ 0,1,2,\ldots $
          \[ f(x)=P(X=x)=\binom{x+r-1}{x}(1-p)^x p^{r-1}p \]
          Example. $ X= $ number of tails before you get the $ r $th head.
    \item \textbf{Poisson.} $ X \sim \poi(\mu) $
          where $ X = 0,1,\ldots $.
          \[ f(x)=P(X=x)=\frac{\mu^x}{x!} e^{-\mu} \]
          where $ x=0,1,2,\ldots $.
\end{itemize}

\begin{Exercise}{}{}
    Verify all that all the probability models above are indeed
    probability functions using~\Cref{prop:ds_prob_prop}.
\end{Exercise}
Solution. TODO

\section{Continuous Random Variables}
\begin{Definition}{Continuous random variable}{}
    Suppose $ X $ is a random variable with c.d.f. $ F $. If $ F $
    is a continuous function for all $ x\in\mathbb{R} $ and
    $ F $ is differentiable except possibly at
    countably many points, then $ X $ is called a
    \textbf{continuous random variable}.
\end{Definition}
Note that this is not a rigorous definition, but it will be
used in this course.

\begin{Definition}{Probability density function, Support set}{}
    The \textbf{probability density function} (p.d.f.) of a continuous random variable
    is
    \[ f(x)=\begin{cases}
            F^\prime(x) & \text{if }F(x)\text{ is differentiable at }x \\
            0           & \text{otherwise}
        \end{cases} \]
    The set $ A=\set{x:f(x)>0} $ is called the \textbf{support set}
    of $ X $.
\end{Definition}

Continuous case: $ f(x)\neq P(X=x) $
\[ P(x<X\leqslant x+\delta)\approx f(x)\delta \]
since
\[ \lim\limits_{{\delta} \to {0}}\frac{F(x+\delta)-F(x)}{\delta}=F^\prime(x)=f(x) \]
\begin{Definition}{Properties of the Probability Density Function}{}
    \begin{enumerate}[label=(\Roman*)]
        \item $ f(x)\geqslant 0 $ for all $ x\in\mathbb{R} $
        \item $ \displaystyle \int_{-\infty}^{\infty} f(x)\, d{x} =
                  \lim\limits_{{x} \to {\infty}} F(x)-
                  \lim\limits_{{x} \to {-\infty}} F(x)= 1 $
        \item $ f(x)=F^\prime(x)=
                  \lim\limits_{{h} \to {0}} \dfrac{F(x+h)-F(x)}{h} $
        \item $ F(x)=\displaystyle \int_{-\infty}^{x} f(t)\, d{t} $
              since $ F(-\infty)=0 $.
        \item $ P(a< X\leqslant b)=P(X\leqslant b)-
                  P(X\leqslant a)=F(b)-F(a)
                  =\displaystyle \int_{a}^{b} f(x)\, d{x}  $
        \item $ P(X=b)=F(b)-\lim\limits_{{a} \to {b^-}}
                  F(a)=F(b)-F(b) =0\neq f(b) $ since $ F $ is continuous.
    \end{enumerate}
\end{Definition}

\begin{Example}{}{}
    Suppose the c.d.f.\ of $ X $ is
    \[ F(x)=\begin{dcases}
            0               & x\leqslant a \\
            \frac{x-a}{b-a} & a<x<b        \\
            1               & x\geqslant b
        \end{dcases} \]
    Find the p.d.f.\ of $ X $.

    \textbf{Solution.}
    \[ f(x)=
        \begin{cases}
            \dfrac{1}{b-a} & a<x<b            \\
            0              & \text{otherwise}
        \end{cases} \]
    We note that $ X \sim \text{Uniform}(a,b) $
\end{Example}

\begin{Example}{}{}
    Let the p.d.f.\ be defined as follows.
    \[ f(x)=\begin{dcases}
            \frac{\theta}{x^{\theta+1}} & x\geqslant 1 \\
            0                           & x<1
        \end{dcases} \]
    \begin{enumerate}[label=(\roman*)]
        \item For what values of $ \theta $ is $ f $ a p.d.f.
        \item Find $ F(x) $.
        \item Find $ P(-2<X<3) $.
    \end{enumerate}
    \textbf{Solution.}

    \begin{enumerate}[label=(\roman*)]
        \item Note that $ \dfrac{\theta}{x^{\theta+1}} \geqslant 0 $
              for all $ \theta\geqslant 0 $.

              \underline{Case 1}: $ \theta=0 $. $ f(x)\equiv 0 $, then $ f $
              cannot be a pdf since $ \int_{-\infty}^{\infty} f(x)\, d{x}=0\neq 1 $

              \underline{Case 2}: $ \theta>0 $.
              \[ \int_{-\infty}^{\infty} f(x)\, d{x}=\int_{-\infty}^{1} f(x)\, d{x}
                  +\int_{1}^{\infty} f(x)\, d{x}=\int_{1}^{\infty} \frac{\theta}{x^{\theta+1}}\, d{x}
                  =\left[ -x^{-\theta} \right]_1^\infty=1 \]

              Therefore, $ f $ is a p.d.f.\ when $ \theta>0 $.

        \item $ F(x)=P(X\leqslant x) $.

              \underline{Case 1}: $ x<1 $.
              \[ P(X\leqslant x)=\int_{-\infty}^{x} f(t)\, d{t} =0 \]

              \underline{Case 2}:  $ x\geqslant 1 $.
              \[ P(X\leqslant x)=\int_{-\infty}^{x} f(t)\, d{t}
                  =\int_{-\infty}^{1} f(t)\, d{t} +\int_{1}^{x} f(t)\, d{t}
                  =\int_{1}^{x} \frac{\theta}{t^{\theta+1}} \, d{t}=\left[ -t^{-\theta} \right]_1^x=
                  1-x^{-\theta}  \]
        \item $ P(-2<X<3) $. Either use the c.d.f.\ we found or the p.d.f.

              \underline{Using the c.d.f.} we have
              \[ F(3)-F(-2)=(1-3^{-\theta})-0 \]

              \underline{Using the p.d.f.} we have
              \[ \int_{-2}^{3} f(x)\, d{x}=
                  \int_{-2}^{1} f(x)\, d{x} +\int_{1}^{3} f(x)\, d{x}
                  =\int_{1}^{3} f(x)\, d{x}=\text{exercise} \]
    \end{enumerate}

\end{Example}
