\makeheading{Lecture 23 | 2020-11-29}
\begin{Example}{}{}
    Suppose $ X_1,\ldots,X_n\stackrel{\text{iid}}{\sim}\N{\mu,\sigma^2} $.
    Find ML estimator of $ \theta $.

    \textbf{Solution.}
    \[ L(\mu,\sigma^2)=\prod_{i=1}^n f(x_i;\mu,\sigma^2) \]
    \begin{align*}
        \ell(\mu,\sigma^2)
         & =\sum_{i=1}^{n}\ln[f(x_i;\mu,\sigma^2)]                  \\
         & =\sum_{i=1}^{n} \ln\biggl[\frac{1}{\sqrt{2\pi \sigma^2}}
        \expon*{-\frac{(x_i-\mu)^2}{2\sigma^2}} \biggr]             \\
         & =\sum_{i=1}^{n}
        \biggl[-\frac{(x_i-\mu)^2}{2\sigma^2}-
            \frac{1}{2} \ln(2\pi\sigma^2) \biggr]
    \end{align*}
    \[ \frac{\partial\ell}{\partial\mu}=
        \frac{\sum_{i=1}^{n} (\mu-x_i)}{\sigma^2}   \]
    \[ \frac{\partial\ell}{\partial \sigma^2}=
        \frac{\sum_{i=1}^{n} (x_i-\mu)^2}{2\sigma^4}-
        \biggl(\frac{n}{2}\biggr)\biggl(\frac{1}{\sigma^2} \biggr)   \]
    ML estimate satisfies
    \begin{align*}
        \frac{\sum_{i=1}^{n} (X_i-\hat{\mu})^2}{\hat{\sigma}^2}    & =0 \\
        \frac{\sum_{i=1}^{n} (X_i-\hat{\mu})}{2(\hat{\sigma}^2)^2}-
        \biggl(\frac{n}{2}\biggr)\biggl(\frac{1}{\sigma^2} \biggr) & =0
    \end{align*}
    \begin{align*}
        \hat{\mu}      & =\frac{1}{n} \sum_{i=1}^{n} X_i               \\
        \hat{\sigma}^2 & =\frac{1}{n} \sum_{i=1}^{n} (X_i-\hat{\mu})^2
    \end{align*}
    ML estimator of $ (\mu,\sigma^2) $ is
    \begin{align*}
        \hat{\mu}    & =\frac{1}{n} \sum_{i=1}^{n}X_i=\bar{X}_n     \\
        \hat{\sigma} & =\frac{1}{n}\sum_{i=1}^{n} (X_i-\bar{X}_n)^2
    \end{align*}
    which is the same as MM estimator.
\end{Example}
\begin{Example}{}{}
    \[ f(x,\theta)=\begin{dcases}
            \frac{1}{\theta} & 0\le x\le \theta \\
            0                & \text{otherwise}
        \end{dcases} \]
    Note that the support of $ X $ depends on $ \theta $.
    Find ML estimator of $ \theta $.

    \textbf{Solution.}
    \[ L(\theta)=\prod_{i=1}^n f(x_i;\theta)
        =\begin{dcases}
            \biggl(\frac{1}{\theta} \biggr)^{\!n} & 0\le x_1,\ldots,x_n\le \theta \\
            0                                     & \text{otherwise}
        \end{dcases}=
        \begin{dcases}
            \frac{1}{\theta^n} & 0\le x_{(1)},x_{(n)}\le \theta \\
            0                  & \text{otherwise}
        \end{dcases} \]
    \begin{itemize}
        \item $ \theta<x_{(n)} $, $ L(\theta)=0 $
        \item $ \theta\ge x_{(n)} $, $ L(\theta) $
              is a strictly monotone decreasing function of $ \theta $.
    \end{itemize}
    This implies that the ML estimate of $ \theta $
    is
    \[ x_{(n)}=\max(x_1,\ldots,x_n) \]
    ML estimator of $ \theta $ is
    \[ \hat{\theta}=\max_{1\le i\le n}X_i=X_{(n)} \]
    is different from the MM estimator $ \hat{\theta}_{\text{MM}}=2\bar{X}_n $.

    Which estimator is better? $ \hat{\theta}_{\text{MM}} $
    or $ \hat{\theta}_{\text{ML}} $? STAT 450 covers this.
    \begin{itemize}
        \item Biased or unbiased estimator. Let $ \hat{\theta} $
              denote one estimator of $ \theta $. If $ \E{\hat{\theta}}=\theta $,
              then $ \hat{\theta} $ is an unbiased estimator of $ \theta $.
              Otherwise, $ \hat{\theta} $ is a biased estimator of $ \theta $.
    \end{itemize}
\end{Example}
\section{Properties of ML Estimator}
In this section:
\begin{enumerate}
    \item We only consider the case that the support of $ X_1,\ldots,X_n $
          does not depend on $ \theta $.
    \item We talk about random variables, only concerned
          about ML estimator.
    \item We only consider $ \theta $ is 1-dimensional or $ \theta $
          is a scalar.
\end{enumerate}
We define some notation first.
\begin{Definition}{Score Function}{}
    The \textbf{score function} is defined as
    \[ S(\theta)=S(\theta;\symbf{x})=
        \frac{d}{d\theta}\ell(\theta)=
        \frac{d}{d\theta}\ln[L(\theta)]   \]
    where $ \symbf{x} $ are the observed data.
    When the support of $ X_1,\ldots,X_n $ does not depend
    on $ \theta $, then $ S(\hat{\theta})=0 $.
\end{Definition}
\begin{Definition}{Information Function}{}
    The \textbf{information function} is defined as
    \[ I(\theta)=I(\theta;\symbf{x})=-\frac{d^2}{d\theta^2}\ell(\theta)=
        -\frac{d^2}{d\theta^2}\ln[L(\theta)]   \]
    where $ \symbf{x} $ are the observed data. $ I(\hat{\theta}) $
    is called the \textbf{observed information}.
\end{Definition}
\begin{Definition}{Fisher Information/Expected Information}{}
    The \textbf{fisher information} (\textbf{expected information})
    is defined as
    \[ J(\theta)=\E{I(\theta;\symbf{X})}=-\E*{\frac{d^2}{d\theta^2}\ell(\theta;
            \symbf{X}) } \]
    where $ \symbf{X} $ is the potential data.

    In particular, when $ \symbf{X}=(X_1,\ldots,X_n) $
    is i.i.d.\ from $ f(x,\theta) $, then
    \[ \ell(\theta;\symbf{x})=\sum_{i=1}^{n} \ln[f(x_i;\theta)] \]
    \[ I(\theta;\symbf{X})=-\frac{d^2
        }{d\theta^2} \sum_{i=1}^{n}\ln[f(X_i;\theta)]
        =-\sum_{i=1}^{n} \frac{d^2}{d\theta^2}\ln[f(X_i;\theta)]   \]
    Therefore,
    \[ J(\theta)=\E*{-\sum_{i=1}^{n} \frac{d^2}{d\theta^2}\ln[f(X_i;\theta)]}
        =-\E*{\frac{d^2}{d\theta^2}\ln[f(X_1;\theta)]} \]
\end{Definition}
\begin{Definition}{Fisher Information of One Observation}{}
    The \textbf{fisher information of one observation}
    is
    \[ J_1(\theta)=-n\E*{\frac{d^2}{d\theta^2}\ln[f(X_1;\theta)]}  \]
    The \textbf{fisher information in $ \symbf{n} $ observations} is
    \[ J(\theta)=n J_1(\theta) \]
\end{Definition}
\begin{Example}{}{}
    Suppose $ X_1,\ldots,X_n\stackrel{\text{iid}}{\sim}\poi{\theta} $.

    \[ L(\theta;\symbf{x})=\prod_{i=1}^n f(x_i;\theta) \]
    \[ \ell(\theta;\symbf{x})=\sum_{i=1}^{n}
        \ln[f(x_i;\theta)]=\sum_{i=1}^{n}
        \ln\biggl[\frac{\theta^{x_i}e^{-\theta}}{x!} \biggr]=
        \biggl(\,\sum_{i=1}^{n} x_i\biggr)\ln(\theta)-
        n\ln(\theta)-\sum_{i=1}^{n} \ln(x_i!) \]
    Score function:
    \[ S(\theta;\symbf{x})=\frac{\partial}{\partial\theta}\ell(\theta;\symbf{x})
        =\frac{\sum_{i=1}^{n} x_i}{\theta}-n   \]
    Observed information function:
    \[ I(\theta;\symbf{x})=
        -\frac{\partial S}{\partial \theta}S(\theta;\symbf{x})
        =\frac{\sum_{i=1}^{n} x_i}{\theta^2}   \]
    Fisher information:
    \[ J(\theta)=\E{I(\theta;\symbf{X})}=
        \E*{\frac{\sum_{i=1}^{n} X_i}{\theta^2}}=
        \frac{n\E{X_1}}{\theta^2}=\frac{n\theta}{\theta^2}=\frac{n}{\theta}  \]
    Recall that: $ \displaystyle \hat{\theta}_{\text{ML}}=\frac{\sum_{i=1}^{n} X_i}{n} $
    \[ \implies \Var{\hat{\theta}_{\text{ML}}}=
        \frac{\Var{X_i}}{n}=\frac{\theta}{n}  \]
    Is there any relationship between $ J(\theta) $
    and $ \Var{\hat{\theta}_{\text{ML}}} $?
\end{Example}
\begin{Theorem}{Cramér–Rao Bound}{}
    The variance of any unbiased estimator
    $ \hat{\theta} $ of $ \theta $ is bounded by the reciprocal
    of the Fisher information $ J(\theta) $:
    \[ \Var{\theta}\ge \frac{1}{J(\theta)}  \]
\end{Theorem}
\begin{Corollary}{}{}
    If $ T $ is an unbiased estimator of $ g(\theta) $, then
    \[ \Var{T}\ge \frac{[g^\prime(\theta)]^2}{J(\theta)} \]
\end{Corollary}
\begin{Theorem}{}{}
    ML estimator satisfies (when support of $ X_1,\ldots,X_n $
    does not depend on $ \theta $)
    \begin{enumerate}[label=(\arabic*)]
        \item $ \hat{\theta}\stackrel{\mathbb{P}}{\to}\theta $
              as $ n\to\infty $.
        \item $ \displaystyle \sqrt{n}(\hat{\theta}-\theta)\stackrel{\text{d}}{\to}
                  \N*{0,\frac{1}{J_1(\theta)}} $
        \item By delta-method,
              $ \displaystyle \sqrt{n}(g(\hat{\theta})-g(\theta))\stackrel{\text{d}}{\to}
                  \N*{0,\frac{[g^\prime(\theta)]^2}{J_1(\theta)} } $
    \end{enumerate}
\end{Theorem}
\begin{Remark}{}{}
    (1) Tells us that $ \hat{\theta} $ is close to $ \theta $
    as $ n\to\infty $.

    (2) Tells us that $ \displaystyle \sqrt{n}(\hat{\theta}-\theta)\approx
        \N*{0,\frac{1}{J_1(\theta)}}\implies
        \hat{\theta}\approx \N*{\theta,\frac{1}{nJ_1(\theta)}}=\N*{
            \theta,\frac{1}{J(\theta)}
        } $
    \[ \Var{\hat{\theta}}\approx \frac{1}{J(\theta)}  \]
    which is the CR lower-bound. $ \E{\hat{\theta}}\approx \theta $.
    \begin{itemize}
        \item $ \hat{\theta} $ is asymptotically unbiased.
        \item $ \hat{\theta} $ is asymptotically efficient.
    \end{itemize}

    (3) Tells us that $ \displaystyle  g(\hat{\theta})\approx \N*{g(\theta),
            \frac{[g^\prime(\theta)]^2}{J(\theta)}} $.
    \begin{itemize}
        \item $ g(\hat{\theta}) $ is asymptotically unbiased.
        \item $ \displaystyle \Var{g(\hat{\theta})}\approx
                  \frac{[g^\prime(\theta)]^2}{J(\theta)} $
              which is the CR lower-bound.
    \end{itemize}
\end{Remark}
Conclusion: ML estimator is asymptotically optimal.
