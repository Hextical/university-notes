\setcounter{chapter}{1}
\chapter{Univariate Random Variable}
\makeheading{Lecture 1 | 2020-09-09}
Review probability model, random variable (r.v.),
expectation, and moment generating function.

\section{Probability Model and Random Variable}
\begin{Definition}{Probability model}{}
    A \textbf{probability model} is used for a random
    experiment and consists of three components:
    \begin{enumerate}[label=(\Roman*)]
        \item Sample space
        \item Event
        \item Probability (density) function
    \end{enumerate}
\end{Definition}

\begin{Definition}{Sample space}{}
    A \textbf{sample space} $ S $ is
    the collection of all possible outcomes of one single
    random experiment.
\end{Definition}
\begin{Definition}{Event}{}
    An \textbf{event} is a subset of $ S $ and is denoted
    by $ A $.
\end{Definition}

\begin{Example}{}{}
    Toss a coin twice. This is a random experiment because
    we do not know the outcome before we toss the coin twice.
    \begin{itemize}
        \item $ S=\left\{ (H,H),(H,T),(T,H),(T,T)\right\} $
    \end{itemize}
    Define $ A $: First toss is an $ H $.

    Clearly, $ A=\left\{ (H,H),(H,T)\right\}\subseteq S $, so $ A $ is an event.
\end{Example}

\begin{Definition}{Probability function}{psf_def}
    A \textbf{probability function} $ \Prob{} $
    is a function that satisfies the following axioms:
    \begin{enumerate}[label=(\Roman*)]
        \item\label{psf_def_1} $ \Prob{A}\geqslant 0 $ for any event $ A $
        \item\label{psf_def_2} $ \Prob{S}=1 $
        \item\label{psf_def_3} \emph{Additivity property}: If $ A_1,A_2,A_3,\ldots $
              are pairwise mutually exclusive events; that is, $ A_i\cap A_j=\varnothing $
              for all $ i\neq j $, then
              \[ \Prob[\bigg]{\bigcup_{i=1}^{\infty}A_i}=\sum\limits_{i=1}^{\infty} \Prob{A_i} \]
    \end{enumerate}
\end{Definition}

\begin{Example}{}{}
    Toss a coin twice, given one event $ A $,
    \[ \Prob{A}=\frac{\text{\# of outcomes in }A}{4}  \]
    since $ |S|= 4 $. $ \Prob{} $ satisfies the three properties,
    therefore $ \Prob{} $ is a probability function.
\end{Example}

\begin{Proposition}{Additional Properties of the Probability Set Function}{add_prop_psf}
    Let $ A $ and $ B $ be events with sample space $ S $
    and let $ \Prob{} $ be a probability function, then
    \begin{enumerate}[label=(\arabic*)]
        \item\label{add_prop_psf_1} $ \Prob{\varnothing}=0 $
        \item\label{add_prop_psf_2} If $ A $ and $ B $ are mutually exclusive events, then $ \Prob{A\cup B}=\Prob{A}+\Prob{B} $
        \item\label{add_prop_psf_3} $ \Prob{\bar{A}}=1-\Prob{A} $
        \item\label{add_prop_psf_4} If $ A\subseteq B $, then $ \Prob{A}\leqslant \Prob{B} $
    \end{enumerate}
\end{Proposition}
\begin{Proof}{\ref{prop:add_prop_psf}}{}
    Proof of~\ref{add_prop_psf_1}: Let $ A_1=S $ and $ A_i=\varnothing $ for $ i=2,3,\ldots $.
    Since $ \bigcup\limits_{i=1}^{\infty}A_i=S $, then by~\ref{psf_def_3}
    it follows that
    \[ \Prob{S}=\Prob{S}+\sum\limits_{i=2}^{\infty} \Prob{\varnothing} \]
    and by~\ref{psf_def_2} we have
    \[ 1=1+\sum\limits_{i=2}^{\infty}\Prob{\varnothing} \]
    By~\ref{psf_def_1} the right side is a series of non-negative numbers which must converge to
    the left side which is 1 which is finite which results in a contradiction
    unless $ \Prob{\varnothing}=0 $ as required.

    Proof of~\ref{add_prop_psf_2}: Let $ A_1=A $, $ A_2=B $, and $ A_i=\varnothing $ for
    $ i=3,4,\ldots $. Since $ \bigcup\limits_{i=1}^\infty A_i=A\cup B $, then by~\ref{psf_def_3}
    \[ \Prob{A\cup B}=\Prob{A}+\Prob{B}+\sum\limits_{i=3}^{\infty} \Prob{\varnothing} \]
    and since $ \Prob{\varnothing}=0 $ by the result of~\ref{add_prop_psf_1} it follows that
    \[ \Prob{A\cup B}=\Prob{A}+\Prob{B} \]
    Proof of~\ref{add_prop_psf_3}: Since $ S=A\cup \bar{A} $ and $ A\cap \bar{A}=\varnothing $
    then by~\ref{psf_def_2} and by~\ref{add_prop_psf_2} it follows that
    \[ 1=\Prob{S}=\Prob{A\cup\bar{A}}=\Prob{A}+\Prob{\bar{A}} \]
    as required.

    Proof of~\ref{add_prop_psf_4}: Since
    \[ B=(A\cap B)\cup (\bar{A}\cap B)=A\cup(\bar{A}\cap B) \]
    and $ A\cap(\bar{A}\cap B)=\varnothing $ then by~\ref{add_prop_psf_2}
    \[ \Prob{B}=\Prob{A}+\Prob{\bar{A}\cap B} \]
    But by~\ref{psf_def_1}, $ \Prob{\bar{A}\cap B}\geqslant 0 $, so the result now follows.
\end{Proof}

\begin{Exercise}{}{}
    Let $ A $ and $ B $ be events with sample space $ S $
    and let $ \Prob{} $ be a probability function, then prove the following:
    \begin{enumerate}
        \item $ 0\leqslant \Prob{A}\leqslant 1 $
        \item $ \Prob{A\cap\bar{B}}=\Prob{A}-\Prob{A\cap B} $
        \item $ \Prob{A\cup B}=\Prob{A}+\Prob{B}-\Prob{A\cap B} $
    \end{enumerate}
\end{Exercise}
\begin{enumerate}
    \item $ \Prob{A}\geqslant 0 $ follows from~\ref{psf_def_1}. From~\ref{add_prop_psf_3}
          we have $ \Prob{\bar{A}}=1-\Prob{A} $. But from~\ref{psf_def_1} $ \Prob{\bar{A}}=\geqslant 0 $
          and therefore $ \Prob{A}\leqslant 1 $.
    \item Since $ A=(A\cap B)\cup (A\cap \bar{B}) $ and $ (A\cap B)\cap (A\cap \bar{B})
              =\varnothing $, then by~\ref{add_prop_psf_2}
          \[ \Prob{A}=\Prob{A\cap B}+\Prob{A\cap\bar{B}} \]
          as required.
    \item $ \Prob{A\cup B}=(A\cap \bar{B})+\Prob{A\cap B}+\Prob{\bar{A}\cap B} $. By the previous
          result,
          \[ \Prob{A\cap \bar{B}}=\Prob{A}-\Prob{A\cap B}\quad\text{ and }\quad\Prob{\bar{A}\cap B}=\Prob{B}-\Prob{A\cap B}\]
          Therefore,
          \begin{align*}
              \Prob{A\cup B} & =(\Prob{A}-\Prob{A\cap B})+\Prob{A\cap B}+(\Prob{B}-\Prob{A\cap B}) \\
                             & =\Prob{A}+\Prob{B}-\Prob{A\cap B}
          \end{align*}
          as required.
\end{enumerate}

\begin{Definition}{Conditional probability}{}
    Suppose $ A $ and $ B $ are two events with
    $ \Prob{B}>0 $. Then the \textbf{conditional probability}
    of $ A $ given that $ B $ has occurred is
    \[ \Prob{A\given B}=\frac{\Prob{A\cap B}}{\Prob{B}} \]
\end{Definition}

\begin{Definition}{Independent events}{}
    Suppose $ A $ and $ B $ are two events. $ A $ and
    $ B $ are \textbf{independent events} if and only if
    \[ \Prob{A\cap B}=\Prob{A}\Prob{B} \]
\end{Definition}

Clearly, $ \Prob{A\given B}=\Prob{A} $ if and only if $ A $ and $ B $ are independent since
\[ \Prob{A\given B}=\frac{\Prob{A\cap B}}{\Prob{B}}=\frac{\Prob{A}\Prob{B}}{\Prob{B}}=\Prob{A}  \]

\begin{Example}{}{}
    Toss a coin twice.
    \begin{itemize}
        \item $ A $: First toss is $ H $
        \item $ B $: Second toss is $ T $
    \end{itemize}
    \[ \Prob{A}=\frac{\text{\# of outcomes in }A}{4}=\frac{2}{4}\quad
        \text{ and }\quad \Prob{B}=\frac{2}{4} \]
    \[ \Prob{A\cap B}=\frac{1}{4}=\Prob{A}\Prob{B} \]
    therefore $ A $ and $ B $ are independent.
\end{Example}

\begin{Definition}{Random variable}{}
    A \textbf{random variable} (r.v.) $ X $
    is a function from a sample space $ S $ to the real numbers $ \mathbb{R} $; that is,
    \[ X:S\to \mathbb{R} \] satisfies for any given $ x\in\mathbb{R} $
    $ \set{X\leqslant x} $ is an event.
    \[ \set{X\leqslant x}=\set{\omega\in S:X(\omega)\leqslant x}
        \subseteq S \]
\end{Definition}

\begin{Example}{}{}
    Toss a coin twice. Let $ X $ be the number of heads ($ H $) in two tosses.
    Verify that $ X $ is a random variable.

    \textbf{Solution.}
    Possible values of $ X $: $ 0,1,2 $. Given $ x\in\mathbb{R} $,
    $ \set{X\leqslant x} $.
    \begin{itemize}
        \item $ x<0 \implies \set{X\leqslant x}=\varnothing $
        \item $ x=0 \implies \set{X\leqslant x}=\set{X=0}=\set{(T,T)}\subseteq S$
        \item $ x=1 \implies \set{X\leqslant x}=\set{X=1}=\set{(H,T),(T,H)}
                  \subseteq S $
        \item $ x=2 \implies \set{X\leqslant x}=\set{X=2}=
                  \set{(H,H)}\subseteq S $
    \end{itemize}
    Thus, $ X $ is a random variable.
\end{Example}

\begin{Definition}{Cumulative distribution function}{}
    The \textbf{cumulative distribution function} (c.d.f.) of a random variable
    $ X $ is defined by
    \[ F(x)=\Prob{X\leqslant x} \]
    for all $ x\in\mathbb{R} $. Note that the c.d.f.\ is defined for all $ \mathbb{R} $.
\end{Definition}

\begin{Definition}{Properties --- Cumulative Distribution Function}{}
    \begin{enumerate}[label=(\arabic*)]
        \item $ F $ is a non-decreasing function; that is, if $ x_1\leqslant x_2 $,
              then $ F(x_1)\leqslant F(x_2) $.

              By looking at:
              \begin{itemize}
                  \item $ \set{X\leqslant x_1}\subseteq \set{X\leqslant x_2} $
                        if $ x_1\leqslant x_2 $.
              \end{itemize}
        \item $ \lim\limits_{{x} \to {\infty}} F(x)=1 $
              and $ \lim\limits_{{x} \to {-\infty}} F(x)=0 $.

              By looking at:
              \begin{itemize}
                  \item $ x\to\infty $: $ \set{X\leqslant x}\to S $
                  \item $ x\to-\infty $: $ \set{X\leqslant x}\to \varnothing $
              \end{itemize}
        \item $ F(x) $ is a right continuous function; that is,
              for any $ a\in\mathbb{R} $,
              \[ \lim\limits_{{x} \to {a^+}} F(a)=F(a) \]
        \item For all $ a<b $
              \[ \Prob{a<X\leqslant b}=\Prob{X\leqslant b}-\Prob{X\leqslant a}=F(b)-F(a) \]
        \item For all $ b $
              \[ \Prob{X=b}=\Prob{\text{jump at }b}=\lim\limits_{{t} \to {b^+}} F(t)-
                  \lim\limits_{{t} \to {b^-}} F(t)=F(b)-\lim\limits_{{t} \to {b^-}} F(t) \]
    \end{enumerate}
\end{Definition}
