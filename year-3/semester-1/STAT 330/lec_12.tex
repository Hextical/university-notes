\makeheading{Lecture 12 | 2020-10-18}
Today: introduce two commonly
used multivariate random variables.
\section{Multinomial Distribution}
\begin{Definition}{Multinomial distribution}{}
    $ (X_1,\ldots,X_k) $ are joint discrete
    random variables with joint p.f.\ given by
    \[ f(x_1,\ldots,x_k)=P(X_1=x_1,\ldots,X_k=x_k)=
        \frac{n!}{x_1!x_2!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k} \]
    where $ x_i=0,1,\ldots,n $ ($ i=1,2,\ldots,k $). Furthermore,
    $ \sum_{i=1}^{k}x_i=n $, $ \sum_{i=1}^{k} p_i=1 $,
    for $ 0<p_i<1 $ $ i=1,\ldots,k $. Then,
    $ (X_1,\ldots,X_k) $ follows a \textbf{multinomial distribution}.
    \[ (X_1,\ldots,X_k)\sim \mult{n;p_1,\ldots,p_k} \]
\end{Definition}
\begin{Example}{Possible Application}{}
    \begin{itemize}
        \item there are $ k $ boxes and each box
              has same balls
        \item probability of choosing a ball from the
              $ i^{\text{th}} $ box is $ p_i $ for $ i=1,2,\ldots,k $.
        \item randomly choose $ n $ balls from $ k $ boxes.
    \end{itemize}
    Let $ X_i\coloneq $ number of boxes from the
    $ i^{\text{th}} $ box for $ i=1,2,\ldots,k $. Then,
    \[ (X_1,\ldots,X_k) \sim \mult{n;p_1,\ldots,p_k} \]
    Note: if there are only two boxes, then $ X_1\sim \bin{n,p_1} $.
\end{Example}
\begin{Proposition}{Properties --- Multinomial Distribution}{}
    Suppose that $ (X_1,\ldots,X_k) \sim \mult{n;p_1,\ldots,p_k} $.
    \begin{enumerate}[label=(\arabic*)]
        \item $ M(t_1,\ldots,t_k)=\E{e^{t_1 X_1+\cdots+t_k X_k}}=
                  (p_1e^{t_1}+\cdots+p_k e^{t_k})^n $
              where $ \abs{t_i}<\infty $ for $ i=1,\ldots,k $.
        \item Marginally, $ X_i \sim \bin{n,p_i} $
              for $ i=1,\ldots,k $.
        \item If $ T=X_i+X_j $, for $ i\neq j $. Then,
              \[ T \sim \bin{n,p_i+p_j} \]
        \item Joint moments:
              \[ \E{X_i}=n p_i \]
              \[ \Var{X_i}=np_i(1-p_i) \]
              \[ \Var{X_j}=np_j(1-p_j) \]
              Note: $ X_i+X_j \sim \bin{n,p_i+p_j} $, therefore
              \[ \Var{X_i+X_j}=n(p_i+p_j)(1-p_i-p_j) \]
              So,
              \[ \Cov{X_i+X_j,X_i+X_j}=\Var{X_i}+\Var{X_j}+2\Cov{X_i,X_j} \]
              \[ \implies n(p_i+p_j)(1-p_i-p_j)=np_i(1-p_i)+\Var{X_j}=np_j(1-p_j)
                  +2\Cov{X_i,X_j} \]
              Therefore,
              \[ \Cov{X_i,X_j}=-n p_i p_j \]
        \item $ X_i\mid X_j=x_j $. $ x_j $ outcomes
              from the $ j^{\text{th}} $ category. There are
              $ (n-x_j) $ balls chosen from the other $ (k-1) $ boxes.
              \[ X_i\mid X_j=x_i \sim \bin*{n-x_j,
                      \frac{p_i}{1-p_j}} \]
        \item $ X_i\mid X_i+X_j=t $.
    \end{enumerate}
\end{Proposition}

\section{Bivariate Normal Distribution}
\begin{Definition}{Bivariate normal distribution}{}
    Suppose that $ X_1 $ and $ X_2 $
    are continuous random variables with joint probability
    density function
    \[ f(x_1,x_2)=\frac{1}{2\pi\abs*{\Sigma}^{1/2}}
        \exp\left\{ -\frac{(\symbf{x}-\symbf{\mu})^\top \Sigma^{-1}(\symbf{x}-\symbf{\mu})^\top
        }{2} \right\}  \]
    with $ \abs{x_1},\abs{x_2}<\infty $. Here,
    \[ \symbf{x}=\begin{bmatrix}
            x_1 \\
            x_2
        \end{bmatrix},\quad
        \symbf{\mu}=\begin{bmatrix}
            \mu_1 \\
            \mu_2
        \end{bmatrix},\quad
        \Sigma=
        \begin{bmatrix}
            \sigma_1^2        & p\sigma_1\sigma_2 \\
            p\sigma_1\sigma_2 & \sigma_2^2
        \end{bmatrix} \]
    where $ \Sigma $ is positive definite. Also,
    $ \abs*{\Sigma} $ is the determinant of $ \Sigma $.
    Then, $ \symbf{X}=(X_1,X_2) $ follows a bivariate normal distribution,
    denoted as,
    \[ \symbf{X}\sim\Bvn{\symbf{\mu},\Sigma}  \]
\end{Definition}
\begin{Remark}{}{}
    Alternatively, we could write
    \[ f(x_1,x_2)=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}
        \exp\left\{ -\frac{1}{2(1-\rho^2)}
        \left[ \left( \frac{x_1-\mu_1}{\sigma_1}  \right)^2+
            \left( \frac{x_2-\mu_2}{\sigma_2}  \right)^2-
            2\left( \frac{\rho(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1\sigma_2} \right) \right] \right\}  \]
    as we found the determinant of $ \Sigma $ and calculated the quadratic form.
    We do not need to know this in the course.
\end{Remark}
\begin{Proposition}{Properties --- Bivariate Normal Distribution}{}
    \begin{enumerate}[label=(\arabic*)]
        \item Joint m.g.f.
              \[ M(t_1,t_2)=\E{
                      e^{t_1X_1+t_2X_2}
                  }=\exp\left\{ \symbf{t}^\top \symbf{\mu}+\frac{1}{2}\symbf{t}^\top \Sigma
                  \symbf{t} \right\} \]
              where $ t=\begin{pmatrix}
                      t_1 \\
                      t_2
                  \end{pmatrix}\in\mathbb{R}^2 $
        \item Marginally,
              \[ M_{X_1}(t_1)=M(t_1,0)=\exp\left\{ t_1\mu_1+\frac{1}{2} t_1^2
                  \sigma_1^2\right\} \]
              which is the m.g.f.\ of $ N(\mu_1,\sigma_1^2) $; that is,
              $ X_1 \sim N(\mu_1,\sigma_1^2) $. Also,
              $ \E{X_1}=\mu_1 $ and $ \Var{X_1}=\sigma_1^2 $.
              \[ M_{X_2}(t_2)=M(0,t_2)=\exp\left\{ t_2\mu_2+\frac{1}{2} t_2^2
                  \sigma_2^2\right\} \]
              which is the m.g.f. of $ N(\mu_2,\sigma_2^2) $; that is,
              $ X_2 \sim N(\mu_2,\sigma_2^2) $. Also,
              $ \E{X_2}=\mu_2 $ and $ \Var{X_2}=\sigma_2^2 $.
        \item Conditional distribution.
              \[ X_2\mid X_1=x_1 \sim N
                  \left( \mu_2+\frac{\rho\sigma_2(x_1-\mu_1)}{\sigma_1} ,
                  \sigma_2^2(1-\rho^2) \right) \]
              \[ X_1\mid X_2=x_2 \sim N
                  \left( \mu_1+\frac{\rho\sigma_1(x_2-\mu_2)}{\sigma_2} ,
                  \sigma_1^2(1-\rho^2) \right) \]
              \[ f_2(x_2\mid x_1)=\frac{f(x_1,x_2)}{f_1(x_1)}  \]
              \[ f_1(x_1\mid x_2)=\frac{f(x_1,x_2)}{f_2(x_2)}  \]
        \item $ \Cov{X_1,X_2}=\rho\sigma_1\sigma_2 $
        \item $ \rho=0\iff X_1\text{ and }X_2 $ are independent.
        \item Linear transformations of bivariate
              normal are still normal.
        \item $ (X-\symbf{\mu})^\top \Sigma^{-1}(X-\symbf{\mu})\sim \chi^2(2) $
    \end{enumerate}
\end{Proposition}

4. \[ \E{X_1X_2}=\E{\E{X_1X_2\given X_1}} \]
Step 1: $ \E{X_1X_2\given X_1=x_1}=\E{x_1X_2\given X_1=x_1}=
    x_1\E{X_2\given X_1=x_1}=x_1
    \left(  \mu_2+\frac{\rho\sigma_2(x_1-\mu_1)}{\sigma_1} \right) $

Step 2:
\[ \E{X_1X_2\given X_1}=X_1
    \left(  \mu_2+\frac{\rho\sigma_2(X_1-\mu_1)}{\sigma_1} \right)
\]
\begin{align*}
    \E{X_1X_2}
     & = \E*{X_1\mu_2+\frac{X_1\rho\sigma_2(X_1-\mu_1)}{\sigma_1}}                         \\
     & =\mu_2\E{X_1}+\frac{\rho\sigma_2}{\sigma_1}
    \left[ \E{X_1^2}-\mu_1\E{X_1} \right]                                                  \\
     & =\mu_2\mu_1+\frac{\rho\sigma_2}{\sigma_1} \left[ \mu_1^2+\sigma_1^2-\mu_1^2 \right] \\
     & =\mu_1\mu_2+\rho\sigma_1\sigma_2
\end{align*}
Thus,
\[ \Cov{X_1,X_2}=\E{X_1X_2}-\frac{\E{X_1}\E{X_2}}{\mu_1\mu_2}=\rho\sigma_1\sigma_2  \]
\[ \Corr{X_1,X_2}=\frac{\Cov{X_1,X_2}}{\sqrt{\Var{X_1}\Var{X_2}}}=\rho  \]

5. We know if $ X_1 $ and $ X_2 $ are independent, then $ \rho=0 $.
If $ \rho=0 $, e.g. $ X_2\given X_1=x_1 \sim N(\mu_2,\sigma_1^2) $
and $ X_1\given X_2=x_2 \sim N(\mu_1,\sigma_1^2) $
In summary: If joint bivariate normal then uncorrelated = independence.

6. Let $ \symbf{c}=\begin{pmatrix}
        c_1 \\
        c_2
    \end{pmatrix} $, then
$ \symbf{c}^\top X=c_1X_2+c_2X_2 \sim N $ with
\[ \E{\symbf{c}^\top X}=c_1\mu_1+c_2\mu_2 \]
\[ \Var{\symbf{c}^\top X}=\symbf{c}^\top \Sigma\symbf{c} \]
Furthermore, $ A\in\mathbb{R}^{2\times 2} $,
$ \symbf{b}=\begin{pmatrix}
        b_1 \\
        b_2
    \end{pmatrix} $
\[ AX+\symbf{b} \sim \Bvn{A\symbf{\mu}+\symbf{b},A\Sigma A^\top} \]
Two linear combinations of BVN is joint BVN.\

7. Note $ \chi^2(1)\coloneq Z^2 $ where $ Z \sim N(0,1) $.
\[ \chi^2(n)=
    \sum_{i=1}^{n} Z_i^2 \]
where $ Z_1,\ldots,Z_n $ are independent $ N(0,1) $.
