\makeheading{Lecture 11 | 2020-10-18}
\begin{Theorem}{Law of Total Variance}{}
    Suppose $ X $ and $ Y $ are random variables then
    \[ \Var{Y}=\E{\Var{Y\given X}}+\Var{\E{Y\given X}} \]
\end{Theorem}
\begin{Remark}{}{}
    $ \Var{Y\given X} $ is a random variable and function of $ X $.

    How to get it? Two steps:
    \begin{enumerate}
        \item $ \Var{Y\given X=x}=\E{Y^2\given X=x}-(\E{Y\given X=x})^2 $.
        \item Replace $ x $ with $ X $ to get
              the random variable $ \Var{Y\given X} $.
    \end{enumerate}
\end{Remark}
\begin{Example}{}{}
    $ Y \sim \poi{\theta} $, $ X\mid Y=y \sim \bin{y,p} $. Find
    $ \Var{X} $.

    \textbf{Solution.} We know that $ X \sim \poi{p\theta} $,
    then $ \Var{X}=p\theta $. But we can alternatively
    use the Double Expectation Theorem.
    \[ \Var{X}=\E{\Var{X\given Y}}+\Var{\E{X\given Y}} \]
    To find $ \Var{X\given Y} $,
    \[ \Var{X\given Y=y}=yp(1-p)\implies \Var{X\given Y}=Yp(1-p) \]
    To find $ \E{X\given Y} $,
    \[ \E{X\given Y=y}=yp\implies \E{X\given Y}=Yp \]
    Therefore,
    \[ \Var{X}=\E{Yp(1-p)}+\Var{pY}=p(1-p)\E{Y}+p^2\Var{Y}=
        p(1-p)\theta+p^2\theta=p\theta \]
\end{Example}
\begin{Example}{}{}
    Suppose $ X \sim \uniform{0,1} $ and
    $ Y\mid X=x \sim \bin{10,x} $. Find $ \E{Y} $
    and $ \Var{Y} $.
    \[ \E{Y}=\E{\E{Y\given X}} \]
    Two steps to find $ \E{Y\given X} $.
    \[ \E{Y\given X=x}=10x\implies \E{Y\given X}=10X \]
    \[ \E{Y}=\E{10X}=10\E{X}=10\biggl( \frac{1+0}{2} \biggr)=5 \]
    \[ \Var{Y}=\E{\Var{Y\given X}}=\Var{\E{Y\given X}} \]
    Two steps to find $ \Var{Y\given X} $.
    \[ \Var{Y\given X=x}=10x(1-p)\implies\Var{Y\given X}=10X(1-X) \]
    \begin{align*}
        \Var{Y}
         & =\E{10X(1-X)}+\Var{10X}                                                       \\
         & =10\E{X}-10\E{X^2}+100\Var{X}                                                 \\
         & =10\biggl( \frac{1+0}{2} \biggr)-
        10\bigl[ \Var{X}+(\E{X})^2 \bigr]+100\Var{X}                                     \\
         & =5-10\Biggl[ \frac{(0-1)^2}{12}+\biggl( \frac{1+0}{2} \biggr)^{\!2}  \Biggr]+
        100\biggl[ \frac{(0-1)^2}{12} \biggr]                                            \\
         & =5-10\biggl( \frac{1}{12} +\frac{1}{4}  \biggr)+
        100\biggl( \frac{1}{12} \biggr)                                                  \\
         & =5-10\biggl( \frac{1}{3} \biggr)+\frac{100}{12}                               \\
         & =10
    \end{align*}
\end{Example}
\begin{Example}{}{}
    Suppose $ Y \sim \poi{\theta} $ and $ X\mid Y=y \sim \bin{y,p} $.
    Find the m.g.f.\ of $ X $ using the Double Expectation Theorem.
        [We \emph{could} use the formula sheet to find $ M_X(t) $
            since we already know $ X \sim \poi{p\theta} $]

    \textbf{Solution.} By definition, the m.g.f.\ of $ X $ is
    \[ M_X(t)=\E*{e^{tX}}=\E*{\E*{e^{tX}\given Y}} \]
    Given $ Y=y $,
    \begin{align*}
        \E*{e^{tX}\given Y=y}
         & =\sum_{x=0}^{y} e^{tx}\binom{y}{x}p^x(1-p)^{y-x} \\
         & =\sum_{x=0}^{y}\binom{y}{x}(pe^t)^x(1-p)^{y-x}   \\
         & =(1-p+pe^t)^y
    \end{align*}
    Therefore, $ \E*{e^{tX}\given Y}=(1-p+pe^t)^Y $. Therefore,
    \begin{align*}
        M_X(t)
         & =\E*{(1-p+pe^t)^Y}                                                          \\
         & =\sum_{y=0}^{\infty} (1-p+pe^t)^y \frac{\theta^y e^{-\theta}}{y!}           \\
         & =e^{-\theta}\sum_{y=0}^{\infty} \frac{\bigl[ \theta(1-p+pe^t) \bigr]^y}{y!} \\
         & =e^{-\theta}\expon{\theta(1-p+pe^t)}                                        \\
         & =\expon{\theta p(e^t-1)}
    \end{align*}
    Actually, this is the m.g.f.\ of $ \poi{\theta p} $.
\end{Example}

\section{Joint Moment Generating Functions}
\begin{Definition}{Joint moment generating function}{}
    If $ X $ and $ Y $ are random variables, then
    \[ M(t_1,t_2)=\E[\big]{e^{t_1X+t_2Y}} \]
    is called the \textbf{joint moment generating function}
    of $ X $ and $ Y $ if $ M(t_1,t_2) $ exists for
    $ \abs{t_1}<h_1 $ and $ \abs{t_2}<h_2 $
    for some $ h_1,h_2>0 $.
\end{Definition}
\begin{Remark}{}{}
    In general, suppose $ X_1,\ldots,X_n $ are random variables,
    then
    \[ M(t_1,\ldots,t_n)=\E[\bigg]{\expon[\bigg]{\,\sum_{i=1}^{n} t_i X_i}} \]
    is the \textbf{joint moment generating function}
    if it exists for $ \abs{t_i}<h_i $
    for some $ h_i>0 $ where $ i=1,\ldots,n $.
\end{Remark}
\begin{Remark}{Applications of Joint Moment Generating Functions}{}
    \begin{enumerate}[label=(\arabic*)]
        \item From joint m.g.f.\ to marginal m.g.f.\
              Given $ M(t_1,t_2) $ for $ \abs{t_1}<h_1 $
              and $ \abs{t_2}<h_2 $
              with $ h_1,h_2>0 $,
              \[ M_X(t_1)=M(t_1,t_2=0)=\E{e^{t_1X}} \]
              \[ M_Y(t_2)=M(0,t_2)=\E{e^{t_2Y}} \]
        \item Independence Property. $ X $ and
              $ Y $ are independent if and only if
              \[ M(t_1,t_2)=M_X(t_1)M_Y(t_2) \]
              More generally, if $ X_1,\ldots,X_n $ are
              independent, then
              \[ M(t_1,\ldots,t_n)=\prod_{i=1}^n M_{X_i}(t_i) \]
    \end{enumerate}
\end{Remark}
\begin{Example}{}{}
    Suppose $ f(x,y)=e^{-y} $ for $ 0<x<y $ is the joint p.d.f.\
    of $ (X,Y) $. Find the joint m.g.f.\ of $ X $ and $ Y $.
    Are they independent? Find the marginal p.d.f.\ of $ X $ and $ Y $.

    \textbf{Solution.}
    \begin{align*}
        M(t_1,t_2)
         & =\E{e^{t_1X+t_2Y}}                                                   \\
         & =\int_{0}^{\infty}
        \biggl[\int_{0}^{y} e^{t_1 x+ t_2 y}e^{-y}\, d{x} \biggr]\, d{y}        \\
         & =\int_{0}^{\infty} e^{(t_2-1)y}
        \biggl[ \frac{1}{t_1} e^{t_1 x} \biggr]_0^y \, d{y}                     \\
         & =\frac{1}{t_1} \int_{0}^{\infty} e^{(t_2-1)y}(e^{t_1 y}-1)\, d{y}    \\
         & =\frac{1}{t_1} \int_{0}^{\infty}
        e^{(t_1+t_2-1)y}-e^{(t_2-1)y}\, d{y}                                    \\
         & =\frac{1}{t_1} \biggl( \frac{1}{1-t_1-t_2} -\frac{1}{1-t_2}  \biggr) \\
         & =\frac{1}{(1-t_1-t_2)(1-t_2)}
    \end{align*}
    with $ t_2-1<0 $ and $ t_1+t_2-1<0 $. Therefore,
    $ t_2<1 $ and $ t_1+t_2<1 $.
    \[ M_X(t_1)=M(t_1,t_2=0)=\frac{1}{1-t_1} \]
    which is the m.g.f.\ of $ \exponential{1} $.
    \[ M_Y(t_2)=M(t_1=0,t_2)=\frac{1}{(1-t_2)^2} \]
    which is the m.g.f.\ of $ \gam{\alpha=2,\beta=1} $.
    Note that the joint support is a triangle (not a rectangle),
    so obviously $ M(t_1,t_2)\neq M_X(t_1)M_Y(t_2) $. Thus,
    $ X $ and $ Y $ are not independent.
\end{Example}
\begin{Example}{Additivity of Poisson Random Variables}{}
    Suppose $ X \sim \poi{\theta_1} $ and $ Y \sim \poi{\theta_2} $
    with $ X $ and $ Y $ independent. Prove
    that $ X+Y \sim \poi{\theta_1+\theta_2} $.

    \textbf{Solution.} We can try to find the p.d.f.\ of $ X+Y $
    (direct method). Alternatively, find $ M_{X+Y}(t) $.
    \begin{align*}
        M_{X+Y}(t)
         & =\E{e^{tX+tY}}                                                                           \\
         & =\E{e^{tX}e^{tY}}                               & \quad & \text{$X$ and $Y$ independent} \\
         & =\E{e^{tX}}\E{e^{tY}}                                                                    \\
         & =\expon{\theta_1(e^t-1)}\expon{\theta_2(e^t-1)}                                          \\
         & =\expon{(\theta_1+\theta_2)(e^t-1)}
    \end{align*}
    which is the m.g.f.\ of $ \poi{\theta_1+\theta_2} $.
\end{Example}
