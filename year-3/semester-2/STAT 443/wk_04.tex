\chapter{ARIMA Models Continued}
\section{Stationary Process Forecasting}
Suppose we observe a time series
$ X_1,\ldots,X_T $
that we believe has been generated by an underlying
stationary process. We would like to
produce an $ h $-step ahead
forecast
\[ \hat{X}_{T+h}=\hat{X}_{T+h\mid T}=f(X_t,\ldots,X_1) \]
forecasting $ X_{T+h} $. Ideally, $ \hat{X}_{T+h} $
would minimize the prediction error
\[ L(X_{T+h},\hat{X}_{T+h})=\min_f
    L(X_{T+h},f(X_{T},\ldots,X_1)) \]
where $ L $ is a loss function.

Frequently, the loss function is taken
to be the \emph{mean-squared error} (MSE)
\[ L(X_{T+h},\hat{X}_{T+h})=
    \E[\big]{(X_{T+h}-\hat{X}_{T+h})^2} \]
When using MSE, it is natural to consider
\[ L^2=\set{\text{Random variables } X: \E{X^2}<\infty} \]
$ L^2 $ is a Hilbert space when equipped
with the inner product
\[ \innerp{X}{Y}=\E{XY} \]
Hilbert spaces are generalizations of Euclidean space ($ \mathbf{R}^d $)
in which the geometry and notation of projection
are preserved.
\[ \text{Proj}(X\to Y)=\innerp{X}{Y}Y \]
\begin{Theorem}{Projection Theoren}{}
    We say $ M\subseteq L^2 $
    is a \textbf{closed linear subspace}, if
    \begin{itemize}
        \item Linearity: $ X,Y\in M $, $ \alpha,\beta\in\mathbf{R} $
              then $ \alpha X+\beta Y\in M $
        \item Closed: If $ X_n\to X $ ($ \E{(X_n-X)^2} $),
              and $ X_n\in M $, then $ X\in M $.
    \end{itemize}
    If $ M $ is a closed linear subspace in $ L^2 $
    and $ x\in L^2 $, then there exists a
    unique $ \hat{X}\in M $ such that
    \[ \E{(X-\hat{X})^2}=\inf_{y\in M}\E{(X-Y)^2} \]
    Moreover, $ \hat{X} $ satisfies the prediction equations/normal
    equations:
    \[ (X-\hat{X})\in M^\perp \implies \E{(X-\hat{X})Y}=0\quad (\forall y\in M) \]
\end{Theorem}
In MSE forecasting, we want to choose
$ \hat{X}_{T+h} $ satisfying
\[ \E{(X_{T+h}-\hat{X}_{T+h})^2}=\inf_{y\in M}\E{(X_{T+h}-y)^2} \]
where $ M $ is a closed linear subspace based on the available
data.
\begin{enumerate}[(1)]
    \item $ M=M_1=\set{z:z=f(X_{T},\ldots,X_{1}), f\text{ is any
                  Borel Measurable function}} $
          In this case
          \[ \hat{X}_{T+h}=\E{X_{T+h}\mid X_{T},\ldots,X_1} \]
          Unfortunately $ M_1 $ is enormous and complicated!
    \item $ M=M_2=\Span{1,X_{T},\ldots,X_1}=
              \set{Y:Y=\alpha_0+\sum_{j=1}^{T} \alpha_j X_j,\,\alpha_0,\ldots,\alpha_T\in\mathbf{R}} $
          which is the linear functions of $ X_1,\ldots,X_T $
          $ \hat{X}_{T+h} $ is called the \textbf{best linear predictor} (BLP).
\end{enumerate}
