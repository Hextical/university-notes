\makeheading{Week 5}
\section*{Primer on Logistic Regression}
\begin{itemize}
      \item Linear regression is an effective method of modelling the relationship between a single response variable
            $ (Y) $, and one or more explanatory variables $ (x_1,x_2,\ldots,x_p) $.
            \begin{itemize}
                  \item However, ordinary linear regression assumes that the response variable follows a normal distribution
                        (i.e., $ Y \sim \N{\mu,\sigma^2} $).
                  \item When the response variable is binary, this assumption is no longer valid.
            \end{itemize}
      \item When we have a binary response, the Bernoulli distribution (i.e., $ Y \sim \Binomial{1,\pi} $) is a much more
            appropriate distributional assumption.
            \begin{itemize}
                  \item But ordinary linear regression is no longer appropriate.
                  \item Instead, we use \textbf{Logistic Regression}.
            \end{itemize}
      \item In the context of a linear regression model, the expected response (given
            the values of the explanatory variables) is equated to the \textbf{linear predictor} $ \beta_0+\beta_1x_1+\cdots+\beta_p x_p $:
            \[ \E{Y\given x_1,x_2,\ldots,x_p}=\mu=\beta_0+\beta_1x_1+\cdots+\beta_p x_p \]
      \item In the context of Logistic Regression we also want to relate the expected response to the linear predictor.
            \begin{itemize}
                  \item But now, $ \E{Y}=\pi\in[0,1] $.
                  \item And equating $ \pi $ and $ \beta_0+\beta_1 x_1+\cdots+\beta_p x_p $ does not make sense. In general, the linear predictor need not lie in $ [0,1] $.
                        \[ \pi=\beta_0+\beta_1x_1+\cdots+\beta_p x_p\quad\text{not a good thing to do} \]
            \end{itemize}
      \item Instead, we relate the linear predictor to $ \E{Y}=\pi $ through a monotonic differentiable \textbf{link function} that maps $ [0,1]\to\mathbf{R} $.
            \begin{itemize}
                  \item Logistic Regression arises when this link function is the \textbf{logit} function:
                        \[ \text{logit}(\pi)=\log*{\frac{\pi}{1-\pi}}=\beta_0+\beta_1x_1+\cdots+\beta_p x_p \]
                  \item Inverting this yields the expected response (given the values of the explanatory variables):
                        \[ \widehat{\E{Y\given x_1,x_2,\ldots,x_p}}=\hat{\pi}=\frac{e^{\hat{\beta}_0+\hat{\beta}_1 x_1+\cdots+\hat{\beta}_p x_p}}{1+e^{\hat{\beta}_0+\hat{\beta}_1 x_1+\cdots+\hat{\beta}_p x_p}}
                              =\text{expit}(\beta_0+\beta_1x_1+\cdots+\beta_p x_p)  \]
            \end{itemize}
      \item To interpret $ \beta_0 $, we set each explanatory variable to zero (i.e., $ x_1=x_2=\cdots=x_p=0 $).
            \begin{itemize}
                  \item We see that $ \beta_0 $ is the \textbf{log-odds} that $ Y=1 $ when $ x_1=x_2=\cdots=x_p=0 $.
                        \[ \log*{\frac{\pi}{1-\pi}}=\beta_0 \]
                  \item Equivalently, $ e^{\beta_0} $ is the \textbf{odds} that the response would equal $ 1 $ when $ x_1=x_2=\cdots=x_p=0 $.
                        Exponentiating both sides yields
                        \[ \frac{\pi}{1-\pi}=e^{\beta_0}  \]
            \end{itemize}
            \begin{Definition}{Odds}{}
                  The \textbf{odds} of an event $ A $ is:
                  \[ \frac{\Prob{A}}{\Prob{A^c}}=\frac{\Prob{A}}{1-\Prob{A}} \]
            \end{Definition}
      \item The interpretation of $\beta_j$, for $ j=1,2,\ldots,p $, is uncovered by considering the Logistic Regression equation
            for different values of $ x_j $.
            \begin{itemize}
                  \item Let $ \pi_x $ be the value of $ \pi $ when $ x_j=x $ and let $ \pi_{x+1} $ be the value of $ \pi $ when $ x_j=x+1 $.
                        \begin{align*}
                              \log*{\frac{\pi_{x+1}}{1-\pi_{x+1}}}-\log*{\frac{\pi_x}{1-\pi_x}}
                               & =(\beta_0+\beta_1x_1+\cdots+\beta_j(x+1)+\cdots+\beta_p x_p)        \\
                               & \quad\quad-(\beta_0+\beta_1x_1+\cdots+\beta_j x+\cdots+\beta_p x_p) \\
                               & =\beta_j
                        \end{align*}
                  \item Thus:
                        \[ \log*{\frac{\pi_{x+1}}{1-\pi_{x+1}}\Bigg/\frac{\pi_x}{1-\pi_x}}=\beta_j \]
                        and so $ \beta_j $ is interpreted as a \textbf{log-odds ratio}, comparing the odds that $ Y=1 $
                        when $ x_j=x+1 $ versus $ x_j=x $ (all else being equal).
                  \item Equivalently, $ e^{\beta_j} $ is interpreted as the \textbf{odds ratio}, comparing the odds that
                        $ Y=1 $ when $ x_j=x+1 $ versus $ x_j=x $ (all else being equal). Exponentiating yields
                        \[ \frac{\pi_{x+1}}{1-\pi_{x+1}} \Bigg/ \frac{\pi_x}{1-\pi_x} =e^{\beta_j} \]
            \end{itemize}
      \item \textbf{Maximum likelihood estimation} is a method that is used to estimate parameters in Logistic Regression.
            \begin{itemize}
                  \item This means that the $ \hat{\beta} $'s are maximum likelihood estimates, whose corresponding estimators have nice properties, such as:
                        \[ \tilde{\beta}\stackrel{\cdot}{\sim}\N*{\beta,\frac{1}{J(\beta)}} \]
                        where $ J(\beta) $ is the Fisher Information.
                  \item A consequence of this is that hypotheses of the form
                        \begin{tightcenter}
                              $ \mathbf{H}_0 $: $ \beta_j =0 $ versus $ \mathbf{H}_\text{A} $: $ \beta_j\ne 0 $
                        \end{tightcenter}
                        are done with $ Z $-\emph{tests} with test statistics given by
                        \[ t=\frac{\hat{\beta}_j-0}{\Se*{\hat{\beta}_j}}\stackrel{\cdot}{\sim}\N{0,1}  \]
                  \item In order to test hypotheses about several $ \beta $'s being simultaneously equal to zero, we use \emph{likelihood ratio tests}.
            \end{itemize}
\end{itemize}
\chapter{BLOCKING}
\begin{itemize}
      \item In the context of designed experiments we categorize factors as either:
            \begin{itemize}
                  \item \emph{Design} factors: we manipulate these to quantify their impact on the response.
                        They define the experimental conditions.
                  \item \emph{Allowed-to-vary} factors: these are unknown, or known but uncontrollable
                        factors that are \underline{not} controlled in the experiment.
                  \item \emph{Nuisance} factors: we control these to eliminate their effect on the response.
            \end{itemize}
      \item But remember: in practice, context dictates whether a factor should be considered a design factor, a
            nuisance factor, or if it should be allowed to vary.
\end{itemize}
\begin{figure}[!htbp]
      \centering
      \includegraphics[width=0.8\textwidth]{browsers.pdf}
      \caption{Four Levels of the \emph{browser} Factor.}
\end{figure}
\begin{enumerate}
      \item Usability testing involves studying the ease with which an individual uses a product or service
            for some intended purpose. Suppose investigators are performing a usability test to determine
            with which browser 70 to 80-year-old users find it easiest to look up the phone number of
            the nearest pharmacy. In this example, experimental units (70 to 80-year-olds) are randomly
            assigned to one of four browser conditions, and the investigators measure the time it
            takes to complete the task.
            \begin{itemize}
                  \item Browser is the design factor.
            \end{itemize}
      \item Suppose that Netflix is experimenting with server-side modifications to improve (reduce) the
            latency of Netflix.com. We hypothesize that the current infrastructure serves as a control condition
            and the modified infrastructure reduces median page load time. It is possible
            that a user's browser may also affect page load time, but this effect is not of interest to
            the investigators. To control for the potential impact of one's browser, Netflix initially
            experiments with only Firefox users.
            \begin{itemize}
                  \item Browser is the nuisance factor.
            \end{itemize}
      \item Suppose that Amazon.ca is experimenting with the width of their search bar. They hypothesize
            that a wider search bar will minimize the amount of mouse movement required to
            navigate to it, thereby minimizing the average time-to-query. The experimenters do not care
            which browser a customer uses and so this factor is uncontrolled and hence is \emph{allowed-to-vary}
            during their experiment.
            \begin{itemize}
                  \item Browser is the allowed-to-vary factor.
            \end{itemize}
\end{enumerate}
\begin{itemize}
      \item It's also important to understand the subtle distinction between nuisance factors and design factors in
            the context of a single experiment.
            \begin{itemize}
                  \item We control both factors in the experiment.
                  \item With a design factor we wish to quantify its influence on the response variable.
                  \item With a nuisance factor we do not care to quantify its effect, we wish only to \emph{eliminate} it.
            \end{itemize}
      \item We eliminate the effect of one or more nuisance factors with \textbf{blocking}.
            \begin{itemize}
                  \item To eliminate the effect of a nuisance factor, it cannot be allowed to vary on its own.
                  \item Blocking fixes the nuisance factor at one or more levels (\textbf{blocks}).
                  \item By holding a nuisance factor fixed, it cannot vary and hence cannot influence the response.
                        \begin{itemize}
                              \item This is how Netflix handled the nuisance factor ``browser'' in Example 2.
                        \end{itemize}
            \end{itemize}
\end{itemize}
\section{Randomized Complete Block Designs}
\begin{itemize}
      \item The randomized complete block design (RCBD) is a simple experimental design that may be applied
            when we wish to investigate:
            \begin{itemize}
                  \item A single design factor; e.g., $ m $ levels, $ m $ conditions, while controlling for a single nuisance factor; e.g., $ b $ levels, $ b $ blocks.
            \end{itemize}
      \item In a RCBD, we carry out each of the experimental conditions in every one of the blocks.
            \begin{itemize}
                  \item $ m $ conditions are happening inside each of the $ b $ blocks.
            \end{itemize}
      \item The \emph{observed} data in such an experiment is $ y_{ijk} $.
            \begin{itemize}
                  \item Response observation for unit $ i=1,2,\ldots,n_{jk} $ in condition $ j=1,2,\ldots,m $
                        within block $ k=1,2,\ldots,b $.
            \end{itemize}
      \item We assume that there are $n_{jk}$ units in $ (\text{condition}, \text{block}) = (j, k) $ and thus an overall total of
            $ N=\sum_{k=1}^{b} \sum_{j=1}^{m} n_{jk} $ units.
            \begin{itemize}
                  \item If $ n_{jk}=n $ for all $ (j,k) $, we call the design ``balanced.''
            \end{itemize}
      \item We tabulate the response data of this form below:
            \begin{table}[!htbp]
                  \centering
                  \caption{Response Observations in a Randomized Complete Block Design}
                  \begin{NiceTabular}{cc|cccc|c}
                        \multicolumn{2}{c}{}       & \multicolumn{4}{c}{\emph{Block}} &                                                                                                                                                                                                                           \\
                        \multicolumn{2}{c}{}       & $1$                                & $2$                                              & $\cdots$                                         & $b$      & \multicolumn{1}{c}{}                                                                                     \\
                        \cmidrule{2-7}
                        \multirow{4}{*}{\emph{Condition}} & $1$                                & $\Set{y_{i11}}_{i=1}^{n_{11}}$                   & $\Set{y_{i12}}_{i=1}^{n_{12}}$                   & $\cdots$ & $\Set{y_{i1b}}_{i=1}^{n_{1b}}$                   & $\bar{y}_{\bullet 1\bullet}$                          \\
                        & $2$                                & $\Set{y_{i21}}_{i=1}^{n_{21}}$                   & $\Set{y_{i22}}_{i=1}^{n_{22}}$                   & $\cdots$ & $\Set{y_{i2b}}_{i=1}^{n_{2b}}$                   & $\bar{y}_{\bullet 2\bullet}$                          \\
                        & $\vdots$                           & $\vdots$                                         & $\vdots$                                         & $\ddots$ & $\vdots$                                         & $\vdots$                                              \\
                        & $m$                                & $\Set{y_{im1}}_{i=1}^{n_{m1}}$                   & $\Set{y_{im2}}_{i=1}^{n_{m2}}$                   & $\cdots$ & $\Set{y_{imb}}_{i=1}^{n_{mb}}$                   & $\bar{y}_{\bullet m\bullet}$                          \\
                        \cmidrule{2-7}
                        \multicolumn{1}{c}{}       & \multicolumn{1}{c}{}               & \multicolumn{1}{c}{$\bar{y}_{\bullet\bullet 1}$} & \multicolumn{1}{c}{$\bar{y}_{\bullet\bullet 2}$} & $\cdots$ & \multicolumn{1}{c}{$\bar{y}_{\bullet\bullet b}$} & \multicolumn{1}{c}{$\bar{y}_{\bullet\bullet\bullet}$}
                  \end{NiceTabular}
            \end{table}
            \begin{itemize}
                  \item Block-specific average responses: $\bar{y}_{\bullet\bullet 1},\bar{y}_{\bullet\bullet 2},\ldots,\bar{y}_{\bullet\bullet b}$.
                  \item Overall average response: $\bar{y}_{\bullet\bullet\bullet}$.
                  \item Condition-specific average responses: $\bar{y}_{\bullet 1\bullet},\bar{y}_{\bullet 2\bullet},\ldots,\bar{y}_{\bullet m\bullet}$.
            \end{itemize}
      \item We calculate the row, column, and overall means as follows:
            \[ \bar{y}_{\bullet j\bullet}=\frac{1}{n_{j+}} \sum_{k=1}^{b} n_{jk}\bar{y}_{\bullet jk}\quad\text{where }n_{j+}=\sum_{k=1}^{b} n_{jk} \]
            \[ \bar{y}_{\bullet\bullet k}=\frac{1}{n_{+k}} \sum_{j=1}^{m} n_{jk}\bar{y}_{\bullet jk}\quad\text{where }n_{+k}=\sum_{j=1}^{m} n_{jk} \]
            \[ \bar{y}_{\bullet\bullet\bullet}=\frac{1}{N} \sum_{k=1}^{b} \sum_{j=1}^{m} n_{jk}\bar{y}_{\bullet jk}=\frac{1}{N} \sum_{k=1}^{b} \sum_{j=1}^{m} \sum_{i=1}^{n_{jk}} y_{ijk} \]
            where $ \bar{y}_{\bullet jk} $ is the average response value in $  (\text{condition}, \text{block}) = (j, k) $ cell, also
            \[ \bar{y}_{\bullet jk}=\frac{1}{n_{jk}} \sum_{i=1}^{n_{jk}} y_{ijk} \]
      \item Simple summaries such as these provide a crude assessment of whether the condition-to-condition and
            block-to-block variation is large.
            \begin{itemize}
                  \item If the condition-specific averages are very different, this suggests that the design factor influences the response.
                  \item If the block-specific averages are very different, this suggests that the nuisance factor influences the response, and that blocking was appropriate.
            \end{itemize}
      \item The primary analysis goal in a RCBD is to determine whether the expected response differs significantly
            from one condition to another.
            \begin{itemize}
                  \item And if so, to identify the optimal condition, while controlling for the potential effect of the nuisance factor.
            \end{itemize}
      \item We've previously done this with gatekeeper tests of the form:
            \begin{tightcenter}
                  $ \mathbf{H}_0 $: $ \theta_1=\theta_2=\cdots=\theta_m $ versus $ \mathbf{H}_\text{A} $: $ \theta_j\ne \theta_k $ for some $ j\ne k $
            \end{tightcenter}
      \item We do the same thing here, while accounting for the nuisance factor, with \emph{appropriately defined} linear (continuous response)
            or logistic (binary response) regression models which contain:
            \begin{itemize}
                  \item An intercept.
                  \item $ m-1 $ indicator variables for the design factor's levels.
                  \item $ b-1 $ indicator variables for the nuisance factor's levels.
            \end{itemize}
      \item We write the linear predictor as:
            \begin{equation}\tag{$\star$}
                  \alpha+\sum_{j=1}^{m-1} \beta_j x_{ij}+\sum_{k=1}^{b-1} \gamma_k z_{ij}\label{lpeqn}
            \end{equation}
            \begin{itemize}
                  \item $ x_{ij}=1 $ if unit $ i $ is in condition $ j=1,2,\ldots,m-1 $, and zero otherwise.
                  \item $ z_{ik}=1 $ if unit $ i $ is in block $ k=1,2,\ldots,b-1 $, and zero otherwise.
                  \item The $ \beta $'s jointly quantify the effect of the design factor.
                  \item The $ \gamma $'s jointly quantify the effect of the nuisance factor.
            \end{itemize}
      \item Two relevant hypotheses are:
            \begin{enumerate}[(1)]
                  \item $ \mathbf{H}_0 $: $ \beta_1=\beta_2=\cdots=\beta_{m-1}=0 $ versus $ \mathbf{H}_\text{A} $: $ \beta_j\ne 0 $ for some $ j $.
                        \begin{itemize}
                              \item If we don't reject $ \mathbf{H}_0 $, this suggests the $ x $'s don't need to be in the model
                                    and hence the design factor doesn't significantly influence the response.
                        \end{itemize}
                  \item $ \mathbf{H}_0 $: $ \gamma_1=\gamma_2=\cdots=\gamma_{b-1}=0 $ versus $ \mathbf{H}_\text{A} $: $ \gamma_k\ne 0 $ for some $ k $.
                        \begin{itemize}
                              \item If we don't reject $ \mathbf{H}_0 $, this suggests the $ z $'s don't need to be in the model
                                    and hence the nuisance factor doesn't significantly influence the response. Therefore, blocking isn't necessary.
                        \end{itemize}
            \end{enumerate}
      \item We test these hypotheses by comparing a \emph{full} model and \emph{reduced} models where the \emph{full} model
            is a model with a linear predictor given by~(\ref{lpeqn}), and a \emph{reduced} model is a model with a linear predictor that arises when $ \mathbf{H}_0 $
            is true.
            \begin{itemize}
                  \item We try to determine whether the full model fits the data significantly better than the reduced one.
            \end{itemize}
\end{itemize}
\subsection{RCBD to Compare Means}
\begin{itemize}
      \item Here, we're interested in testing the following hypothesis (while accounting for the influence of the
            nuisance factor):
            \begin{tightcenter}
                  $ \mathbf{H}_0 $: $ \mu_1=\mu_2=\cdots=\mu_m $ versus $ \mathbf{H}_\text{A} $: $ \mu_j\ne \mu_k $ for some $ j\ne k $
            \end{tightcenter}
            where $ \mu_j $ is the expected response in condition $ j=1,2,\ldots,m $.
      \item We do this by testing:
            \begin{tightcenter}
                  $ \mathbf{H}_0 $: $ \beta_1=\beta_2=\cdots=\beta_{m-1}=0 $ versus $ \mathbf{H}_\text{A} $: $ \beta_j\ne 0 $ for some $ j $
            \end{tightcenter}
            with an ANOVA in the context of the following linear regression model.
            \[ Y_i=\alpha+\sum_{j=1}^{m-1} \beta_j x_{ij}+\sum_{k=1}^{b-1} \gamma_k z_{ik}+\varepsilon_i \]
            where $ Y_i $ is the response observation for unit $ i=1,2,\ldots,N=\sum_{k=1}^{b} \sum_{j=1}^{m} n_{jk} $
            and $ \varepsilon_i \stackrel{\text{iid}}{\sim}\N{0,\sigma^2} $ is a random error term.
      \item The relevant ANOVA table is~\Cref{two_way_ANOVA_RCBD}.
            \begin{table}[!htbp]
                  \centering
                  \caption{Two-Way ANOVA Table Associated With a Randomized Complete Block Design}\label{two_way_ANOVA_RCBD}
                  \begin{NiceTabular}{|l|c|c|c|c|}
                        \toprule
                        Source    & SS                     & d.f.        & MS                                                                   & Test Statistic                                           \\
                        \midrule
                        Condition & $ \SSC $ & $ m-1 $     & $ \MSC=\SSC/(m-1) $     & $ t_\text{C}=\MSC/\MSE $ \\
                        Block     & $ \SSB $ & $ b-1 $     & $ \MSB=\SSB/(b-1) $     & $ t_\text{B}=\MSB/\MSE $ \\
                        Error     & $ \SSE $ & $ N-m-b+1 $ & $ \MSE=\SSE/(N-m-b+1) $ &                                                      \\
                        \midrule
                        Total     & $ \SST $ & $ N-1 $\\
                        \bottomrule
                  \end{NiceTabular}
            \end{table}
      \item The sums of squares given in~\Cref{two_way_ANOVA_RCBD} are:
            \begin{itemize}
                  \item Total sum of squares (quantifies overall response variation):
                        \[ \SST=\sum_{k=1}^{b} \sum_{j=1}^{m} \sum_{i=1}^{n_{jk}} (y_{ijk}-\bar{y}_{\bullet\bullet\bullet})^2=\SSC+\SSB+\SSE \]
                  \item Condition sum of squares (quantifies condition-to-condition response variation):
                        \[ \SSC=\sum_{k=1}^{b} \sum_{j=1}^{m} \sum_{i=1}^{n_{jk}} (\bar{y}_{\bullet j\bullet}-\bar{y}_{\bullet\bullet\bullet})^2 \]
                  \item Block sum of squares (quantifies block-to-block response variation):
                        \[ \SSB=\sum_{k=1}^{b} \sum_{j=1}^{m} \sum_{i=1}^{n_{jk}}(\bar{y}_{\bullet\bullet k}-\bar{y}_{\bullet\bullet\bullet})^2   \]
                  \item Error sum of squares (quantifies residual response variation not accounted for by conditions of blocks):
                        \[ \SSE=\sum_{k=1}^{b} \sum_{j=1}^{m} \sum_{i=1}^{n_{jk}}(y_{ijk}-\bar{y}_{\bullet j\bullet}-\bar{y}_{\bullet\bullet k}+\bar{y}_{\bullet\bullet\bullet})^2   \]
            \end{itemize}
      \item So how do we use this table?
            \begin{itemize}
                  \item We test: $ \mathbf{H}_0 $: $ \beta_1=\beta_2=\cdots=\beta_{m-1}=0 $ using $ t_\text{C}=\MSC/\MSE $.
                        \begin{itemize}
                              \item $ p\text{-value}=\Prob{T\ge t_\text{C}} $ where $ T \sim F(m-1,N-m-b+1) $.
                        \end{itemize}
                  \item We test: $ \mathbf{H}_0 $: $ \gamma_1=\gamma_2=\cdots=\gamma_{b-1}=0 $ using $ t_\text{B}=\MSB/\MSE $.
                        \begin{itemize}
                              \item $ p\text{-value}=\Prob{T\ge t_\text{B}} $ where $ T \sim F(b-1,N-m-b+1) $.
                        \end{itemize}
            \end{itemize}
\end{itemize}
\subsection{Example: Promotions at The Gap}
\begin{Example}{Promotions at The Gap}{}
      The Gap has three versions of an online weekday promotion that a customer sees when they go to \href{https://www.gapcanada.ca/}{gapcanada.ca}:
      \begin{itemize}
            \item Version 1: $ 50\% $ discount on one item.
            \item Version 2: $ 20\% $ discount on your entire order.
            \item Version 3: Spend $ \$ 50 $ and get a $ \$ 10 $ gift card.
      \end{itemize}
      Interest lies in determining whether there is a difference in the average purchase total (i.e, the average
      dollar value of a customer's purchase) between promotion versions. However, the amount of money one
      spends may also be influenced by the nuisance factor, day of week. As such, we ran a randomized complete block
      design with $m = 3$ experimental conditions (corresponding to the three promotions) and $b = 5$
      blocks (corresponding to the day of the week). Here $n_{jk}=50$ for all $(j, k)$, and so the design was ``balanced.'' For
      each visitor to \href{https://www.gapcanada.ca/}{gapcanada.ca}, their purchase total (in dollars) was recorded.
      The regression model fit to these response observations is:
      \[ Y_i=\alpha+\beta_2 x_{i2}+\beta_3 x_{i3}+\gamma_1 z_{i1}+\gamma_2 z_{i2}+\gamma_3 z_{i3}+\gamma_4 z_{i4}+\varepsilon_i \]
      where $x_{i2}$ and $x_{i3}$ are condition indicators for promotions 2 and 3 (promotion 1 is the baseline) and $ z_{i1},\ldots,z_{i4} $
      are block indicators for Monday-Thursday (Friday is the baseline). The ANOVA Table for this experiment is~\Cref{GAPANOVA}.
      \begin{center}
            \captionsetup{type=table}
            \captionof{table}{The Gap RCBD ANOVA Table}\label{GAPANOVA}
            \begin{NiceTabular}{|l|c|c|c|c|}
                  \toprule
                  Source    & SS           & d.f.    & MS           & Test Statistic             \\
                  \midrule
                  Condition & $ 49618.34 $ & $ 2 $   & $ 24809.17 $ & $ t_\text{C}=2165.39 $ \\
                  Block     & $ 19258.30 $ & $ 4 $   & $ 4814.58 $  & $ t_\text{B}=420.22 $  \\
                  Error     & $ 8512.67 $  & $ 743 $ & $ 11.46 $    &                        \\
                  \midrule
                  Total     & $ 77389.32 $ & $ 749 $\\
                  \bottomrule
            \end{NiceTabular}
      \end{center}
      \begin{itemize}
            \item $ \mathbf{H}_0 $: $ \beta_2=\beta_3=0 $ tells us whether the design factor is significant.
                  \begin{itemize}
                        \item $ p\text{-value}=\Prob{T\ge t_\text{C}}=\Prob{T\ge 2165.39}=1.101\times 10^{-310} $ where $ T \sim F(2,743) $.
                        \item Therefore, we reject $ \mathbf{H}_0 $ and conclude that the expected response is not the same in all conditions.
                  \end{itemize}
            \item $ \mathbf{H}_0 $: $ \gamma_1=\gamma_2=\gamma_3=\gamma_4=0 $ tells us whether the nuisance factor is significant.
                  \begin{itemize}
                        \item $ p\text{-value}=\Prob{T\ge t_\text{B}}=\Prob{T\ge 420.22}=4.345\times 10^{-189} $ where $ T \sim F(4,743) $.
                        \item Therefore, we reject $ \mathbf{H}_0 $ and conclude that blocking was appropriate.
                  \end{itemize}
      \end{itemize}
      \href{https://github.com/Hextical/university-notes/blob/master/year-3/semester-3/STAT 430/code/W5/Comparing_means_within_blocks.R}{[R Code] \texttt{Comparing\_means\_within\_blocks}}
\end{Example}
\subsection{RCBD to Compare Proportions}
\begin{itemize}
      \item Here we're interested in testing the following hypothesis (while accounting for the influence of the
            nuisance factor):
            \begin{tightcenter}
                  $ \mathbf{H}_0 $: $ \pi_1=\pi_2=\cdots=\pi_m $ versus $ \mathbf{H}_\text{A} $: $ \pi_j\ne \pi_k $ for some $ j\ne k $
            \end{tightcenter}
            where $ \pi_j $ is the expected response in condition $ j=1,2,\ldots,m $.
      \item We do this by testing:
            \begin{tightcenter}
                  $ \mathbf{H}_0 $: $ \beta_1=\beta_2=\cdots=\beta_{m-1}=0 $ versus $ \mathbf{H}_\text{A} $: $ \beta_j\ne 0 $ for some $ j$
            \end{tightcenter}
            with a likelihood ratio test (LRT) in the context of the following logistic regression model:
            \[ \log*{\frac{\pi_i}{1-\pi_i}}=\alpha+\sum_{j=1}^{m-1} \beta_j x_{ij}+\sum_{k=1}^{b-1} \gamma_k z_{ik} \]
            where $ Y_i $ is the response observation for unit $ i=1,2,\ldots,N=\sum_{k=1}^{b} \sum_{j=1}^{m} n_{jk} $,
            and $ \pi_i=\E{Y_i}=\Prob{Y_i=1} $.
            \begin{itemize}
                  \item The likelihood ratio test compares the full model to the one without the $ x $'s.
            \end{itemize}
      \item Similarly, we test:
            \begin{tightcenter}
                  $ \mathbf{H}_0 $: $ \gamma_1=\gamma_2=\cdots=\gamma_{b-1} $ versus $ \mathbf{H}_\text{A} $: $ \gamma_k\ne 0 $ for some $ k $
            \end{tightcenter}
            with a LRT that compares the full model to the reduced one without the $ z $'s.
            \begin{tightcenter}
                  $ \mathbf{H}_0 $: Reduced model fits as well versus $ \mathbf{H}_\text{A} $: Full model fits better than reduced model.
            \end{tightcenter}
      \item The observed test statistic for both of these tests is:
            \begin{align*}
                  t & =2\log*{\frac{\text{Likelihood}_{\text{Full Model}}}{\text{Likelihood}_{\text{Reduced Model}}}}      \\
                    & =2\Bigl[\text{Log-Likelihood}_{\text{Full Model}}-\text{Log-Likelihood}_{\text{Reduced Model}}\Bigr]
            \end{align*}
            which follows an approximate $ \chi^2(\ell) $, if $ \mathbf{H}_0 $ is true, where
            \[ \ell=(\#\text{ parameters in full model})-(\#\text{ parameters in reduced model}) \]
            \begin{itemize}
                  \item $ p\text{-value}=\Prob{T\ge t} $ where $ T \sim \chi^2(\ell) $.
            \end{itemize}
\end{itemize}
\subsection{Example: Enterprise Banner Ads}
\begin{Example}{Enterprise Banner Ads}{}
      Enterprise is experimenting with $m = 3$ banner ads as a mechanism to drive traffic to their website. Since
      there are known regional differences in consumer preferences in the US, they wish to control for the nuisance
      factor ``region'' with $b = 4$ blocks corresponding to the four major US geographic regions: Northeast (NE),
      Northwest (NW), Southeast (SE), and Southwest (SW). We randomize a total of $n_{jk} = 5000$ for all $ (j,k) $ people
      to each ad condition in each region.

      \vspace{2mm}

      Interest lies in determining whether the different ads perform similarly with respect to click-
      through-rate (CTR) --- and we wish to determine which one maximizes CTR --- but we want to control for
      the effect of region. We do so with the following logistic regression model:
      \[ \log*{\frac{\pi_i}{1-\pi_i}}=\alpha+\beta_2 x_{i2}+\beta_3 x_{i3}+\gamma_1 z_{i1}+\gamma_2 z_{i2}+\gamma_3 z_{i3} \]
      where $ x_{i2} $ and $ x_{i3} $ are condition indicators for ads 2 and 3 (ad 1 is the baseline),
      and $ z_{i1},z_{i2},z_{i3} $ are block indicators for NW, SE, SW regions (NE is the baseline).
      \begin{itemize}
            \item $ \mathbf{H}_0 $: $ \beta_2=\beta_3=0 $.
                  \begin{itemize}
                        \item $ p\text{-value}=\Prob{T\ge t_\text{C}}=\Prob{T\ge 249.924}=5.367\times 10^{-55} $ where $ T \sim \chi^2(2) $.
                        \item Therefore, we reject $ \mathbf{H}_0 $ and conclude that the design factor is significant and the CTR is \underline{not}
                              the same in every condition.
                  \end{itemize}
            \item $ \mathbf{H}_0 $: $ \gamma_1=\gamma_2=\gamma_3=0 $.
                  \begin{itemize}
                        \item $ p\text{-value}=\Prob{T\ge t_\text{B}}=\Prob{T\ge 139.824}=4.126\times 10^{-30} $ where $ T \sim \chi^2(3) $.
                        \item Therefore, we reject $ \mathbf{H}_0 $ and conclude that the nuisance factor is significant
                              and therefore blocking was a good thing to do.
                  \end{itemize}
      \end{itemize}
      \href{https://github.com/Hextical/university-notes/blob/master/year-3/semester-3/STAT 430/code/W5/Comparing_proportions_within_blocks.R}{[R Code] \texttt{Comparing\_proportions\_within\_blocks}}
\end{Example}
