\setcounter{section}{4}
\section{Review of Taylor Polynomials}
\begin{Definition}{$ n $-th degree Taylor polynomial}{}
    If $ f $ is $ n $-times differentiable at $ x=a $,
    the \textbf{nth degree Taylor polynomial} for $ f $
    centred at $ x=a $ is
    \[ T_{n,a}(x)=\sum\limits_{n=0}^{n} \frac{f^{(k)}(a)}{k!} (x-a)^k \]
\end{Definition}

\begin{Definition}{$ n $-th degree Taylor remainder function}{}
    If $ f $ is $ n $-times differentiable at $ x=a $, we define
    the \textbf{nth degree Taylor remainder function} centred at
    $ x=a $ to be:
    \[ R_{n,a}(x)=f(x)-T_{n,a}(x) \]
\end{Definition}
The \textbf{error} in using $ T_{n,a}(x) $ to approximate
$ f(x) $ is given by $ \text{Error}=\abs{R_{n,a}(x)} $.

To estimate the size of the error we use:
\begin{Theorem}{Taylor's Theorem}{}
    Assume $ f $ is $ n+1 $-times differentiable on an interval
    $ I $ containing $ x=a $. Let $ x\in I $. Then, there exists
    a point $ c $ between $ x $ and $ a $ such that
    \[ f(x)-T_{n,a}(x)=R_{n,a}(x)=
        \frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}  \]
\end{Theorem}

\begin{Corollary}{Taylor's Inequality}{}
    \[ \abs{R_{n,a}(x)}\leqslant \frac{M\abs{x-a}^{n+1}}{(n+1)!} \]
    where $ \abs{f^{(n+1)}(c)}\leqslant M $ for all $ c $
    between $ x $ and $ a $.
\end{Corollary}

\setcounter{section}{6}
\section{Taylor Series and Convergence}
Last week we examined how to obtain power series representations
for certain functions (functions related to $ \sfrac{1}{(1-x)} $),
but is there a more general method? Let's see!

Suppose $ f(x)=\sum\limits_{n=0}^{\infty} a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\cdots $
for $ \abs{x-a}<R $, $ R>0 $. What are the $ a_n $'s?

First, at $ x=a $: $ f(a)=a_0 $. Next: differentiate!
\[ f^\prime(x)=a_1+2a_2(x-a)+3a_3(x-a)^2+\cdots \]
at $ x=a $: $ f^\prime(a)=a_1 $. Keep going!
\[ f^{\prime\prime}=2a_2+6a_3(x-a)+\cdots\implies f^{\prime\prime}(a)=2a_2 \]
Therefore $ a_2=\dfrac{f^{\prime\prime}(a)}{2} $.

Another iteration gives $ a_3=\dfrac{f^{(3)}(a)}{6}=\dfrac{f^{(3)}(a)}{3!}  $, etc.

In general: $ a_n=\dfrac{f^{(n)}(a)}{n!} $.

Hey look! We just proved:

\begin{Theorem}{}{}
    If $ f(x) $ has a power series representation about $ x=a $, say $ f(x)=\sum\limits_{n=0}^{\infty}
        a_n(x-a)^n $ for $ \abs{x-a}<R $, $ R>0 $, then
    \[ a_n=\frac{f^{(n)}(a)}{n!} \]
    That is,
    \[ f(x)=\sum\limits_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n  \]
    Called the \textbf{Taylor series} for $ f $ centred at $ x=a $.
\end{Theorem}

\underline{Special case}: $ a=0 $: $ \displaystyle \sum\limits_{n=0}^{\infty}
    \frac{f^{(n)}(0)}{n!} x^n $ is called the \textbf{Maclaurin series} for $ f $.

\begin{Remark}{}{}
    The theorem has some strengths and weaknesses:
    \begin{itemize}
        \item Strength: The theorem says that \emph{no matter how} you find a series
              expansion for a function:
              \begin{itemize}
                  \item Manipulating known series
                  \item Integrating/differentiating
                  \item Using the Taylor series formula
              \end{itemize}
              you will get the Taylor series for $ f $.
        \item Weakness: The theorem \emph{assumes} $ f $ has a power
              series expansion, and concludes it must be the Taylor series.
              It doesn't say that \emph{every} function is equal
              to its Taylor series.
    \end{itemize}
\end{Remark}
Indeed, some functions are \emph{not} equal
to their Taylor series. For example,
\[ f(x)=
    \begin{cases}
        \sfrac{1}{e} & \text{if }x<-1                     \\
        e^x          & \text{if }-1\leqslant x\leqslant 1 \\
        e            & \text{if }x>1
    \end{cases} \]
Let's find $ f $'s Maclaurin series! Clearly
$ f^{(n)}(0)=1 $ for all $ n $, so $ f $'s Maclaurin series is
$ \displaystyle \sum\limits_{n=0}^{\infty} \frac{x^n}{n!} $,
which we all know converges on $ \mathbb{R} $. But we also know
that $ \displaystyle
    e^x=\sum\limits_{n=0}^{\infty} \frac{x^n}{n!} $ for all $ x\in\mathbb{R} $,
but that means $ \displaystyle f(2)=e\neq e^2=\sum\limits_{n=0}^{\infty} \frac{2^n}{n!} $.
So, while both $ f $ and the series exist everywhere,
$ \displaystyle f(x)\neq \sum\limits_{n=0}^{\infty} \frac{x^n}{n!}  $ for all $ x\in\mathbb{R} $.

This means we need to develop a way to determine if a function, $ f $,
is equal to its Taylor Series on the interval of convergence.

The first thing we need to notice is that the partial sums of a
Taylor series are the Taylor polynomials!

So, what we want to determine is for which $ x\in\mathbb{R} $ is
\[ f(x)=\lim\limits_{{n} \to {\infty}} T_{n,a}(x) \]
Or, since we know $ f(x)=T_{n,a}(x) + R_{n,a}(x) $,
we need to check if
\[ \lim\limits_{{n} \to {\infty}} R_{n,a}(x)=0 \]
For each $ x $ where $ R_{n,a}(x)\to 0 $, we can conclude that
$ f(x) $ is equal to its Taylor series.

In order to show that $ R_{n,a}(x)\to 0 $, it would be
great to have a way to approximate
its size, and we do!

\begin{Theorem}{Taylor's Inequality}{}
    If $ \abs*{f^{(n+1)}(x)}\leqslant M $ for $ \abs{x-a}\leqslant d\in\mathbb{R} $,
    then
    \[ \abs{R_{n,a}(x)}\leqslant \frac{M\abs{x-a}^{n+1}}{(n+1)!}  \]
    for $ \abs{x-a}<d $.
\end{Theorem}
With this, we establish the convergence theorem!
\begin{Theorem}{Convergence Theorem for Taylor Series}{}
    Assume $ f $ has derivatives of all orders on an interval
    $ I $ containing $ x=a $.

    Assume also that there exists $ M\in\mathbb{R} $ such that
    $ \abs{f^{(k)}(x)}\leqslant M $ for all $ k\in\mathbb{N} $
    and $ x\in I $. Then,
    \[ f(x)=\sum\limits_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n \]
    for $ x\in I $.
\end{Theorem}

\begin{Proof}{}{}
    First, if $ x=a $ then $ \displaystyle \sum\limits_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(a-a)^n
        =f(a) $, so we only need to prove it for $ x\neq a $. Say $ x_0\in I $,
    $ x_0\neq a $. Taylor's Inequality says that since $ \abs{f^{(n+1)}(x)}\leqslant M $,
    \[ 0\leqslant \abs{R_{n,a}(x_0)}\leqslant
        \frac{M\abs{x_0-a}^{n+1}}{(n+1)!} \]
    Also, we have already show that $ \displaystyle\lim\limits_{{n} \to {\infty}}\frac{x^n}{n!} =0 $
    for all $ x\in\mathbb{R} $, so $ \displaystyle\lim\limits_{{n} \to {\infty}}
        \frac{M\abs{x_0-a}^{n+1}}{(n+1)!} =0 $. Therefore, by the Squeeze Theorem,
    $ \lim\limits_{{n} \to {\infty}} \abs{R_{n,a}(x_0)}=0 $, as desired.
\end{Proof}

\begin{Corollary}{}{}
    \[ e^x=\sum\limits_{n=0}^{\infty}\frac{x^n}{n!}  \]
    for all $ x\in\mathbb{R} $.
\end{Corollary}
\begin{Proof}{}{}
    Fix $ B>0 $, then for $ f(x)=e^x $, $ \abs{f^{(n+1)}(x)}=e^x\leqslant e^B $
    for all $ x\in\interval{-B}{B} $. Therefore, by the Convergence
    Theorem, $ \displaystyle e^x=\sum\limits_{n=0}^{\infty} \frac{x^n}{n!}  $ for $ x\in\interval{-B}{B} $.
    But $ B $ was arbitrary, so $ \displaystyle\sum\limits_{n=0}^{\infty} \frac{x^n}{n!} $
    for all $ x\in\mathbb{R} $.
\end{Proof}

\begin{Corollary}{}{}
    Both $ \sin(x) $ and $ \cos(x) $ are equal to their Maclaurin
    series for all $ x\in\mathbb{R} $.
\end{Corollary}

\begin{Proof}{}{}
    Both functions are infinitely differentiable on
    $ \mathbb{R} $, and their derivatives (namely $ \pm \sin(x) $ or $ \pm\cos(x) $)
    are bounded above by 1. So, by the Convergence Theorem,
    the result follows.
\end{Proof}

Now we know that $ \sin(x) $ and $ \cos(x) $ are equal to their
Maclaurin series for all $ x\in\mathbb{R} $, but we haven't
determined what they are!

Let's start with the Maclaurin series for $ \cos(x) $.
\begin{itemize}
    \item $ f(x)=\cos(x)\implies f(0)=1 $
    \item $ f^\prime(x)=-\sin(x)\implies f^\prime(0)=0 $
    \item $ f^{\prime\prime}(x)=-\cos(x)\implies f^{\prime\prime}(0)=-1 $
    \item $ f^{(3)}(0)=\sin(x)\implies f^{(3)}(0)=0 $
    \item $ f^{(4)}(0)=\cos(x)\implies f^{(4)}(0)=1 $
\end{itemize}
this repeats.

So, the series is
\[ 1-\frac{x^2}{2!} +\frac{x^4}{4!} -\frac{x^6}{6!} +\cdots=
    \sum\limits_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}  \]
So, for all $ x\in\mathbb{R} $,
\[ \cos(x)=\sum\limits_{n=0}^{\infty}\frac{(-1)^n x^{2n}}{(2n)!}   \]
What about $ \sin(x) $? We could use the formula, or integrate the
series for $ \cos(x) $.
\[ \sin(x)=\int \cos(x)\, d{x} =
    \int \sum\limits_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!} \, d{x}
    =\sum\limits_{n=0}^{\infty} \left[ \frac{(-1)^n x^{2n+1}}{(2n)!(2n+1)} \right]+C  \]
But $ \sin(0)=0\implies 0=0+c\implies C=0 $. So, for all $ x\in\mathbb{R} $,
\[ \sin(x)=\sum\limits_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!}  \]

The point is: we can use whatever methods we want to find a Taylor/Maclaurin
series, we don't always need to use the formula!

\begin{Example}{}{}
    Find the Taylor series for $ e^x $ centred at $ x=3 $.

    \textbf{Solution.} We want powers of $ (x-3) $,
    and we know $ \displaystyle e^x=\sum\limits_{n=0}^{\infty} \frac{x^n}{n!} $. So,
    \[ e^x=e^{x-3+3}=e^3 e^{x-3}
        =e^3 \sum\limits_{n=0}^{\infty} \frac{(x-3)^n}{n!}
        =\sum\limits_{n=0}^{\infty} \frac{e^3}{n!}(x-3)^n \]
\end{Example}

\begin{Example}{}{}
    Find the Maclaurin Series for $ f(x)=x^2\sin(x) $.

    \textbf{Solution.}
    \[ x^2\sin(x)=x^2
        \sum\limits_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!}
        =\sum\limits_{n=0}^{\infty} \frac{(-1)^n x^{2n+3}}{(2n+1)!} \]
    for all $ x\in\mathbb{R} $ with $ R=\infty $.
\end{Example}

\begin{Example}{}{}
    Find the Taylor series about $ x=\sfrac{\pi}{2} $ for $ f(x)=\sin(x) $.

    \textbf{Solution.}
    \begin{align*}
        \sin(x) & = \sin\left[ \left(x-\frac{\pi}{2}\right)+\frac{\pi}{2} \right]                                                                                                   \\
                & =\sin\left( x-\frac{\pi}{2}  \right)\cos\left( \frac{\pi}{2} \right) +\cos\left( x-\frac{\pi}{2} \right)\sin\left( \frac{\pi}{2} \right) & \text{trig.\ identity} \\
                & =\cos\left( x-\frac{\pi}{2} \right)                                                                                                                               \\
                & =\sum\limits_{n=0}^{\infty} \frac{(-1)^n \left( x-\frac{\pi}{2} \right)^{2n}}{(2n)!}
    \end{align*}
    for all $ x\in\mathbb{R} $ with $ R=\infty $.
\end{Example}
