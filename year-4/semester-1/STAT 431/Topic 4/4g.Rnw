\section*{Topic 4g: A Generalization to Three-way Tables}
\addcontentsline{toc}{section}{Topic 4g: A Generalization to Three-way Tables}
\subsubsection*{Log Linear Models for 2-Way Tables}
\begin{itemize}
    \item Subjects are classified with respect to tow factor variables denoted $V$ and $W$ with
          $I$ and $J$ levels respectively.
    \item We are interested in testing for \textcolor{Blue}{independence}
          \[ \HN\colon \pi_{ij}=\pi_{i\bullet}\pi_{\bullet j}. \]
    \item The corresponding log linear model is:
          \[ \textcolor{Blue}{\log{\mu_{ij}}=u+u_i^V+u_j^W} \]
          with $ u_1^V=u_1^W=0 $ (corner-point constraints).
    \item $ \text{Number of model parameters}=1+(I-1)+(J-1)=I+J-1 $.
    \item Deviance test statistic:
          \[ D=2\sum_i\sum_j y_{ij}\log*{\frac{y_{ij}}{y_{i\bullet}y_{\bullet j}/y_{\bullet\bullet}}} \sim \chi^2_{(I-1)(J-1)}\text{ under $ \HN $}. \]
    \item $ \text{Residual df}=IJ-I-J-1=(I-1)(J-1) $.
\end{itemize}
\subsection*{3-way Contingency Tables}
\addcontentsline{toc}{subsection}{3-way Contingency Tables}
\begin{itemize}
    \item Consider the general problem in which subjects are classified with respect to three
          factor variables denoted $V$, $W$, and $Z$ with $I$, $J$, and $K$ levels respectively.
    \item As with two-way tables, we initially assume
          \[ \textcolor{Blue}{Y_{ijk}\sim \POI{\mu_{ijk}}}, \]
          $ i=1,2,\ldots,I $, $ j=1,2,\ldots,J $, $ k=1,2,\ldots,K $.
    \item As before, if $ Y_{\bullet\bullet\bullet}=y_{\bullet\bullet\bullet} $ is fixed by design (as it usually would be),
          we condition on this to give the multinomial distribution:
          \[ \textcolor{Blue}{ \Prob{Y_{ijk}=y_{ijk}\forall (i,j,k)\given Y_{\bullet\bullet\bullet}=y_{\bullet\bullet\bullet} }
              =\frac{y_{\bullet\bullet\bullet}!}{\prod_i\prod_j\prod_k y_{ijk}!}\prod_i\prod_j\prod_k \pi_{ijk}^{y_{ijk}}}. \]
    \item $ \pi_{ijk}=\mu_{ijk}/\mu_{\bullet\bullet\bullet}=\Prob{V=i,W=j,Z=k} $ are the parameters of interest ($ \sum\sum\sum \pi_{ijk}=1 $).
    \item In the case of 2-way contingency tables we discussed the connection between
          log-linear models and questions about the association between the two factors.
    \item Main effects accommodated non-uniform distributions of the row and column
          totals, and the interaction terms allowed for association between the two factors
          of interest.
    \item In terms of an association, it was either present or absent.
    \item As we will see in what follows, with 3-way tables (contingency tables involving 3
          factor variables) the nature of the associations present may be somewhat more
          complicated.
          \begin{enumerate}[1.]
              \item Mutual Independence.
              \item Joint Independence.
              \item Conditional Independence.
              \item Homogeneous Association.
          \end{enumerate}
    \item The \textcolor{Blue}{saturated} model for a 3-way contingency table is:
          \[ \textcolor{Blue}{
              \log{\mu_{ijk}}=u+u_i^V+u_j^W+u_k^Z+u_{ij}^{VW}+u_{ik}^{VZ}+u_{jk}^{WZ}+u_{ijk}^{VWZ}
              } \]
          with corner-point constraints:
          \begin{itemize}
              \item $ u_1^V=u_1^W=u_1^Z=0 $.
              \item $ u_{1j}^{VW}=u_{i1}^{VW}=u_{1k}^{VZ}=u_{i1}^{VZ}=u_{1k}^{WZ}+u_{j1}^{WZ}=0 $ for all $ i,j,k $.
              \item $ u_{1jk}^{VWZ}=u_{i1k}^{VWZ}=u_{ij1}^{VWZ} $ for all $ i,j,k $.
          \end{itemize}
    \item Shorthand notation: This model is denoted $ \textcolor{Blue}{(VWZ)} $ where we list the highest order terms involving each of the factors.
    \item It provides a perfect fit to the data
          \begin{align*}
              \tilde{\pi}_{ijk} & =y_{ijk}/y_{\bullet\bullet\bullet},                  \\
              \tilde{\mu}_{ijk} & =y_{\bullet\bullet\bullet}\tilde{\pi}_{ijk}=y_{ijk}.
          \end{align*}
    \item To investigate the relationship between factors $ V $, $ W $, and $ Z $ we will consider simpler log-linear models.
\end{itemize}
\subsubsection*{1. Mutual Independence $\HN\colon \pi_{ijk}=\pi_{i\bullet\bullet}\pi_{\bullet j\bullet}\pi_{\bullet\bullet k}$}
\begin{itemize}
    \item $ \HN $: All 3 factors $V$, $W$, and $Z$ are independent of each other.
          \[ \textcolor{HotPink}{\pi_{ijk}=\pi_{i\bullet\bullet}\pi_{\bullet j\bullet}\pi_{\bullet\bullet k}}, \]
          \[ \textcolor{HotPink}{\Prob{V=i,W=j,Z=k}=\Prob{V=i}\Prob{W=j}\Prob{Z=k}}. \]
    \item The corresponding log-linear model is $ \textcolor{Blue}{(V,W,Z)} $
          \[ \textcolor{Blue}{\log{\mu}_{ijk}=u+u_i^V+u_j^W+u_k^Z} \]
          with $ u_1^V=u_1^W=u_1^Z=0 $ (with corner-point constraints).
    \item This model will fit the marginal totals exactly.
    \item The fitted values are:
          \[ \hat{\mu}_{i j k
              }=y_{\bullet\bullet\bullet} \hat{\pi}_{ijk}
              =y_{\bullet\bullet\bullet} \hat{\pi}_{i\bullet k} \hat{\pi}_{\bullet j\bullet}
              =y_{\bullet\bullet\bullet}\biggl(\frac{y_{i\bullet k}}{y_{\bullet\bullet\bullet}}\biggr)
              \biggl(\frac{y_{\bullet j\bullet}}{y_{\bullet\bullet\bullet}}\biggr) \]
    \item $ \text{Number of model parameters}=1+(I-1)+(J-1)+(K-1)+(I-1)(K-1) $.
    \item $ \text{Residual df}=IJK-(IK+J-1) $.
    \item Similar to ordinary 2-way independence between $W$ and a new variable with $IK$
          levels of $V$ and $Z$ combined.
    \item The joint distribution of $(V,Z)$ is the same at any level of $W$.
    \item For 3-way tables there are 3 possible joint independence hypotheses and models:
          $(V,WZ)$, $(VZ,W)$, and $(VW,Z)$.
\end{itemize}
\subsubsection*{2. Joint Independence $ \HN\colon \pi_{ijk}=\pi_{i\bullet k}\pi_{\bullet j\bullet} $}
\begin{itemize}
    \item $ \HN $: Factor $ W $ is jointly independent of $ V $ and $ Z $
          \[ \textcolor{HotPink}{\pi_{ijk}=\pi_{i\bullet k}\pi_{\bullet j\bullet}}, \]
          \[ \textcolor{HotPink}{\Prob{V=i,W=j,Z=k}=\Prob{V=i,Z=k}\Prob{W=j}}. \]
    \item The nature of the association between $ V $ and $ Z $ does not depend on the level of $ W $.
    \item The corresponding log-linear model is $ \textcolor{Blue}{(VZ,W)} $
          \[ \log{\mu_{ijk}}=u+u_i^V+u_j^W+u_k^Z+u_{ik}^{VZ} \]
          with $ u_1^V=u_1^W=u_1^Z $, $ u_{1k}^{VZ}=u_{i1}^{VZ}=0 $ for all $ i,k $.
    \item This model will fit the marginal totals and $ VZ $ combination totals ($ y_{i\bullet k} $) exactly.
    \item The fitted values are:
          \[ \hat{\mu}_{ijk}=y_{\bullet\bullet\bullet}\hat{\pi}_{ijk}=y_{\bullet\bullet\bullet}\hat{\pi}_{i\bullet k}\hat{\pi}_{\bullet j\bullet}=y_{\bullet\bullet\bullet}\biggl(\frac{y_{i\bullet k}}{y_{\bullet\bullet\bullet}}\biggr)\biggl(\frac{y_{\bullet j\bullet}}{y_{\bullet\bullet\bullet}}\biggr). \]
    \item $ \text{Number of model parameters}=1 + (I - 1) + (J - 1) + (K - 1) + (I - 1)(K - 1) $.
    \item $ \text{Residual df}=IJK-(IK+J-1) $.
    \item Similar to ordinary 2-way independence between W and a new variable with $IK$
          levels of $V$ and $Z$ combined.
    \item The joint distribution of $(V,Z)$ is the same at any level of $W$.
    \item For 3-way tables there are 3 possible joint independence hypotheses and models:
          $(V,WZ)$, $(VZ,W)$, and $(VW,Z)$.
\end{itemize}
\subsubsection*{3. Conditional Independence $ \HN\colon \pi_{ij\mid k}=\pi_{i\bullet\mid k}\pi_{\bullet j\mid k} $}
\begin{itemize}
    \item Conditional probability notation: $ \pi_{ij\mid k}=\pi_{ijk}/\pi_{\bullet\bullet k} $
          \[ \pi_{ijk}=\pi_{ij\mid k}\pi_{\bullet\bullet k}, \]
          \[ \Prob{V=i,W=j,Z=k}=\Prob{V=i,W=j\given Z=k}\Prob{Z=k}. \]
    \item $ \HN $: Factors $ V $ and $ W $ are conditionally independent given $ Z $.
          \[ \textcolor{HotPink}{
                  \pi_{ijk}=\pi_{ij\mid k}\pi_{\bullet\bullet k}=\pi_{i\bullet\mid k}\pi_{\bullet j\mid k}
              }, \]
          \[ \textcolor{HotPink}{
                  \Prob{V=i,W=j,Z=k}=\Prob{V=i\given Z=k}\Prob{W=j\given Z=k}\Prob{Z=k}
              }. \]
    \item That is, the association between $ V $ and $ W $ can be \emph{fully explained} by $ Z $.
    \item The corresponding log-linear model is $ \textcolor{Blue}{(VZ,WZ)} $
          \[ \textcolor{Blue}{\log{\mu}_{ijk}=u_i^V+u_j^W+u_k^Z+u_k^Z+u_{ik}^{VZ}+u_{jk}^{WZ}}. \]
    \item This model will fit all marginal totals and $ VWZ $ and $ WZ $ combination totals ($ y_{i\bullet k} $
          and $ y_{\bullet jk} $ exactly).
    \item The fitted values are:
          \[ \hat{\mu}_{ijk}=y_{\bullet\bullet\bullet}\hat{\pi}_{ijk}=y_{\bullet\bullet\bullet}\frac{\hat{\pi}_{i\bullet k}\hat{\pi}_{\bullet jk}}{\hat{\pi}_{\bullet\bullet k}}=\frac{y_{i\bullet k}y_{\bullet jk}}{y_{\bullet\bullet k}}. \]
    \item $ \text{Number of model parameters}=1 + (I - 1) + (J - 1) + (K - 1) + (I - 1)(K - 1) + (J - 1)(K - 1) $.
    \item $ \text{Residual df}=IJK - (IK + JK - K) $.
    \item Similar to ordinary 2-way independence between $V$ and $W$ at each level of $Z$.
    \item That is, make $K$ 2-way tables $ (I\times J) $ and test independence of each table.
    \item For 3-way tables there are 3 possible conditional independence hypotheses and
          models: $(VZ,WZ)$, $(VW,VZ)$, and $(VW,WZ)$.
\end{itemize}
\subsubsection*{4. Homogeneous Association}
\begin{itemize}
    \item The remaining log-linear model is $ \textcolor{Blue}{(VW,VZ,WZ)} $
          \[ \textcolor{Blue}{\log{\mu_{ijk}}=u+u_i^V+u_j^W+u_k^Z+u_{ij}^{VW}+u_{ik}^{VZ}+u_{jk}^{WZ}}. \]
    \item Let's examine the model at $ k^\star $ an arbitrary fixed level of factor $ Z $:
          \begin{align*}
              \log \mu_{i j k^{\star}} & =u+u_{i}^{V}+u_{j}^{W}+u_{k^{\star}}^{Z}+u_{i j}^{V W}+u_{i k^{\star}}^{V Z}+u_{j k^{\star}}^{W Z}                                        \\
                                       & =\left(u+u_{k^{\star}}^{Z}\right)+\left(u_{i}^{V}+u_{i k^{\star}}^{V Z}\right)+\left(u_{j}^{W}+u_{j k^{\star}}^{W Z}\right)+u_{i j}^{V W} \\
                                       & =u^{\star}+u_{i}^{\star V}+u_{j}^{\star W}+u_{i j}^{V W}.
          \end{align*}
          \begin{itemize}
              \item This is a saturated model for the 2-way table of $ V $ and $ W $ at $ Z=k^\star $.
              \item $ V $ and $ W $ are not independent at level $ Z=k^\star $.
              \item However, at a different level $ Z=k^\dagger $, the parameter $ u_{ij}^{VW} $ representing the association between $ V $ and $ W $
                    does not change.
          \end{itemize}
    \item \textcolor{Blue}{Homogeneous Association}: There is a relationship between all pairs of factors, but
          the nature of the association is the same (i.e., homogeneous) for all levels of the
          third factor.
    \item The fitted values are not given by simple, intuitive formulas.
    \item $ \text{Number of model parameters}=1 + (I- 1) + (J- 1) + (K- 1) + (I- 1)(J- 1) + (I- 1)(K- 1) + (J- 1)(K- 1) $.
    \item $ \text{Residual df}=(I- 1)(J- 1)(K- 1) $.
    \item For 3-way tables there is only one homogeneous association hypothesis and model
          $(UW, VZ, WZ)$.
    \item The relationship implied by this model is also sometimes referred to as All Pairs
          Conditionally Independent.
\end{itemize}
\subsubsection*{Testing Nested Models for 3-way Contingency Tables}
These are called \textcolor{Blue}{hierarchical} log-linear models:
\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \textcolor{Blue}{Type of Independence} & \textcolor{Blue}{Null Hypothesis}                                               & \textcolor{Blue}{Log Linear Model}    \\
        \midrule
        None                                   & ---                                                                             & $ (VWZ) $                             \\
        Homogeneous Association                & 3x Conditional Independence $ \HN $                                             & $ (VW,VZ,WZ) $                        \\
        Conditional Independence               & $ \pi_{ij\mid k}=\pi_{i\bullet k}\pi_{\bullet j\mid k} $                        & $ (VZ,WZ) $, $ (VW,WZ) $, $ (VW,VZ) $ \\
        Joint Independence                     & $ \pi_{ijk}=\pi_{i\bullet k}\pi_{\bullet j\bullet} $                            & $ (VZ,W) $, $ (V,WZ) $, $ (VW,Z) $    \\
        Mutual Independence                    & $ \pi_{ijk}=\pi_{i\bullet\bullet}\pi_{\bullet j\bullet}\pi_{\bullet\bullet k} $ & $ (V,W,Z) $                           \\
        \bottomrule
    \end{tabular}
\end{table}
\subsubsection*{Goodness of Fit Statistics for Log Linear Models}
\begin{itemize}
    \item The fit of a log linear model can be judged based on the deviance assuming an
          underlying Poisson distribution for the cell counts.
    \item We know from before that the deviance statistic has the form
          \[ D=2\sum_i\sum_j\sum_k O_{ijk}\log*{\frac{O_{ijk}}{E_{ijk}}}. \]
    \item $ D \sim \chi^2_{(IJK)-q} $ under $ \HN $ where $ q $ is the number of parameters in the model under $ \HN $.
    \item For nested models:
          \[ \Delta D=D_0-D_A \sim \chi^2_{p-q}. \]
\end{itemize}
\subsection*{Application 1: General Social Survey}
\addcontentsline{toc}{subsection}{Application 1: General Social Survey}
\begin{Example}{2008 US General Social Survey ($ 2\times 5\times 7 $)}
    \begin{center}
        \begin{NiceTabular}{crrrrrrrr}
            Gender ($ G $)     & Highest Degree ($ D $) & \Block{1-7}{Political Party Affiliation ($ P $)} \\
            &                        & 1  & 2  & 3  & 4  & 5  & 6  & 7 \\
            \midrule
            \Block{5-1}{Males}   &
            $<$ High school      & 32                     & 20 & 18 & 29 & 11 & 12 & 9      \\
            &$<$ High school      & 67                     & 85 & 63 & 68 & 48 & 65 & 44     \\
            &Junior college       & 12                     & 14 & 6  & 9  & 13 & 17 & 6      \\
            &Bachelor             & 23                     & 21 & 29 & 20 & 19 & 32 & 20     \\
            &Graduate             & 16                     & 9  & 12 & 13 & 7  & 14 & 13     \\
            \midrule
            \Block{5-1}{Females} &
            $<$ High school      & 31                     & 25 & 16 & 58 & 8  & 8  & 16     \\
            &High school          & 118                    & 98 & 69 & 88 & 30 & 82 & 54     \\
            &Junior college       & 20                     & 16 & 13 & 13 & 7  & 16 & 7      \\
            &Bachelor             & 33                     & 23 & 28 & 11 & 16 & 44 & 23     \\
            &Graduate             & 38                     & 20 & 8  & 13 & 3  & 13 & 9      \\
            \bottomrule
        \end{NiceTabular}
    \end{center}
\end{Example}
\begin{itemize}
    \item Note that there is no obvious response variable.
    \item Since we are interested in the association among all three variables, we consider
          methods based on log-linear models.
    \item Let $G$ denote gender, $D$ denote highest degree obtained, and $P$ denote political
          party affiliation.
    \item We know the log linear model
          \[ \textcolor{Blue}{
              \log{\mu_{ijk}}
              =u+u_i^G+u_j^D+u_k^P+u_{ij}^{GD}+u_{ik}^{GP}+u_{jk}^{DP}+u_{ijk}^{GDP}
              } \]
          will provide a perfect fit to the data (since it is saturated).
    \item We seek to find a simpler model which describes the data well.
    \item In other words, we are looking for a simpler representation of the relationship
          between the gender, highest degree, and political party affiliation.
\end{itemize}
\subsubsection*{R Code}
%\begin{noindent}
      <<results='hide'>>=
      ## Input the data for the 5 x 7 x 2 contingency table
      freq = c(32,67,12,23,16,20,85,14,21,9,18,63,6,29,12,29,68,9,
      20,13,11,48,13,19,7,12,65,17,32,14,9,44,6,20,13,
      31,118,20,33,38,25,98,16,23,20,16,69,13,28,8,58,88,
      13,11,13,8,30,7,16,3,8,82,16,44,13,16,54,7,23,9)
      names = list(D=c("LT HSc","HSc","JunCol","Bachelor","Graduate"),
      P=c("1","2","3","4","5","6","7"),
      G=c("male","female"))
      party.3D = array (freq, c(5,7,2), dimnames=names)

      ## Flattened contingency table
      library(plyr)
      party = count(as.table(party.3D))
      party = party[,1:4]
      names(party)=c("D","P","G","Y")

      # Fit the saturated model
      model1 = glm( Y ~ G*D*P, family=poisson, data=party)
      model1$df.residual
      model1$deviance

      # Fit the homogeneous association model
      model2 <- glm(Y ~ G*D + G*P + D*P, family=poisson, data=party)
      model2$df.residual
      model2$deviance
      1-pchisq(model2$deviance-model1$deviance,model2$df.residual-model1$df.residual)

      # Fit the three conditional independence models
      model3 <- glm( Y ~ G*D + G*P, family=poisson, data=party)
      model3$df.residual
      model3$deviance
      1-pchisq(model3$deviance-model2$deviance,model3$df.residual-model2$df.residual)

      model4 <- glm( Y ~ G*D + D*P, family=poisson, data=party)
      model4$df.residual
      model4$deviance
      1-pchisq(model4$deviance-model2$deviance,model4$df.residual-model2$df.residual)

      model5 <- glm( Y ~ G*P + D*P, family=poisson, data=party)
      model5$df.residual
      model5$deviance
      1-pchisq(model5$deviance-model2$deviance,model5$df.residual-model2$df.residual)

      # Fit the two joint independence models nested within model5
      model6 <- glm( Y ~ G + D*P, family=poisson, data=party)
      model6$df.residual
      model6$deviance
      1-pchisq(model6$deviance-model5$deviance,model6$df.residual-model5$df.residual)

      model7 <- glm( Y ~ G*P + D, family=poisson, data=party)
      model7$df.residual
      model7$deviance
      1-pchisq(model7$deviance-model5$deviance,model7$df.residual-model5$df.residual)
      @
%\end{noindent}
\subsubsection*{R Output: Models 1 $(GDP)$ and 2 $(GD, GP, DP)$}
%\begin{noindent}
      <<>>=
      # Fit the saturated model
      model1 = glm( Y ~ G*D*P, family=poisson, data=party)
      model1$df.residual
      model1$deviance

      # Fit the homogeneous association model
      model2 <- glm(Y ~ G*D + G*P + D*P, family=poisson, data=party)
      model2$df.residual
      model2$deviance
      1-pchisq(model2$deviance-model1$deviance,
      model2$df.residual-model1$df.residual)
      @
%\end{noindent}
\begin{itemize}
    \item $ \HN $: Homogeneous association model (2) is adequate
          \[ \HN\colon u_{ijk}^{GDP}=0\;\forall i,j,k\text{ versus }\HA\colon \exists i,j,k\text{ s.t. }u_{ijk}^{GDP}\ne 0. \]
          \[ \Delta D=D_0-D_A=28.818-0 \sim \chi^2_{24}\text{ under $ \HN $}. \]
          \[ p=\Prob*{\chi^2_{24}>28.818}=0.227. \]
    \item \textcolor{Blue}{Do not reject} $ \HN $ that the fit of model 2 is adequate, as compared to model 1.
\end{itemize}
\subsubsection*{R Output: Models 3 $(GD, GP)$, 4 $(GD, DP)$, 5 $(GP, DP)$}
%\begin{noindent}
      <<>>=
      model3 <- glm( Y ~ G*D + G*P, family=poisson, data=party)
      model3$df.residual
      model3$deviance
      1-pchisq(model3$deviance-model2$deviance,model3$df.residual-model2$df.residual)
      model4 <- glm( Y ~ G*D + D*P, family=poisson, data=party)
      model4$df.residual
      model4$deviance
      1-pchisq(model4$deviance-model2$deviance,model4$df.residual-model2$df.residual)
      model5 <- glm( Y ~ G*P + D*P, family=poisson, data=party)
      model5$df.residual
      model5$deviance
      1-pchisq(model5$deviance-model2$deviance,model5$df.residual-model2$df.residual)
      @
%\end{noindent}
\begin{itemize}
    \item \textcolor{Blue}{Reject} $ \HN $ that the fit of models 3 and 4 are adequate, as compared to model 2.
    \item \textcolor{Blue}{Do no reject} $ \HN $ that the fit of model 5 is adequate, as compared to model 2.
\end{itemize}
\subsubsection*{R Output: Models 6 $(G,DP)$ and 7 $(D, GP)$}
%\begin{noindent}
      <<>>=
      # Fit the two joint independence models nested within model5
      model6 <- glm( Y ~ G + D*P, family=poisson, data=party)
      model6$df.residual
      model6$deviance
      1-pchisq(model6$deviance-model5$deviance,model6$df.residual-model5$df.residual)

      model7 <- glm( Y ~ G*P + D, family=poisson, data=party)
      model7$df.residual
      model7$deviance
      1-pchisq(model7$deviance-model5$deviance,model7$df.residual-model5$df.residual)
      @
%\end{noindent}
\begin{itemize}
    \item \textcolor{Blue}{Reject} $ \HN $ that the fit of models 6 and 7 are adequate, as compared to model 5. That is,
          we can conclude that model 5 is the ``best'' model.
\end{itemize}
%\begin{noindent}
      <<echo=FALSE,fig.width=8.5,fig.height=5>>=
      par(mfrow=c(2,3))
      plot(model2$fitted.values,residuals.glm(model2, type = "deviance"),xlab="Fitted Values",ylab="Deviance Residuals",main="Model 2",ylim=c(-4.5,4.5),xlim=c(0,125))
      abline(h = -2,lty=2)
      abline(h = 2,lty=2)
      plot(model3$fitted.values,residuals.glm(model3, type = "deviance"),xlab="Fitted Values",ylab="Deviance Residuals",main="Model 3",ylim=c(-4.5,4.5),xlim=c(0,125))
      abline(h = -2,lty=2)
      abline(h = 2,lty=2)
      plot(model4$fitted.values,residuals.glm(model4, type = "deviance"),xlab="Fitted Values",ylab="Deviance Residuals",main="Model 4",ylim=c(-4.5,4.5),xlim=c(0,125))
      abline(h = -2,lty=2)
      abline(h = 2,lty=2)
      plot(model5$fitted.values,residuals.glm(model5, type = "deviance"),xlab="Fitted Values",ylab="Deviance Residuals",main="Model 5",ylim=c(-4.5,4.5),xlim=c(0,125))
      abline(h = -2,lty=2)
      abline(h = 2,lty=2)
      plot(model6$fitted.values,residuals.glm(model6, type = "deviance"),xlab="Fitted Values",ylab="Deviance Residuals",main="Model 6",ylim=c(-4.5,4.5),xlim=c(0,125))
      abline(h = -2,lty=2)
      abline(h = 2,lty=2)
      plot(model7$fitted.values,residuals.glm(model7, type = "deviance"),xlab="Fitted Values",ylab="Deviance Residuals",main="Model 7",ylim=c(-4.5,4.5),xlim=c(0,125))
      abline(h = -2,lty=2)
      abline(h = 2,lty=2)
      @
%\end{noindent}
\subsubsection*{Summary of Fitted Models}
The following analysis of deviance table summarizes our findings.
\begin{table}[H]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        \textcolor{Blue}{Model} & \textcolor{Blue}{Form} & \textcolor{Blue}{Residual Deviance} & \textcolor{Blue}{Residual d.f.} & \textcolor{Blue}{$ p $-value} \\
        \midrule
        1                       & $ (GDP) $              & 0                                   & 0                               & NA                            \\
        \midrule
        2                       & $(GD, GP, DP)$         & $28.82$                             & $24$                            & $0.228$ (vs 1)                \\
        \midrule
        3                       & $(GD, GP)$             & $130.34$                            & $48$                            & $0.000$ (vs 2)                \\
        4                       & $(GD, DP)$             & $52.77$                             & $30$                            & $0.001$ (vs 2)                \\
        5                       & $(GP, DP)$             & $29.32$                             & $28$                            & $0.973$ (vs 2)                \\
        \midrule
        6                       & $(G, DP)$              & $53.84$                             & $34$                            & $0.000$ (vs 5)                \\
        7                       & $(D, GP)$              & $131.41$                            & $52$                            & $0.000$ (vs 5)                \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{itemize}
    \item \textcolor{Green}{Conclude that Model 5 $(GP, DP)$ is most appropriate}.
    \item \textcolor{Blue}{Conditional Independence}: The responders educational level ($D$) is conditionally
          independent of his/her gender ($G$), given his/her party affiliation ($P$).
    \item We will return to this analysis in the next topic to discuss interpretation of the
          regression parameters.
\end{itemize}