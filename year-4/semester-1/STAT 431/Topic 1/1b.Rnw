\makeheading{Week 2}{\daterange{2021-09-13}{2021-09-17}}
\section*{Topic 1b: A Brief Review of Likelihood Methods}
\addcontentsline{toc}{section}{Topic 1b: A Brief Review of Likelihood Methods}
\subsection*{Likelihood Methods for Scalar Parameters}
\addcontentsline{toc}{subsection}{Likelihood Methods for Scalar}
\begin{Regular}{Setup}
      \begin{itemize}
            \item $ Y $ is a random variable with a probability density or mass function $ f(y\mid \theta) $, where
                  $ \theta\in\Omega $ is a continuous parameter.
            \item The true value of $ \theta $ is unknown
            \item We wish to make inferences about $ \theta $ (i.e., we may want to estimate $ \theta $, or carry out
                  tests of hypotheses regarding $ \theta $).
      \end{itemize}
\end{Regular}
\begin{itemize}
      \item Today's material: Appendix A1 \& A2 of Stat 431 course notes, Dunn \& Smyth Chapter 4, Stat 330 notes.
\end{itemize}
\subsection*{Likelihood Function}
\begin{itemize}
      \item The \textcolor{Blue}{Likelihood function} is any function which is proportional to the probability of
            observing the data you actually obtained:
            \[ \mathcal{L}(\theta\mid y)=c\Prob{Y=y\mid\theta}=cf(y\mid\theta) \]
      \item $ c $ is a \emph{proportionality constant} which may be any positive function that does not
            depend on $ \theta $.
      \item $ \mathcal{L}(\theta\mid y) $ contains all the information regarding $ \theta $ from the data.
      \item $ \mathcal{L}(\theta\mid y) $ ranks the parameter values of their consistency with the data.
      \item Since $ \mathcal{L}(\theta\mid y) $ is defined in terms of the random variable $ y $, it is
            itself a random variable.
\end{itemize}
\subsection*{Maximum Likelihood Estimator}
\begin{itemize}
      \item For the purposes of estimation we typically want to find the parameter value that
            makes the observed data the most likely (hence the term \textcolor{Blue}{maximum likelihood}).
      \item The \textcolor{Blue}{maximum likelihood estimator} (MLE) of $ \theta $ is the value $ \hat{\theta} $ that maximizes the
            likelihood function, that is:
            \[ \mathcal{L}(\hat{\theta}\mid y)\ge \mathcal{L}(\theta\mid y)\qquad\forall \theta\in \Omega \]
      \item Estimation is a simple optimization problem.
      \item Equivalently, since the log function is monotonic, $ \hat{\theta} $ maximizes the \textcolor{Blue}{log-likelihood function}:
            $ \ell(\theta\mid y)=\log[\big]{\mathcal{L}(\theta\mid y)} $.
      \item Often it is easier to work with $ \ell(\theta\mid y) $ rather than $ \mathcal{L}(\theta\mid y) $.
      \item For simplicity drop the $ y $ and use $ \mathcal{L}(\theta)=\mathcal{L}(\theta\mid y) $.
\end{itemize}
\subsection*{Other Important Functions}
\begin{itemize}
      \item $ \ell(\theta)=\log[\big]{\mathcal{L}(\theta)} $ be the \textcolor{Blue}{log-likelihood function}.
      \item $ S(\theta)=\ell^\prime(\theta) $ be the first derivative of the log-likelihood function which is called
            the \textcolor{Blue}{Score function}.
      \item $ I(\theta)=-\ell^{\prime\prime}(\theta) $ be the negative second derivative of the log-likelihood function
            which is called the \textcolor{Blue}{Information function}.
      \item $ \mathcal{I}(\theta)=\E{I(\theta)} $ be the \textcolor{Blue}{Expected information function}.
      \item $ R(\theta)=\mathcal{L}(\theta)/\mathcal{L}(\hat{\theta}) $ be the \textcolor{Blue}{Relative likelihood function}, which is the likelihood
            function standardized by its maximum value so that the relative likelihood will
            have a maximum value of $ 1 $ ($ 0\le R(\theta)\le 1 $).
      \item $ r(\theta)=\log[\big]{\mathcal{L}(\theta)/\mathcal{L}(\hat{\theta})} $ be the \textcolor{Blue}{log relative likelihood function} which
            will have a maximum value of $ 0 $.
\end{itemize}
\subsection*{Maximum Likelihood Estimation}
\begin{itemize}
      \item Want $ \theta $ that maximizes $ \ell(\theta) $, or equivalently solves $ S(\theta)=0 $.
      \item Sometimes $ S(\theta)=0 $ can be solved explicitly (easy in this case), but often we must solve iteratively.
      \item Check that the solution corresponds to a maxima of $ \ell(\theta) $ by verifying the value of
            the second derivative at $ \hat{\theta} $ is negative, or:
            \[ I(\hat{\theta})=-\ell^{\prime\prime}(\hat{\theta})>0 \]
      \item Should also check if there are any values of $ \theta $ at the edges of $ \Omega $ that give
            a local maxima of $ \ell(\theta) $.
      \item \textcolor{Blue}{Invariance property of MLEs}: if $ g(\theta) $ is any function of the parameter
            $ \theta $, then the MLE of $ g(\theta) $ is $ g(\hat{\theta}) $.
\end{itemize}
\subsection*{Example: Poisson Distribution}
\begin{Example}{Example: Poisson Distribution}
      Let $ Y_1,Y_2,\ldots,Y_n $ be iid Poisson random variables with
      \[ f(y_i\mid \theta)=\frac{\theta^{y_i}e^{-\theta}}{y_i!},\qquad \theta>0,y_i=0,1,2,\ldots \]
      with unknown parameter $ \theta $. \textcolor{Green}{Find the MLE of $ \theta $}.
\end{Example}
\begin{itemize}
      \item \textcolor{Blue}{Likelihood function}:
            \[ \mathcal{L}(\theta\mid y)=\prod_{i=1}^n f(y_i\mid \theta)=\prod_{i=1}^n \frac{\theta^{y_i}\exp{-\theta}}{y_i!}=\frac{\theta^{\sum_i y_i}\exp{-n\theta}}{\prod_{i}y_i!}  \]
      \item \textcolor{Blue}{log-likelihood function}:
            \[ \ell(\theta\mid y)=\Bigl(\sum y_i \Bigr)\log{\theta}-n\theta-\sum\bigl(\log{y_i!}\bigr)  \]
      \item \textcolor{Blue}{Score function}:
            \[ S(\theta)=\odv{\ell}{\theta}=\frac{1}{\theta}\sum y_i-n \]
      \item \textcolor{Green}{Maximum likelihood estimate}:
            \[ 0=\frac{1}{\hat{\theta}}\sum y_i-n\implies \hat{\theta}=\frac{\sum y_i}{n}=\bar{y}  \]
      \item Second derivative test using \textcolor{Blue}{Information function}:
            \[ I(\theta)=-\odv[order=2]{\ell}{\theta}=\frac{1}{\theta^2}\sum y_i>0\qquad\forall \theta>0  \]
            Confirms that $ \hat{\theta}=\bar{y} $ is the \textcolor{Green}{maximum likelihood estimate}.
      \item See Appendix A2 for a Binomial example.
\end{itemize}
\subsection*{Example: Topical cyclones}
\begin{Example}{Tropical cyclones}
      Number of tropical cyclones in Northeastern Australia for the 13 successive seasons
      1956-57 through 1968-69 (Dobson ยง1.6.5)
      \begin{center}
            \begin{NiceTabular}{cccccccccccccc}
                  \toprule
                  Season & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13\\
                  Cyclones  & 6 & 5 & 4 & 6 & 6 & 3 & 12 & 7 & 4 & 2 & 6 & 7 & 4\\
                  \bottomrule
            \end{NiceTabular}
      \end{center}
\end{Example}
\begin{itemize}
      \item Let $ Y_i= $ number of cyclones in season $ i $.
      \item Assume the $ Y_i $'s are iid Poisson random variables with unknown parameter $ \theta $.
      \item The \textcolor{Green}{maximum likelihood estimate} of $ \theta $ is:
            \[ \hat{\theta}=\bar{y}=\frac{\sum y_i}{n}=\frac{72}{13} =5.538  \]
\end{itemize}
\subsection*{Example: Plot of log relative likelihood function}
\[ r(\theta)=\ell(\theta)-\ell(\hat{\theta})=\Bigl(\sum y_i\Bigr)\log*{\frac{\theta}{\hat{\theta}}}-n(\theta-\hat{\theta}) \]
% \begin{noindent}
      <<echo=FALSE>>=
      y=c(6,5,4,6,6,3,12,7,4,2,6,7,4)
      n=length(y)
      theta=seq(3,8,by=0.001)
      theta.hat=mean(y)
      plot(theta,sum(y)*log(theta/theta.hat)-n*(theta-theta.hat), type="l", ylab = "log relative likelihood", xlab="")
      abline(v=theta.hat, lty = 2)
      @
% \end{noindent}
\subsection*{Newton Raphson Algorithm}
\addcontentsline{toc}{subsection}{Newton Raphson}
\begin{itemize}
      \item Sometimes we need to solve $ S(\theta) $ iteratively.
      \item \textcolor{Blue}{Taylor Series} expansion of a differentiable function $ f(x) $:
            \[ f(x)=f(a)+\frac{f^\prime(a)}{1!}(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n+\cdots  \]
      \item Now suppose we wish to find $ \hat{\theta} $, the root of $ S(\theta)=0 $, and $ \theta_0 $ is a guess that is ``close to $ \hat{\theta} $.''
      \item Consider the Taylor series expansion of $ S(\theta) $ about $ \theta_0 $:
            \[ S(\theta)=S(\theta_0)+\frac{S^\prime(\theta_0)}{1!}(\theta-\theta_0)+\frac{S^{\prime\prime}(\theta_0)}{2!}(\theta-\theta_0)^2+\cdots \]
      \item For $ \abs{\theta-\theta_0} $ small, we can drop the second and higher order terms and to a good approximation we have:
            \begin{align*}
                  S(\theta) & \simeq S(\theta_0)+S^\prime(\theta_0)(\theta-\theta_0) \\
                  S(\theta) & \simeq S(\theta_0)-I(\theta_0)(\theta-\theta_0)
            \end{align*}
      \item We are approximating $ S(\theta) $ with a linear function that has the same value and slope as $ S(\theta) $
            at $ \theta=\theta_0 $. Then at $ \theta=\hat{\theta} $,
            \begin{align*}
                  S(\hat{\theta})                    & \simeq S(\theta_0)-I(\theta_0)(\hat{\theta}-\theta_0) \\
                  I(\theta_0)(\hat{\theta}-\theta_0) & \simeq S(\theta_0)                                    \\
                  (\hat{\theta}-\theta_0)            & \simeq I^{-1}(\theta_0)S(\theta_0)                    \\
                  \hat{\theta}                       & \simeq \theta_0+I^{-1}(\theta_0)S(\theta_0)
            \end{align*}
      \item This suggests a revised guess for $ \hat{\theta} $ is:
            \[ \theta_1=\theta_0+I^{-1}(\theta_0)S(\theta_0) \]
\end{itemize}
\begin{Regular}{Newton Raphson Algorithm for finding the MLE}
      We wish to maximize the function $ \ell(\theta) $ by solving $ S(\theta)=0 $.
      \begin{itemize}
            \item Begin with an initial estimate $ \theta_0 $.
            \item Iteratively obtain estimates $ \theta_1,\theta_2,\theta_3,\ldots $ using:
                  \[ \theta_{i+1}=\theta_i+I^{-1}(\theta_i)S(\theta_i) \]
            \item Iteration should continue until $ \theta_{i+1}\simeq \theta_i $. (i.e., $ \abs{\theta_{i+1}-\theta_i} $ is within a specified tolerance).
            \item Then set $ \hat{\theta}=\theta_{i+1} $.
            \item To determine if it is a maxima of $ \ell(\theta) $, check that $ I(\hat{\theta})>0 $.
      \end{itemize}
\end{Regular}
\subsection*{Example: Newton Raphson for Cyclone Data}
% \begin{noindent}
<<>>=
# Input the Cyclone data, define the score and information function
y=c(6,5,4,6,6,3,12,7,4,2,6,7,4)
Score=function(theta,y){sum(y)/theta-length(y)}
Info=function(theta,y){sum(y)/(theta^2)}
#Run one-parameter Newton Raphson algorithm (track each iteration)
theta.old=0; theta.new=5
track=c(theta.new,Score(theta.new,y))
while((theta.new-theta.old)^2>10^(-3)){
            theta.old=theta.new
            theta.new=theta.old+Score(theta.old,y)/Info(theta.old,y)
            track=rbind(track,c(theta.new,Score(theta.new,y)))}
track
mean(y)
@
% \end{noindent}
\subsection*{Inference for Scalar Parameters}
\addcontentsline{toc}{subsection}{Inference for Scalar}
\begin{itemize}
      \item So far we have discussed estimation of $ \hat{\theta} $.
      \item Next, we want to conduct inference (carry out hypothesis tests and construct
            confidence intervals).
      \item Several techniques are available, all based to varying degrees on the likelihood
            function.
\end{itemize}
\begin{Regular}{Useful asymptotic distributional results}
      \begin{itemize}
            \item \textcolor{Blue}{(log) Likelihood ratio statistic}: $ -2\log[\big]{R(\theta)}=-2r(\theta)\sim \chi^2_{(1)} $.
            \item \textcolor{Blue}{Score statistic}: $ \bigl(S(\theta)\bigr)^2/I(\theta) \sim \chi^2_{(1)} $.
            \item \textcolor{Blue}{Wald statistic}: $ (\hat{\theta}-\theta)^2 I(\hat{\theta}) \sim \chi^2_{(1)} $.
      \end{itemize}
      (approximations improve as sample size increases)
\end{Regular}
\subsection*{Confidence Intervals}
\addcontentsline{toc}{subsubsection}{Confidence Intervals}
Suppose we want a $ \SI[parse-numbers = false]{100(1-\alpha)}{\percent} $ confidence interval for $ \theta $.
\begin{itemize}
      \item The \textcolor{Blue}{Likelihood ratio} (LR) based pivotal gives a confidence interval:
            \[ \Set*{\theta\given -2r(\theta)<\chi^2_{(1)}(1-\alpha)} \]
      \item The \textcolor{Blue}{Wald}-based pivotal gives an interval:
            \[ \Set*{\theta\given (\hat{\theta}-\theta)I(\hat{\theta})<\chi^2_{(1)}(1-\alpha)} \]
            where $ \chi^2_{(1)}(1-\alpha) $ is the upper $ \alpha $ percentage point of the $ \chi^2_{(1)} $ distribution.
      \item The \textcolor{Blue}{Wald}-based interval should actually be familiar to you:
      \item Recall if $ Z \sim \N{0,1} $ then $ Z^2 \sim \chi^2_{(1)} $.
      \item So we have:
            \begin{align*}
                  \Set*{\theta\given (\hat{\theta}-\theta)^2 I(\hat{\theta})<\chi^2_{(1)}(1-\alpha)}
                   & =\Set*{\theta\given \abs*{(\hat{\theta}-\theta)I^{1/2}(\hat{\theta})}<z_{(1-\alpha/2)}}                                     \\
                   & =\Set*{\theta\given \abs*{\hat{\theta}-\theta}<I^{-1/2}z_{(1-\alpha/2)}}                &  & \text{since }I(\hat{\theta})>0 \\
                   & =\hat{\theta}\pm z_{(1-\alpha/2)}I^{-1/2}(\hat{\theta})
            \end{align*}
      \item This is the ``standard'' normal based confidence interval with $ \se{\hat{\theta}}=I^{-1/2}(\hat{\theta}) $.
\end{itemize}
\subsection*{Example: LR Confidence Interval for Cyclone Data}
\textcolor{Blue}{Likelihood Ratio} based interval: $ \Set*{\theta\given -2r(\theta)<\chi^2_{(1)}(1-\alpha)} $.
\begin{itemize}
      \item For the Poisson distribution $ \hat{\theta}=\bar{y} $ so:
            \[ r(\theta)=\ell(\theta)-\ell(\hat{\theta})=n\bar{y}\log*{\frac{\theta}{\bar{y}}}-n(\theta-\bar{y}) \]
      \item To find the interval find the roots of $ -2r(\theta)-\chi^2_{(1)}(1-\alpha) $.
\end{itemize}
%\begin{noindent}
<<>>=
ybar=mean(y); n=length(y)
LRest = function(theta,ybar,n)
{
      -2*(n*ybar*log(theta/ybar)-n*(theta-ybar))-qchisq(.95,1)
}
uniroot(LRest,interval=c(3,ybar),ybar=ybar,n=n)$root
uniroot(LRest,interval=c(ybar,8),ybar=ybar,n=n)$root
@
%\end{noindent}
The likelihood ratio based \qty{95}{\percent} confidence interval is $ (4.36,6.92) $.
% \begin{noindent}
      <<echo=FALSE>>=
      y=c(6,5,4,6,6,3,12,7,4,2,6,7,4)
      n=length(y)
      theta=seq(3,8,by=0.001)
      theta.hat=mean(y)
      plot(theta,sum(y)*log(theta/theta.hat)-n*(theta-theta.hat), type="l", ylab = "log relative likelihood", xlab="", main = "LR Based Confidence Interval")
      abline(v=theta.hat, lty = 2)
      left=uniroot(LRest,interval=c(3,ybar),ybar=ybar,n=n)$root
      right=uniroot(LRest,interval=c(ybar,8),ybar=ybar,n=n)$root
      abline(v=left, col = "blue", lty = 3)
      abline(v=right, col = "blue", lty = 3)
      abline(h=sum(y)*log(right/theta.hat)-n*(right-theta.hat), col = "blue", lty = 2)
      @
% \end{noindent}
\subsection*{Example: Wald Confidence Interval for Cyclone Data}
\textcolor{Blue}{Wald} based interval: $ \Set*{\theta\given (\hat{\theta}-\theta)I(\hat{\theta})<\chi^2_{(1)}(1-\alpha)} $.
\begin{itemize}
      \item For the Poisson distribution $ \hat{\theta}=\bar{y} $ and
            \[ I(\hat{\theta})=\frac{1}{\hat{\theta}^2}\sum y_i=\frac{n\bar{y}}{\bar{y}^2} =\frac{n}{\bar{y}} \]
      \item So we solve:
            \begin{align*}
                  \hat{\theta}\pm 1.96\bigl(I(\hat{\theta})\bigr)^{-1/2}
                   & =\bar{y}\pm 1.96(n/\bar{y})^{-1/2} \\
                   & = 5.538462\pm 1.96(0.652714)       \\
                   & = (4.2591, 6.8178)
            \end{align*}
\end{itemize}
The likelihood ratio based \qty{95}{\percent} confidence interval is $(4.36, 6.92)$.

The Wald based \qty{95}{\percent} confidence interval is $(4.26, 6.82)$.
% \begin{noindent}
      <<echo=FALSE>>=
      y=c(6,5,4,6,6,3,12,7,4,2,6,7,4)
      n=length(y)
      theta=seq(3,8,by=0.001)
      theta.hat=mean(y)
      plot(theta,sum(y)*log(theta/theta.hat)-n*(theta-theta.hat), type="l", ylab = "log relative likelihood", xlab="", main = "Wald Based Confidence Interval")
      abline(v=theta.hat, lty = 2)
      left=uniroot(LRest,interval=c(3,ybar),ybar=ybar,n=n)$root
      right=uniroot(LRest,interval=c(ybar,8),ybar=ybar,n=n)$root
      abline(v=left, col = "blue", lty = 3)
      abline(v=right, col = "blue", lty = 3)
      abline(h=sum(y)*log(right/theta.hat)-n*(right-theta.hat), col = "blue", lty = 2)
      left.2=ybar-qnorm(0.975)*1/sqrt((n/ybar))
      right.2=ybar+qnorm(0.975)*1/sqrt((n/ybar))
      abline(v=left.2, col = "red", lty = 3)
      abline(v=right.2, col = "red", lty = 3)
      @
% \end{noindent}
\subsection*{Testing Hypotheses}
\begin{center}
      $ \HN $: $ \theta=\theta_0 $ vs. $ \HA $: $ \theta\ne \theta_0 $.
\end{center}
\begin{itemize}
      \item \textcolor{Blue}{Likelihood ratio (LR) test}:
            \[ p=\Prob*{\chi^2_{(1)}>-2r(\theta_0)} \]
      \item \textcolor{Blue}{Score test}:
            \[ p=\Prob*{\chi^2_{(1)}>\bigl(S(\theta_0)\bigr)^2/I(\theta_0)} \]
      \item \textcolor{Blue}{Wald test}:
            \[ p=\Prob*{\chi^2_{(1)}>(\hat{\theta}-\theta_0)^2 I(\hat{\theta})} \]
            or
            \[ p=\Prob*{\abs{Z}>\abs*{\theta-\theta_0}\sqrt{I(\hat{\theta})}} \]
\end{itemize}
\subsection*{Example: Hypothesis Tests for Cyclone Data}
Suppose we wish to test whether there were an average of 5 cyclones per year
\begin{center}
      $ \HN $: $ \theta=5 $ vs. $ \HA $: $ \theta\ne 5 $.
\end{center}
\begin{itemize}
      \item \textcolor{Blue}{Likelihood Ratio} based test:
            \[ r(\theta_0=5)=n\bar{y}\log*{\frac{5}{\bar{y}}}-n(5-\bar{y})=-0.3641 \]
            The $ p $-value for this test is:
            \[ p=\Prob*{\chi^2_{(1)}>-2r(5)}=\Prob*{\chi^2_{(1)}>0.7282}=0.3934 \]
            Therefore we \emph{do not reject} $ \HN $.
\end{itemize}
\subsection*{Notes on Asymptotic Inference}
\begin{itemize}
      \item Asymptotic results: approximation improves as sample size increases.
      \item Results are exact for a Normal linear model if $ \theta $ is the mean parameter and $ \sigma^2 $ is
            known.
      \item \textcolor{Blue}{LR approach}:
            \begin{itemize}
                  \item Need to evaluate (log) likelihood at two locations.
                  \item Not always a closed from solution for a CI\@.
                  \item Usually the best approach.
            \end{itemize}
      \item \textcolor{Blue}{Score approach}:
            \begin{itemize}
                  \item Usually the least powerful test.
                  \item Don't actually need to find MLE to use.
            \end{itemize}
      \item \textcolor{Blue}{Wald's approach}:
            \begin{itemize}
                  \item Always get a closed form solution for a CI\@.
                  \item May not behave well for skewed likelihoods (transform?).
            \end{itemize}
      \item All three are asymptotically equivalent!
\end{itemize}
\subsection*{Likelihood Methods for Parameter Vectors (A3)}
\addcontentsline{toc}{subsection}{Likelihood Methods for Vector}
Suppose $ \Vector{\theta}\in \Omega $ is a continuous $ p\times 1 $ parameter vector indexing a probability density
or mass function $ f(\Vector{y}\mid\Vector{\theta}) $.
\begin{itemize}
      \item $ \mathcal{L}( \Vector{\theta}) $ is the \textcolor{Blue}{Likelihood function}.
      \item $ \ell( \Vector{\theta})=\log[\big]{\mathcal{L}(\Vector{\theta})} $ is the \textcolor{Blue}{log-likelihood function}.
      \item $ \Vector{S}(\Vector{\theta})=\pdv{\ell( \Vector{\theta})}{ \Vector{\theta}} $ is the $ p\times 1 $ \textcolor{Blue}{Score vector}.
      \item $ \Matrix{I}(\Vector{\theta})=-\pdv{\ell(\Vector{\theta})}{ \Vector{\theta}^\top,\Vector{\theta}} $ is the $ p\times p $ \textcolor{Blue}{Information matrix}.
      \item $ R(\Vector{\theta})=\mathcal{L}(\Vector{\theta})/\mathcal{L}(\hat{ \Vector{\theta}}) $ is the \textcolor{Blue}{Relative likelihood function}.
      \item $ r(\Vector{\theta})=\log[\big]{\mathcal{L}(\Vector{\theta})/\mathcal{L}(\hat{\Vector{\theta}})}=\ell(\Vector{\theta})-\ell(\hat{\Vector{\theta}}) $ is the
            \textcolor{Blue}{log relative likelihood function}.
\end{itemize}
\begin{itemize}
      \item The Newton Raphson algorithm applies as before, but with vectors and matrices
            as follows:
            \[ \Vector{\theta}_{i+1}=\Vector{\theta}_i+\bigl(\Matrix{I}(\Vector{\theta})\bigr)^{-1}\Vector{S}(\Vector{\theta}) \]
      \item Again, we apply iteratively until we obtain convergence, but now check to
            see if $ \Matrix{I}(\Vector{\theta}) $ is a positive definite matrix.
      \item Analogs to the LR, Score and Wald results apply based on partitioning the
            Information matrix by $ \Vector{\theta}=(\Vector{\alpha},\Vector{\beta})^\top $,
            where $ \Vector{\alpha} $ is a $ p\times 1 $ vector of nuisance parameters and $ \Vector{\beta} $ is a $ q\times 1 $ vector of parameters of interest:
            \[ \Matrix{I}=\Matrix{I}(\Vector{\alpha},\Vector{\beta})=\begin{pmatrix}
                        \Matrix{I}_{\Vector{\alpha}\Vector{\alpha}}(\Vector{\alpha},\Vector{\beta}) & \Matrix{I}_{\Vector{\alpha}\Vector{\beta}}(\Vector{\alpha},\Vector{\beta}) \\
                        \Matrix{I}_{\Vector{\beta}\Vector{\alpha}}(\Vector{\alpha},\Vector{\beta})  & \Matrix{I}_{\Vector{\beta}\Vector{\beta}}(\Vector{\alpha},\Vector{\beta})
                  \end{pmatrix} \]
            where
            $ \Matrix{I}_{\Vector{\alpha}\Vector{\alpha}}(\Vector{\alpha},\Vector{\beta})=-\pdv{\ell}{\Vector{\alpha},\Vector{\alpha}^\top} $ is $ p\times p $,
            $ \Matrix{I}_{\Vector{\alpha}\Vector{\beta}}(\Vector{\alpha},\Vector{\beta})=-\pdv{\ell}{\Vector{\alpha},\Vector{\beta}^\top} $ is $ p\times q $,
            $ \Matrix{I}_{\Vector{\beta}\Vector{\alpha}}(\Vector{\alpha},\Vector{\beta})=-\pdv{\ell}{\Vector{\beta},\Vector{\alpha}^\top} $ is $ q\times p $, and
            $ \Matrix{I}_{\Vector{\beta}\Vector{\beta}}(\Vector{\alpha},\Vector{\beta})=-\pdv{\ell}{\Vector{\beta},\Vector{\beta}^\top} $ is $ q\times q $.
\end{itemize}
