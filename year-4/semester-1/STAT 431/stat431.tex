\documentclass{article}\usepackage[]{graphicx}\usepackage[svgnames]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[svgnames]{xcolor}
\usepackage[british]{babel}
\usepackage[protrusion,expansion,tracking,kerning,babel,final]{microtype}
\usepackage[margin=1in]{geometry}
\usepackage[pdfversion=1.7]{hyperref}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{nicematrix}
\usepackage{derivative}
\usepackage{etoolbox}
\usepackage{siunitx}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[scaled=.98]{XCharter}
\usepackage[scaled=1.04,varqu,varl]{inconsolata}% inconsolata typewriter
\usepackage{amssymb}
\makeatletter
\@namedef{T1/zi4/m/it}{<->ssub*XCharterx/m/it}
\makeatother

\usepackage{bm}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

% Functions
\providecommand\given{} % just to make sure it exists
\DeclarePairedDelimiterXPP{\E}[1]{\operatorname{\mathbb{E}}}[]{}{%
    \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP{\V}[1]{\operatorname{\textsf{V}}}(){}{%
    \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP{\Var}[1]{\operatorname{\textsf{Var}}}(){}{%
    \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP{\Cov}[1]{\operatorname{\textsf{Cov}}}(){}{%
    \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP\Prob[1]{\operatorname{\mathbb{P}}}(){}{%
    \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP\Ind[1]{\operatorname{\mathbb{I}}}\{\}{}{%
    \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP{\se}[1]{\operatorname{\textsf{se}}}(){}{%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP{\estse}[1]{\widehat{\operatorname{\textsf{se}}}}(){}{%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP{\estV}[1]{\widehat{\operatorname{\textsf{V}}}}(){}{
    \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\DeclarePairedDelimiterXPP{\estVar}[1]{\widehat{\operatorname{\textsf{Var}}}}(){}{
    \renewcommand\given{\nonscript\:\delimsize\vert\nonscript\:\mathopen{}}%
    \ifblank{#1}{\:\cdot\:}%
    #1}%
\let\exp\relax%
\let\log\relax%
\let\ln\relax%
\DeclarePairedDelimiterXPP{\exp}[1]{\operatorname{\textsf{exp}}}\{\}{}{#1}%
\DeclarePairedDelimiterXPP{\log}[1]{\operatorname{\textsf{log}}}(){}{#1}%
\DeclarePairedDelimiterXPP{\ln}[1]{\operatorname{\textsf{ln}}}(){}{#1}%
\DeclarePairedDelimiterXPP{\diag}[1]{\operatorname{\textsf{diag}}}(){}{#1}%
\DeclarePairedDelimiterXPP{\sign}[1]{\operatorname{\textsf{sign}}}(){}{#1}%

\DeclarePairedDelimiterXPP{\expit}[1]{\operatorname{\textsf{expit}}}(){}{#1}%
\DeclarePairedDelimiterXPP{\logit}[1]{\operatorname{\textsf{logit}}}(){}{#1}%
\newcommand{\HN}{\textsl{H}_{\textsl{0}}}%
\newcommand{\HA}{\textsl{H}_{\textsl{A}}}%

% Distributions
\DeclarePairedDelimiterXPP{\N}[1]{\mathcal{N}}(){}{#1}%
\DeclarePairedDelimiterXPP{\POI}[1]{\text{POI}}(){}{#1}%
\DeclarePairedDelimiterXPP{\BIN}[1]{\text{BIN}}(){}{#1}%
\DeclarePairedDelimiterXPP{\BERN}[1]{\text{BERN}}(){}{#1}%
\DeclarePairedDelimiterXPP{\MVN}[1]{\text{MVN}}(){}{#1}%

\newcommand{\iid}{\overset{\text{iid}}{\sim}}%
\newcommand{\ind}{\overset{\text{ind}}{\sim}}%
\newcommand{\OR}{\text{OR}}%
\newcommand{\RR}{\text{RR}}%

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
% can be useful to refer to this outside \Set
\newcommand\SetSymbol[1][]{%
    \nonscript\:#1\vert{}
    \allowbreak\nonscript\:
    \mathopen{}}
\DeclarePairedDelimiterX\Set[1]\{\}{%
    \renewcommand\given{:}
    #1
}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}

% Table of Contents
\hypersetup{colorlinks,linkcolor=[rgb]{0,0.5,1}}%

\title{%
    {\LARGE Generalized Linear Models}\\%
    {\large STAT 431/STAT 831}\\%
    {\normalsize Spring 2022 (idk)}%
}%
\author{%
    \LaTeX{}er: \emph{Cameron Roopnarine}\\%
    Instructor: \emph{Leilei Zeng}%
}%
\date{\today}%

\providecommand{\RandomVector}[1]{\bm{#1}}% general vectors in bold italic
\providecommand{\Vector}[1]{\bm{#1}}% general vectors in bold italic
\providecommand{\Matrix}[1]{\bm{#1}}
\providecommand{\MatrixCal}[1]{\bm{\mathcal{#1}}}
\providecommand{\Field}[1]{\bm{#1}}

\usepackage{stackengine}
\usepackage[british]{isodate}
\newcommand{\makeheading}[2]%
{%
\begin{center}%
    \makebox[\linewidth]{\raisebox{-.5ex}[0cm][0cm]{\stackanchor{\textcolor{Gray}{\textsc{#1}}}{\scriptsize\itshape\printyearoff#2}\;}\color{Crimson!50}\hrulefill}%
\end{center}%
}%

\usepackage[breakable]{tcolorbox}
\tcbset{
    regular/.style={
        boxrule=0pt,
        breakable,
        sharp corners
    }
}

\newtcolorbox{Example}[1]{regular,colframe=Green!20!white,colback=Green!10!white,coltitle=Green,title={#1}}%
\newtcolorbox{Regular}[1]{regular,colframe=Navy!15!white,colback=Navy!5!white,coltitle=Navy,title={#1}}%
\newtcolorbox{Result}[1]{regular,colframe=Red!15!white,colback=Red!5!white,coltitle=Red,title={#1}}%

\hypersetup{colorlinks=true,%
linkcolor=[rgb]{0,0.5,1},%
pdftitle={Generalized Linear Models and their Applications (STAT 431/STAT 831)},%
pdfauthor={Cameron Roopnarine, Leilei Zeng},%
pdfsubject={Statistics},%
pdfkeywords={University of Waterloo, Fall 2021 (1219)}}%

\title{%
\LARGE Generalized Linear Models and their Applications\\%
\large STAT 431/STAT 831\thanks{STAT 431 $ \equiv $ STAT 831}\\%
\normalsize Fall 2021 (1219)\thanks{Online Course}}%
\author{Cameron Roopnarine\thanks{\LaTeX{}er}\and Leilei Zeng\thanks{Instructor}}%
\date{\today}%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}


\maketitle
\newpage
\tableofcontents
\newpage
% Week 1

\makeheading{Week 1}{\daterange{2021-09-08}{2021-09-10}}
\section*{Topic 1a: Review of Linear Regression}
\addcontentsline{toc}{section}{Topic 1a: Review of Linear Regression}

\subsection*{Example: low birthweight infants study\footnote{Principles of Biostatistics 2nd Edition by Marcello Pagano, Kimberlee Gauvreau.}}
A study was conducted at two teaching
hospitals in Boston, Massachusetts,
where the head circumference, gestational age and some other variables
are recorded for 100 low birth weight
infants.

Question: what is the relationship between \textcolor{Blue}{\emph{gestational age}} \& \textcolor{Blue}{head circumference}?
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-14-1} 

}


\end{knitrout}
% \end{noindent}
We wish to model the relationship between \emph{gestational age} and \emph{head
      circumference} using a straight line!
% \begin{noidnent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-15-1} 

}


\end{knitrout}
% \end{noidnent}

\subsection*{The Model Fitting Process}
\begin{enumerate}[label=\color{Blue}\protect\circled{\arabic*}]
      \item \textcolor{Red}{Model Specification}: select a probability distribution for the response
            variable and a linear equation linking the response to the explanatory
            variables.
      \item \textcolor{Red}{Estimation}: finding the equation (the parameters of the model).
      \item \textcolor{Red}{Model checking}: how well does the model fit the data?
      \item \textcolor{Red}{Inference}: interpret the fitted model, calculate confidence intervals,
            conduct hypothesis tests.
\end{enumerate}

\subsection*{\circled{1} Model Specification}
\begin{Regular}{Notation}
      For each subject $ i=1,\ldots,n $ we have:
      \begin{itemize}
            \item $ Y_i = $ random variable representing the response, and
            \item $ \Vector{x}_i =(1,x_{i1},\ldots,x_{ip})^\top $, a vector of explanatory variables.
      \end{itemize}
\end{Regular}
\begin{Regular}{Specification for Multiple Linear Regression}
      \begin{itemize}
            \item Linear regression equation:
                  \[ Y_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip}+\varepsilon_i\text{ where }\varepsilon_i \iid\N{0,\sigma^2}. \]
            \item Equivalently, $Y_i$'s are independent $ \N{\mu_i,\sigma^2} $ random variables or
                  \[ \mu_i=\E{Y_i}=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip}. \]
            \item For convenience, we often write linear regression models in matrix form as
                  \[ \RandomVector{Y}=\Matrix{X}\Vector{\beta}+\RandomVector{\varepsilon}, \]
                  where
                  \[ \RandomVector{Y}=\begin{bmatrix}
                              Y_1    \\
                              Y_2    \\
                              \vdots \\
                              Y_n
                        \end{bmatrix},\quad
                        \Matrix{X}=\begin{bmatrix}
                              1      & x_{11} & \cdots & x_{1p} \\
                              1      & x_{21} & \cdots & x_{2p} \\
                              \vdots & \vdots & \ddots & \vdots \\
                              1      & x_{n1} & \cdots & x_{np}
                        \end{bmatrix},\quad
                        \Vector{\beta}=\begin{bmatrix}
                              \beta_0 \\
                              \beta_1 \\
                              \vdots  \\
                              \beta_p
                        \end{bmatrix},\quad
                        \RandomVector{\varepsilon}=\begin{bmatrix}
                              \varepsilon_1 \\
                              \varepsilon_2 \\
                              \vdots        \\
                              \varepsilon_n
                        \end{bmatrix} \]
                  and
                  \[ \RandomVector{\varepsilon}\sim \MVN{\Vector{0},\sigma^2\Matrix{I}}. \]
      \end{itemize}
\end{Regular}
\subsection*{\circled{2} Estimation}
\begin{Regular}{Least Squares Method}
      We wish to minimize a loss function:
      \begin{align*}
            S(\Vector{\beta})
             & =\sum_{i=1}^{n} (y_i-\hat{y}_i)^2                                                             \\
             & =\sum_{i=1}^{n} \bigl(y_i-(\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip})\bigr)^2               \\
             & =(\RandomVector{Y}-\Matrix{X}\Vector{\beta})^\top(\RandomVector{Y}-\Matrix{X}\Vector{\beta}).
      \end{align*}
      The least squares estimators (LSE) are the solutions to the equations:
      \[ \pdv{S}{\Vector{\beta}}=\pdv*{(\RandomVector{Y}-\Matrix{X}\Vector{\beta})^\top(\RandomVector{Y}-\Matrix{X}\Vector{\beta})}{\Vector{\beta}}=0. \]
\end{Regular}
\begin{Regular}{Maximum Likelihood Method}
      The probability density function for $ Y_i $ is:
      \[ f(y_i)=\frac{1}{\sqrt{2\mathrm{\pi}\sigma^2}}\exp*{-\frac{1}{2\sigma^2}\bigl(y_i-(\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip})\bigr)^2 }.  \]
      The log-likelihood function is therefore:
      \begin{align*}
            \ell(\Vector{\beta},\sigma^2)
             & =\log[\bigg]{\prod_{i=1}^n f(y_i)}                                                                                                                         \\
             & =\sum_{i=1}^{n} \biggl(-\frac{1}{2} \log{2\mathrm{\pi}\sigma^2}-\frac{1}{2\sigma^2}\bigl(y_i-(\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip})\bigr)^2 \biggr) \\
             & =-\frac{n}{2} \log{2\sigma^2}-\frac{1}{2\sigma^2} (\RandomVector{Y}-\Matrix{X}\Vector{\beta})^\top(\RandomVector{Y}-\Matrix{X}\Vector{\beta}).
      \end{align*}
      The maximum likelihood estimators (MLE) of $ \Vector{\beta} $ are obtained by solving:
      \[ \pdv{\ell}{\Vector{\beta}}=\pdv*{\biggl[-\frac{1}{2\sigma^2}(\RandomVector{Y}-\Matrix{X}\Vector{\beta})^\top(\RandomVector{Y}-\Matrix{X}\Vector{\beta})\biggr]}{\Vector{\beta}}=0. \]
\end{Regular}
\begin{itemize}
      \item \textcolor{Red}{Parameter Estimates}: For linear regression LSE and MLE of $ \Vector{\beta} $ are the same
            \[ \hat{\Vector{\beta}}=(\Matrix{X}^\top\Matrix{X})^{-1}\Matrix{X}^\top\RandomVector{Y}. \]
      \item \textcolor{Red}{Fitted values}: $ \hat{\RandomVector{Y}}=\Matrix{X}\hat{\Vector{\beta}} $.
      \item \textcolor{Red}{Residuals}: $ \hat{r}_i=(y_i-\hat{y}_i) $.
      \item \textcolor{Red}{Variance estimates}:
            \begin{itemize}
                  \item An unbiased estimate of $ \sigma^2 $ is:
                        \[ \hat{\sigma}^2=\frac{1}{n-(p+1)} \sum_{i=1}^{n} \hat{r}_i^2. \]
                  \item An estimate of the variance of $ \hat{\Vector{\beta}} $ is:
                        \[ \estV{\hat{\Vector{\beta}}}=\hat{\sigma}^2(\Matrix{X}^\top\Matrix{X})^{-1}. \]
            \end{itemize}
\end{itemize}
\subsubsection*{Low Birthweight Infant Data Example}
\begin{itemize}
      \item For $ n=100 $ infants, we have observed $ Y_i= $ head circumference and $ x_i= $ gestational age for baby $ i $, $ i=1,\ldots,100 $.
      \item Consider a simple linear regression model:
            \[ Y_i=\beta_0+\beta_1x_{i}+\varepsilon_i. \]
      \item We can fit the model and obtain LSE/MSE using the \texttt{lm()} function in R.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{lowbwt} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"lowbwt.txt"}\hlstd{,} \hlkwc{header} \hlstd{= T)}
\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(headcirc} \hlopt{~} \hlstd{gestage,} \hlkwc{data} \hlstd{= lowbwt)}
\hlkwd{summary}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}

Call:
lm(formula = headcirc ~ gestage, data = lowbwt)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.5358 -0.8760 -0.1458  0.9041  6.9041 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  3.91426    1.82915    2.14   0.0348 *  
gestage      0.78005    0.06307   12.37   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.59 on 98 degrees of freedom
Multiple R-squared:  0.6095,	Adjusted R-squared:  0.6055 
F-statistic: 152.9 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
      \item What is the interpretation of regression parameters $ \beta_0 $ and $ \beta_1 $?
            \begin{itemize}
                  \item $ \beta_0 $ (intercept): expected \texttt{headcirc} for a baby of a gestational age zero ($ x=0 $).
                  \item $ \beta_1 $ (slope): expected change in \texttt{headcirc} associated with a one unit increase in gestational age.
            \end{itemize}
\end{itemize}

\subsection*{\circled{3} Model Checking}
\textcolor{Red}{Standardized Residuals}:
\[ d_i=\frac{r_i}{\sqrt{\hat{\sigma}^2(1-h_{ii})}},  \]
where $ h_{ii} $ is the $ (i,i) $ element of $ \Matrix{H}=(\Matrix{X}^\top\Matrix{X})^{-1}\Matrix{X}^\top $.
By asymptotic theory, if the model provides a good fit to the data then we
should expect that:
\[ d_i\iid \N{0,1}. \]
We visually check this by examining residual plots such as:
\begin{itemize}
      \item Standardized residuals versus the fitted values.
      \item Standardized residuals versus the explanatory variable(s).
      \item Normal probability plot (QQ plot) of the standardized residuals.
\end{itemize}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-17-1} 

}




{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-17-2} 

}


\end{knitrout}
% \end{noindent}

\subsection*{\circled{4} Inference}
\begin{itemize}
      \item Under suitable assumptions, the fitted regression parameters are asymptotically
            normally distributed:
            \begin{align*}
                  \hat{\Vector{\beta}} & \sim \MVN[\big]{\Vector{\beta},\sigma^2(\Matrix{X}^\top \Matrix{X})^{-1}},                                        \\
                  \hat{\beta}_j        & \sim \N{\beta_j,\sigma^2v_{jj}},\qquad\text{where $v_{jj}=\bigl[(\Matrix{X}^\top\Matrix{X})^{-1}\bigr]_{(j,j)}$}.
            \end{align*}
      \item Since $ \sigma^2 $ is generally unknown, we replace it with the unbiased estimate $ \hat{\sigma}^2 $, and obtain $ \se{\hat{\beta}_j}=\sqrt{\hat{\sigma}^2v_{jj}} $.
      \item The inference is then based on the $t$-distribution result:
            \[ \frac{\hat{\beta}_j-\beta_j}{\se{\hat{\beta}_j}}\sim t_{n-p-1}.  \]
\end{itemize}
\subsubsection*{Low Birthweight Infant Data Example}
\begin{itemize}
      \item Is there a significant (linear) relationship between head circumference and
            gestational age?

            We wish to test $ \HN $: $ \beta_1=0 $ vs $ \HA $: $ \beta_1\ne 0 $.
            \[ t=\frac{\hat{\beta}_1-(0)}{\se{\hat{\beta}_1}}\sim t_{98}, \]
            if $ \HN $ is true, and we reject $ \HN $ if $ \abs{t}>t_{98,0.975}=1.985 $.
            Here we have $ t=0.78/0.063=12.37\gg 1.985 $, so we reject $ \HN $.
      \item What is the \qty{95}{\percent} confidence interval for the expected increase in head
            circumference when the gestational age of a baby increases by 1 week?

            A \qty{95}{\percent} CI for $ \beta_1 $:
            \[ \hat{\beta}_1\pm t_{98,0.975}\se{\hat{\beta}_1}=0.78\pm 1.985(0.063)=(0.665,0.905). \]
\end{itemize}

\subsection*{Linear models with multiple predictors}
\subsubsection*{Low Birthweight Infant Data Example}
\begin{itemize}
      \item \emph{Toxemia}, a pregnancy complication characterized by high blood pressure
            and signs of damage to liver and kidneys, may also have an impact on the
            development of babies.
            %\begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-18-1} 

}


\end{knitrout}
          %\end{noindent}
      \item Does \emph{toxemia}, after adjustment for gestational age, also affect the head
            circumference?
            \[ Y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\varepsilon_i. \]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(headcirc} \hlopt{~} \hlstd{gestage} \hlopt{+} \hlkwd{factor}\hlstd{(toxemia),} \hlkwc{data} \hlstd{= lowbwt)}
\hlkwd{summary}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}

Call:
lm(formula = headcirc ~ gestage + factor(toxemia), data = lowbwt)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.8427 -0.8427 -0.0525  0.8109  6.4092 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       1.49558    1.86799   0.801  0.42530    
gestage           0.87404    0.06561  13.322  < 2e-16 ***
factor(toxemia)1 -1.41233    0.40615  -3.477  0.00076 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.507 on 97 degrees of freedom
Multiple R-squared:  0.6528,	Adjusted R-squared:  0.6456 
F-statistic: 91.18 on 2 and 97 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
            What is the interpretation of $ \beta_2 $?

            $ \hat{\beta}_3=-1.41233 $. After adjustment of gestational age, the babies whose mothers had toxemia have smaller (by \qty{1.41}{\cm}) than
            those whose mothers did not. This difference is significant (test $ \HN $: $ \beta_2=0 $, $ p\text{-value}=0.0076<0.05$).
      \item Is the rate of increase of head circumference with gestational age the same
            for infants whose mothers with toxemia as those whose mother without it?
            \[ Y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i1}x_{i2}+\varepsilon_i. \]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(headcirc} \hlopt{~} \hlstd{gestage} \hlopt{*} \hlkwd{factor}\hlstd{(toxemia),} \hlkwc{data} \hlstd{= lowbwt)}
\hlkwd{summary}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}

Call:
lm(formula = headcirc ~ gestage * factor(toxemia), data = lowbwt)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.8366 -0.8366 -0.0928  0.7910  6.4341 

Coefficients:
                         Estimate Std. Error t value Pr(>|t|)    
(Intercept)               1.76291    2.10225   0.839    0.404    
gestage                   0.86461    0.07390  11.700   <2e-16 ***
factor(toxemia)1         -2.81503    4.98515  -0.565    0.574    
gestage:factor(toxemia)1  0.04617    0.16352   0.282    0.778    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.515 on 96 degrees of freedom
Multiple R-squared:  0.6531,	Adjusted R-squared:  0.6422 
F-statistic: 60.23 on 3 and 96 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
            What is the interpretation of $ \beta_3 $?

            $ \beta_3 $ is the differences in slopes between the two groups (\texttt{toxemia=1} vs \texttt{toxemia=0}).
            We want to test $ \HN $: $ \beta_3=0 $, $ t=0.282 $, $ p\text{-value}=0.778>0.05 $. No evidence to reject $ \HN $.
\end{itemize}

\subsection*{Limitations of Linear Regression}
Linear regression models can be very useful but may not be appropriate to use
when response $ Y $ is not continuous and can not be assumed to be normally
distributed, e.g.,
\begin{itemize}
      \item Binary data ($ Y=0 $ or $ Y=1 $),
      \item Count data ($ Y=0,1,2,3,\ldots $).
\end{itemize}
\textcolor{Red}{Generalized Linear Models (GLM)} extend the linear regression framework to
address the above issue.
\begin{itemize}
      \item Suitable for continuous and discrete data.
      \item Normal/Gaussian linear regression is a special case of GLM.
      \item Inference based on maximum likelihood methods (review next class --- 431
            Appendix, Stat 330 notes).
\end{itemize}
% Week 2

\makeheading{Week 2}{\daterange{2021-09-13}{2021-09-17}}
\section*{Topic 1b: Review of Likelihood Methods}
\addcontentsline{toc}{section}{Topic 1b: Review of Likelihood Methods}
\subsection*{Distributions with a Single Parameter}
\begin{Regular}{Setup}
      \begin{itemize}
            \item Suppose $ Y $ is a random variable with probability density (or mass) function
                  $ f(y;\theta) $, where $ \theta\in\Omega $ is a continuous parameter.
            \item The true value of $ \theta $ is unknown.
            \item We wish to make inferences about $ \theta $ (i.e., we may want to estimate $ \theta $, calculate
                  a \qty{95}{\percent} CI or carry out tests of hypotheses regarding $ \theta $).
      \end{itemize}
\end{Regular}
\subsection*{Likelihood Function}
\begin{itemize}
      \item The \textcolor{Red}{Likelihood function} is any function which is proportional to the probability
            of observing the data one actually obtained, i.e.,
            \[ L(\theta;y)=cf(y;\theta)=c\Prob{Y=y;\theta}, \]
            where $ c $ is a \emph{proportionality constant} that does not depend on $ \theta $.
      \item $ L(\theta;y) $ contains all the information regarding $ \theta $ from the data.
      \item $ L(\theta;y) $ ranks the various parameter values in terms of their consistency
            with the data.
      \item Since $ L(\theta;y) $ is defined in terms of the random variable $ y $, it is itself a
            random variable.
\end{itemize}
\subsection*{Maximum Likelihood Estimator}
\begin{itemize}
      \item For the purposes of estimation we typically want to find $ \theta $ value that makes the
            observed data the most likely (hence the term \textcolor{Red}{maximum likelihood}).
      \item The \textcolor{Red}{maximum likelihood estimator (MLE)} of $ \theta $ is
            \[ \hat{\theta}=\argmax_\theta L(\theta;y). \]
      \item Estimation becomes a simple optimization problem!
      \item It is often easier to work with the logarithm of the likelihood function, i.e., the
            \textcolor{Red}{log-likelihood function}
            \[ \ell(\theta;y)=\log[\big]{L(\theta;y)}. \]
      \item Equivalently, since the $ \log{\:\cdot\:} $ function is monotonic, the value of $ \theta $ that maximizes $ L(\theta;y) $ also
            maximizes the log-likelihood $ \ell(\theta;y) $.
      \item For simplicity, we drop the $ y $ and use $ L(\theta)=L(\theta;y) $ and $ \ell(\theta)=\ell(\theta;y) $.
\end{itemize}

\subsection*{A List of Important Functions}
\begin{itemize}
      \item \textcolor{Red}{Log-likelihood function}: $ \ell(\theta)=\log[\big]{L(\theta)} $.
      \item \textcolor{Red}{Score function}: $ S(\theta)=\pdv{\ell(\theta)}{\theta}=\ell^\prime(\theta)$.
      \item \textcolor{Red}{Information function}: $ I(\theta)=-\pdv[order=2]{\ell(\theta)}{\theta}=-\ell^{\prime\prime}(\theta) $.
      \item \textcolor{Red}{Fisher information function}: $ \mathcal{I}(\theta)=\E[\big]{I(\theta)} $.
      \item \textcolor{Red}{Relative likelihood function}: $ R(\theta)=L(\theta)/L(\hat{\theta}) $, $ 0\le R(\theta)\le 1 $.
      \item \textcolor{Red}{Log relative likelihood function}: $ r(\theta)=\log[\big]{L(\theta)/L(\hat{\theta})}=\ell(\theta)-\ell(\hat{\theta}) $, $ r(\theta)\le 0 $.
\end{itemize}
\subsection*{Maximum Likelihood Estimation}
\begin{itemize}
      \item Want $ \theta $ that maximizes $ \ell(\theta) $, or equivalently solves $ S(\theta)=0 $.
      \item Sometimes $ S(\theta)=0 $ can be solved explicitly (easy in this case), but often we must solve iteratively.
      \item Check that the solution corresponds to a maxima of $ \ell(\theta) $ by verifying the value of the second derivative at $ \hat{\theta} $ is negative, or
            \[ I(\hat{\theta})=-\ell^{\prime\prime}(\hat{\theta})>0. \]
      \item \textcolor{Red}{Invariance property of MLEs}: if $ g(\theta) $ is any function of the parameter $ \theta $, then the MLE of $ g(\theta) $ is $ g(\hat{\theta}) $.
            \begin{Example}{}
                  If $ \hat{\theta} $ is the MLE of $ \theta $, then $ \mathrm{e}^{\hat{\theta}} $ is the MLE of $ \mathrm{e}^{\theta} $.
            \end{Example}
\end{itemize}
\subsection*{Example: Binomial Distribution}
\begin{Example}{Example: Binomial Distribution}
      \begin{itemize}
            \item A study was conducted to examine the risk for hormone use in healthy
                  postmenopausal women.
            \item Suppose a group of $ n $ women received a combined hormone therapy, and were
                  monitored for the development of breast cancer during 8.5 years follow-up.
            \item Let
                  \[ Y_i=\begin{cases*}
                              1, & if woman $ i $ developed breast cancer, \\
                              0, & otherwise,
                        \end{cases*} \]
                  for $ i=1,\ldots,n $.
            \item Suppose $ Y_i \iid\BERN{\pi} $ where $ \pi=\Prob{Y_i=1} $, then the total number of woman developed breast cancer is:
                  \[ Y=\sum_{i=1}^{n} Y_i \sim \BIN{n,\pi}. \]
            \item We wish to find the MLE of unknown parameter $ \pi $ (probability of cancer).
      \end{itemize}
\end{Example}
\begin{itemize}
      \item \textcolor{Red}{Likelihood function}:
            \[ L(\pi;y)=c\Prob{Y=y;\pi}=\pi^y(1-\pi)^{n-y}, \]
            where we take $ c=1/\binom{n}{y} $ to simplify the likelihood.
      \item \textcolor{Red}{Log-likelihood function}:
            \[ \ell(\pi)=y\log{\pi}+(n-y)\log{1-\pi}. \]
      \item \textcolor{Red}{Score function}:
            \[ S(\pi)=\frac{y}{\pi}-\frac{n-y}{1-\pi}.  \]
      \item \textcolor{Red}{Maximum Likelihood Estimator}:
            \[ S(\pi)=0\implies \hat{\pi}=\frac{\sum_{i=1}^{n} y_i}{n}=\bar{y}. \]
      \item Second derivative test using \textcolor{Red}{information function}:
            \[ I(\pi)=-\ell^{\prime\prime}(\pi)=\frac{y}{\pi^2}+\frac{n-y}{(1-\pi)^2}>0\; \forall \pi\in(0,1). \]
            Confirms that $ \hat{\pi}=\bar{y} $ is the MLE\@.
\end{itemize}
\begin{Example}{Example: Hormone Therapy Data}
      \begin{itemize}
            \item A group of $ n=8506 $ postmenopausal women aged 50-79 received EPT and $ Y=166 $
                  developed invasive breast cancer during the follow-up.
            \item Assume $ Y \sim \BIN{n,\pi} $ with unknown parameter $ \pi $.
            \item The \textcolor{Red}{maximum likelihood estimate} of $ \pi $ is:
                  \[ \hat{\pi}=\bar{y}=\frac{y}{n} =\frac{166}{8506}=0.0195. \]
                  Therefore, the probability of breast cancer is estimated to be about \qty{2}{\percent}.
      \end{itemize}
\end{Example}
\subsection*{Example: Poisson Distribution}
Suppose $ y_1,\ldots,y_n $ is an iid sample from a Poisson distribution with probability mass function:
\[ f(y;\lambda)=\Prob{Y=y;\lambda}=\frac{\lambda^y \mathrm{e}^{-\lambda}}{y!},\; \lambda>0,\,y=0,1,2,\ldots.  \]
\begin{itemize}
      \item \textcolor{Red}{Likelihood function}:
            \[ L(\lambda;y_1,\ldots,y_n)=\prod_{i=1}^n f(y_i;\lambda)=\frac{\lambda^{\sum y_i}\mathrm{e}^{-n\lambda}}{\prod_i y_i!}.  \]
      \item \textcolor{Red}{Log-likelihood function}:
            \[ \ell(\lambda)=\biggl(\sum_i y_i\biggr)\log{\lambda}-n\lambda-\sum_{i=1}^{n} \log{y_i!}. \]
      \item \textcolor{Red}{Score function}:
            \[ S(\lambda)=\frac{\sum_i y_i}{\lambda}-n=0\implies \hat{\lambda}=\frac{\sum_{i=1}^{n} y_i}{n} =\bar{y}.  \]
            Need second derivative test to verify $ \hat{\lambda} $ is the MLE\@.
\end{itemize}
\subsection*{Newton Raphson Algorithm For Finding MLE}
\begin{itemize}
      \item Sometimes, solving $ S(\theta)=0 $ can be challenging and closed form solutions may
            not be obtained, iterative method need to be used to find the MLE.
      \item Recall \textcolor{Red}{Taylor Series} expansion of a differentiable function $ f(x) $ about a point $ a $:
            \[ f(x)=f(a)+\frac{f^\prime(a)}{1!}(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2+\cdots.  \]
      \item Now suppose we wish to find $ \hat{\theta} $, the root of $ S(\theta)=0 $ and $ \theta^{(0)} $ is a guess that
            is ``close'' to $ \hat{\theta} $.
      \item Consider the Taylor series expansion of $ S(\theta) $ about $ \theta^{(0)} $:
            \[ S(\theta)=S(\theta^{(0)})+\frac{S^{\prime}(\theta^{(0)})}{1!}(\theta-\theta^{(0)})+\frac{S^{\prime\prime}(\theta^{(0)})}{2!}(\theta-\theta^{(0)})^2+\cdots.   \]
      \item For $ \abs{\theta-\theta^{(0)}} $ very small, the second and higher order terms can be dropped to a good approximation:
            \begin{align*}
                  S(\theta) & \simeq S(\theta^{(0)})+S^\prime(\theta^{(0)})(\theta-\theta^{(0)}). \\
                  S(\theta) & \simeq S(\theta^{(0)})-I(\theta^{(0)})(\theta-\theta^{(0)}).
            \end{align*}
      \item Then at $ \theta=\hat{\theta} $,
            \begin{align*}
                  S(\hat{\theta})                            & \simeq S(\theta^{(0)})-I(\theta^{(0)})(\hat{\theta}-\theta^{(0)}) \\
                  I(\theta^{(0)})(\hat{\theta}-\theta^{(0)}) & \simeq S(\theta^{(0)})                                            \\
                  (\hat{\theta}-\theta^{(0)})                & \simeq I^{-1}(\theta^{(0)})S(\theta^{(0)})                        \\
                  \hat{\theta}                               & \simeq \theta^{(0)}+I^{-1}(\theta^{(0)})S(\theta^{(0)}).
            \end{align*}
      \item This suggests a revised guess for $ \hat{\theta} $ is:
            \[ \theta^{(1)}=\theta^{(0)}+I^{-1}(\theta^{(0)})S(\theta^{(0)}) \]
\end{itemize}
\begin{Regular}{Newton Raphson Algorithm for finding the MLE}
      \begin{itemize}
            \item Begin with an initial estimate $ \theta^{(0)} $.
            \item Iteratively obtain updated estimate by using:
                  \[ \theta^{(i+1)}=\theta^{(i)}+I^{-1}(\theta^{(i)})S(\theta^{(i)}). \]
            \item Iteration continues until $ \theta^{(i+1)}\simeq \theta^{(i)} $ within a specified tolerance.
            \item Then set $ \hat{\theta}=\theta^{(i+1)} $, check that $ I(\hat{\theta})>0 $.
      \end{itemize}
\end{Regular}
\subsection*{Inference for Scalar Parameters $ \theta $}
\begin{itemize}
      \item So far we have discussed estimation of $ \hat{\theta} $, next we want to conduct inference
            about $ \theta $, i.e., carry out hypothesis tests and construct confidence intervals of $ \theta $.
      \item Likelihood inference relies on the following \textcolor{Red}{asymptotic distribution results}:
            \begin{Regular}{Useful asymptotic distributional results}
                  \begin{itemize}
                        \item \textcolor{Red}{(log) Likelihood ratio statistic}: $ -2\log[\big]{R(\theta)}=-2r(\theta)\sim \chi^2_{(1)} $.
                        \item \textcolor{Red}{Score statistic}: $ \bigl(S(\theta)\bigr)^2/I(\theta)\sim \chi^2_{(1)} $.
                        \item \textcolor{Red}{Wald statistic}: $ (\hat{\theta}-\theta)^2 I(\hat{\theta}) \sim \chi^2_{(1)} $ or $ (\hat{\theta}-\theta)\sqrt{I(\hat{\theta})}\sim \N{0,1} $
                              since $ Z \sim \N{0,1} \implies Z^2 \sim \chi^2_{1} $.
                  \end{itemize}
            \end{Regular}
\end{itemize}
\subsection*{Confidence Interval (CI)}
Suppose we want a $ 100(1-\alpha)\, \% $ confidence interval for $ \theta $.
\begin{itemize}
      \item The \textcolor{Red}{Likelihood ratio (LR)} based pivotal gives a confidence interval:
            \[ \Set*{\theta:-2r(\theta)<\chi^2_{1}(1-\alpha)}, \]
            where $ \chi^2_1(1-\alpha) $ is the upper $ \alpha $ percentage point of the $ \chi^2_1 $ distribution.
      \item The \textcolor{Red}{Wald}-based pivotal gives an interval:
            \[ \Set*{\theta:(\hat{\theta}-\theta)^2 I(\hat{\theta})<\chi^2_1(1-\alpha)}, \]
            or equivalently
            \[ \hat{\theta}\pm Z_{1-\alpha/2}\bigl(I(\hat{\theta})\bigr)^{-1/2}, \]
            where $ Z_{1-\alpha/2} $ is the upper $ \alpha/2 $ percentage point of the standard normal.
\end{itemize}
\subsection*{Example: Hormone Therapy Data}
\textcolor{Red}{Likelihood Ratio} based $ \qty{95}{\percent} $ CI: $ \Set*{\theta:-2r(\theta)<\chi^2_{1}(0.95)} $
where $ r(\theta)=\ell(\theta)-\ell(\hat{\theta}) $.
\begin{itemize}
      \item For the Binomial distribution: $ \hat{\theta}=y/n $, and
            \[ r(\theta)=\underbrace{\bigl(y\log{\theta}+(n-y)\log{1-\theta}\bigr)}_{\ell(\theta)}-
                  \underbrace{\biggl(y\log*{\frac{y}{n}}+(n-y)\log*{1-\frac{y}{n}}\biggr)}_{\ell(\hat{\theta})}. \]
      \item To find the root of $ -2r(\theta)=\chi^2_1(0.95) \iff -2r(\theta)-\chi^2_1(0.95) $:
            %\begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{y} \hlkwb{=} \hlnum{166}
\hlstd{n} \hlkwb{=} \hlnum{8506}
\hlstd{LRCI} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{n}\hlstd{) \{}
  \hlopt{-}\hlnum{2} \hlopt{*} \hlstd{(y} \hlopt{*} \hlkwd{log}\hlstd{(theta)} \hlopt{+} \hlstd{(n} \hlopt{-} \hlstd{y)} \hlopt{*} \hlkwd{log}\hlstd{(}\hlnum{1} \hlopt{-} \hlstd{theta)} \hlopt{-} \hlstd{y} \hlopt{*} \hlkwd{log}\hlstd{(y}\hlopt{/}\hlstd{n)} \hlopt{-}
    \hlstd{(n} \hlopt{-} \hlstd{y)} \hlopt{*} \hlkwd{log}\hlstd{(}\hlnum{1} \hlopt{-} \hlstd{y}\hlopt{/}\hlstd{n))} \hlopt{-} \hlkwd{qchisq}\hlstd{(}\hlnum{0.95}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{\}}
\hlstd{mle} \hlkwb{=} \hlstd{y}\hlopt{/}\hlstd{n}
\hlkwd{uniroot}\hlstd{(LRCI,} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{, mle),} \hlkwc{y} \hlstd{= y,} \hlkwc{n} \hlstd{= n)}\hlopt{$}\hlstd{root}
\end{alltt}
\begin{verbatim}
[1] 0.01673867
\end{verbatim}
\begin{alltt}
\hlkwd{uniroot}\hlstd{(LRCI,} \hlkwd{c}\hlstd{(mle,} \hlnum{1}\hlstd{),} \hlkwc{y} \hlstd{= y,} \hlkwc{n} \hlstd{= n)}\hlopt{$}\hlstd{root}
\end{alltt}
\begin{verbatim}
[1] 0.02260709
\end{verbatim}
\end{kframe}
\end{knitrout}
            %\end{noindent}
      \item The likelihood ratio based $ \qty{95}{\percent} $ CI is $(0.017, 0.023)$.
            \[ -2r(\theta)<\chi^2_1(0.95)\iff r(\theta)>-\frac{1}{2} \chi^2_1(0.95)=-1.92. \]
            \begin{figure}[!htbp]
                  \centering
                  \includegraphics{figures/1bLR.pdf}
            \end{figure}
\end{itemize}
\textcolor{Red}{Wald} based $ \qty{95}{\percent} $ CI: $ \hat{\theta}\pm Z_{0.975}\bigl(I(\hat{\theta})\bigr)^{-1/2} $.
\begin{itemize}
      \item For Binomial distribution $ \hat{\theta}=y/n $ and
            \[ I(\hat{\theta})=\frac{y}{\hat{\theta}^2}+\frac{n-y}{(1-\hat{\theta})^2}=n^2\biggl(\frac{1}{y} +\frac{1}{n-y}\biggr).   \]
      \item So we solve:
            \begin{align*}
                  \hat{\theta}\pm 1.96\bigl(I(\hat{\theta})\bigr)^{-1/2}
                   & =0.0195 \pm 1.96(0.0015) \\
                   & =(0.017, 0.022).
            \end{align*}
      \item The Wald based $ \qty{95}{\percent} $ CI is: $ (0.017, 0.022) $.
\end{itemize}
\subsection*{Hypotheses Test}
Suppose we are interested in testing hypotheses:
\[ \text{$\HN$: $\theta=\theta_0$ vs $\HA$: $\theta\ne \theta_0$.} \]
\begin{itemize}
      \item \textcolor{Red}{Likelihood ratio (LR) test}: $ p\text{-value}=\Prob*{\chi^2_1>-2r(\theta_0)} $.
      \item \textcolor{Red}{Score test}: $ p\text{-value}=\Prob*{\chi^2_1>\bigl(S(\theta)\bigr)^2/I(\theta_0)} $.
      \item \textcolor{Red}{Wald test}:
            \[ p\text{-value}=\Prob*{\chi^2_1>(\hat{\theta}-\theta_0)^2 I(\hat{\theta})}\text{, or }
                  p\text{-value}=\Prob*{\abs{Z}>\abs{\hat{\theta}-\theta_0}\sqrt{I(\hat{\theta})}}. \]
\end{itemize}
\subsection*{Example: Hormone Therapy Data}
Suppose we wish to test if women received EPT would have a risk of breast
cancer same as that of the general population, say about \qty{1.5}{\percent}.
\[ \text{$\HN$: $\theta=0.015$ vs $\HA$: $\theta\ne 0.015$.} \]
\begin{itemize}
      \item \textcolor{Red}{Likelihood Ratio} based test:
            \begin{align*}
                  r(\theta_0=0.015)
                   & =\biggl(y\log{0.015}+(n-y)\log{1-0.15}\biggr)-\biggl(y\log*{\frac{y}{n}}+(n-y)\log*{1-\frac{y}{n} }\biggr) \\
                   & =-5.3637.
            \end{align*}
            Thus, the $ p $-value for the test is given by:
            \[ p=\Prob*{\chi^2_{(1)}>-2r(0.015)}=\Prob*{\chi^2_{(1)}>10.7274}=0.001. \]
            Therefore, we \emph{reject} $ \HN $ and conclude that the risk of breast cancer for women received EPT is
            significantly different from \qty{1.5}{\percent}.
\end{itemize}
\subsection*{Notes on Asymptotic Inference}
\begin{itemize}
      \item Asymptotic results: approximation improves as sample size increases.
      \item Results are exact for a Normal linear model if $ \theta $ is the mean parameter and $ \sigma^2 $ is
            known.
      \item \textcolor{Red}{LR approach}:
            \begin{itemize}
                  \item Need to evaluate (log) likelihood at two locations.
                  \item Not always a closed from solution for a CI.
                  \item Usually the best approach.
            \end{itemize}
      \item \textcolor{Red}{Score approach}:
            \begin{itemize}
                  \item Usually the least powerful test.
                  \item Don't actually need to find MLE to use.
            \end{itemize}
      \item \textcolor{Red}{Wald's approach}:
            \begin{itemize}
                  \item Always get a closed form solution for a CI.
                  \item May not behave well for skewed likelihoods (transform?).
            \end{itemize}
      \item All three are asymptotically equivalent!
\end{itemize}
\subsection*{Likelihood Methods for Parameter Vectors}
Suppose $ \Vector{\theta}=(\theta_1,\theta_2,\ldots,\theta_p)^\top\in \Omega $ is a continuous $ p\times 1 $ parameter vector
indexing a probability density (or mass) function $ f(\Vector{y};\Vector{\theta}) $. The likelihood and
log-likelihood functions are defined as before, but
\begin{itemize}
      \item $ \Vector{S}(\Vector{\theta})=\pdv{\ell( \Vector{\theta})}{ \Vector{\theta}} $ is the $ p\times 1 $ \textcolor{Red}{Score vector}, i.e.,
            \[ \Vector{S}(\Vector{\theta})=\begin{bmatrix}
                        \pdv{\ell(\theta)}{\theta_1} \\
                        \vdots                       \\
                        \pdv{\ell(\theta)}{\theta_p}
                  \end{bmatrix}. \]
      \item $ \Matrix{I}(\Vector{\theta})=-\pdv{\ell(\Vector{\theta})}{ \Vector{\theta}^\top,\Vector{\theta}} $ is the $ p\times p $ \textcolor{Red}{Information matrix}, i.e.,
            \[ \Matrix{I}(\Vector{\theta})=\begin{bmatrix}
                        -\pdv[order=2]{\ell(\theta)}{\theta_1} & -\pdv{\ell(\theta)}{\theta_1,\theta_2} & \cdots & \pdv{\ell(\theta)}{\theta_1,\theta_p} \\
                                                               & -\pdv[order=2]{\ell(\theta)}{\theta_2} & \cdots & \pdv{\ell(\theta)}{\theta_1,\theta_p} \\
                                                               &                                        & \ddots & \pdv[order=2]{\ell(\theta)}{\theta_p}
                  \end{bmatrix}. \]

\end{itemize}
\begin{itemize}
      \item The Newton Raphson algorithm applies as before, but with vectors and matrices
            as follows:
            \[ \Vector{\theta}^{(i+1)}=\Vector{\theta}^{(i)}+\Matrix{I}^{-1}(\Vector{\theta}^{(i)})\Vector{S}(\Vector{\theta}^{(i)}). \]
            Again, we apply iteratively until we obtain convergence, but now check to
            see if $ \Matrix{I}(\hat{\Vector{\theta}}) $ is a positive definite matrix.
\end{itemize}
Suppose we want to make inference about a specific parameter in $ \Vector{\theta} $, say we
partition vector $ \Vector{\theta}=(\alpha,\Vector{\beta})^\top $ and \textcolor{Red}{$ \alpha $
      is the parameter of interest}. Analogues to the LR, Score, and Wald results apply, e.g.,
\begin{itemize}
      \item LR statistic: $ -2\bigl[\ell(\alpha,\hat{\Vector{\beta}}_{\alpha})-\ell(\hat{\alpha},\hat{\Vector{\beta}})\bigr]\sim \chi^2_{(1)} $.
            \begin{itemize}
                  \item $ \hat{\Vector{\beta}}_{\alpha} $ is the MLE of $ \Vector{\beta} $ given $ \alpha $ is fixed.
                  \item $ \hat{\alpha} $ and $ \hat{\Vector{\beta}} $ are the joint MLE of $ \alpha $ and $ \Vector{\beta} $.
            \end{itemize}
      \item Score statistic: $ S_\alpha(\alpha,\hat{\Vector{\beta}}_\alpha)^2 I^{\alpha\alpha}\sim \chi^2_{(1)} $.
            \begin{itemize}
                  \item $ S_\alpha=\pdv{\ell}{\alpha} $.
                  \item $ I^{\alpha\alpha} $ is the $ (\alpha,\alpha) $ element of $ \Matrix{I}(\alpha,\Vector{\beta})^{-1} $ (inverse of Information matrix).
            \end{itemize}
      \item Wald statistic: $ (\hat{\alpha}-\alpha)^2/I^{\alpha\alpha}\sim \chi^2_{(1)} $.
\end{itemize}

\section*{Topic 2a: Formulation of Generalized Linear Models}
\addcontentsline{toc}{section}{Topic 2a: Formulation of Generalized Linear Models}
\subsection*{The Exponential Family}
\begin{Regular}{Definition (Exponential Family)}
    Consider a random variable $ Y $ with probability density (or mass) function $ f(y;\theta,\phi) $,
    we say that the distribution is a member of the \textcolor{Red}{exponential family} if we can write
    \[ f(y;\theta,\phi)=\exp*{\frac{y\theta-b(\theta)}{a(\phi)}+c(y;\phi)}, \]
    for some functions $ a(\:\cdot\:) $, $ b(\:\cdot\:) $, and $ c(\:\cdot\:) $.
    \begin{itemize}
        \item The parameter $ \theta $ is called the \textcolor{Red}{canonical} parameter, and it is unknown.
        \item The parameter $ \phi $ is called the \textcolor{Red}{scale/dispersion} parameter, is constant, and assumed to be known.
    \end{itemize}
\end{Regular}
Many well known distributions (continuous/discrete) can be shown to be a
member of the exponential family.
\subsection*{Examples}
\begin{itemize}
    \item Poisson Distribution: $ Y \sim \POI{\lambda} $,
          \[ f(y;\lambda)=\frac{\lambda^y \mathrm{e}^{-\lambda}}{y!},\; \lambda>0,\, y=0,1,\ldots.  \]
          Show that Poisson is a member of exponential family and identify the canonical
          parameter and the functions $ a(\:\cdot\:) $, $ b(\:\cdot\:) $, and $ c(\:\cdot\:) $.

          \textbf{Solution.} $ f(y;\lambda)=\exp[\big]{\log{f(y;\lambda)}}=\exp*{\frac{y\log{\lambda}-\lambda}{1} -\log{y!}} $. Therefore,
          \begin{align*}
              \theta    & =\log{\lambda}\qquad\text{(canonical/natural parameter)}, \\
              b(\theta) & =\lambda=\mathrm{e}^{\theta},                             \\
              \phi      & =1,                                                       \\
              a(\phi)   & =1,                                                       \\
              c(y;\phi) & =-\log{y!}.
          \end{align*}
    \item Normal Distribution: $ Y \sim \N{\mu,\sigma^2} $ and $ \sigma^2 $ known,
          \[ f(y;\theta,\phi)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp*{-\frac{(y-\mu)^2}{2\sigma^2}}. \]
          Show that this Normal distribution is a member of the exponential family.

          \textbf{Solution.}
          \begin{align*}
              f(y;\mu,\sigma^2)
               & =\exp*{-\frac{y^2-2\mu y+\mu^2}{\sigma^2}-\frac{1}{2} \log{2\pi\sigma^2}}                   \\
               & =\exp*{\frac{y\mu-\mu^2/2}{\sigma^2}-\frac{y^2}{2\sigma^2}-\frac{1}{2} \log{2\pi\sigma^2}}.
          \end{align*}
          Therefore,
          \begin{align*}
              \theta    & =\mu,                                                   \\
              \phi      & =\sigma^2,                                              \\
              a(\phi)   & =\phi=\sigma^2,                                         \\
              b(\theta) & =\frac{\mu^2}{2}=\frac{\theta^2}{2},                    \\
              c(y;\phi) & =-\frac{y^2}{2\sigma^2}-\frac{1}{2} \log{2\pi\sigma^2}.
          \end{align*}
\end{itemize}
\subsection*{Properties of Exponential Family}
Consider a single observation $y$ from the exponential family.
\begin{align*}
    L(\theta,\phi;y)    & =f(y;\theta,\phi)=\exp*{\frac{y\theta-b(\theta)}{a(\phi)}+c(y;\phi)}.      \\
    \ell(\theta,\phi;y) & =\log[\big]{f(y;\theta,\phi)}=\frac{y\theta-b(\theta)}{a(\phi)}+c(y;\phi). \\
    S(\theta)           & =\pdv{\ell}{\theta}=\frac{y-b^\prime(\theta)}{a(\phi)}.                    \\
    I(\theta)           & =-\pdv[order=2]{\ell}{\theta}=\frac{b^{\prime\prime}(\theta)}{a(\phi)}.    \\
    \mathcal{I}(\theta) & =\E*{-\pdv[order=2]{\ell}{\theta}}=I(\theta).
\end{align*}
\subsection*{Some General Results for Score and Information}
\begin{Result}{Result \# 1}
    The expectation of the score function is zero.
    \[ \E[\big]{S(\theta)}=0. \]
    \tcblower{}
    \textbf{Proof}:
    \begin{align*}
        \int f(y;\theta,\phi)\odif{y}                                                         & =1                               \\
        \pdv*{\int f(y;\theta,\phi)\odif{y}}{\theta}                                          & =0                               \\
        \int\pdv*{f(y;\theta,\phi)}{\theta} \odif{y}                                          & =0                               \\
        \int\biggl(\pdv*{\log[\big]{f(y;\theta,\phi)}}{\theta}\biggr)f(y;\theta,\phi)\odif{y} & =0 &  & \label{2a:eq1}\tag*{(1)} \\
        \int S(\theta)f(y;\theta,\phi)\odif{y}                                                & =0                               \\
        \E[\big]{S(\theta)}                                                                   & =0
    \end{align*}
\end{Result}
\begin{Result}{Result \# 2}
    The expectation of the score function squared is the expected information.
    \[  \E[\big]{S(\theta;y)^2}=\E[\big]{I(\theta;y)} \]
    \tcblower{}
    \textbf{Proof}: Differentiate~\ref{2a:eq1} again,
    \begin{align*}
        \pdv*{\int\biggl(\pdv*{\log[\big]{f(y;\theta,\phi)}}{\theta}\biggr)f(y;\theta,\phi)\odif{y}}{\theta}                                                                                                 & =0 \\
        \int \biggl(\pdv*[order=2]{\log[\big]{f(y;\theta,\phi)}}{\theta}\biggr)f(y;\theta,\phi)\odif{y}+\int\biggl(\pdv*{\log[\big]{f(y;\theta,\phi)}}{\theta}\biggr)\pdv*{f(y;\theta,\phi)}{\theta}\odif{y} & =0 \\
        \int \pdv*[order=2]{\log[\big]{f(y;\theta,\phi)}}{\theta}f(y;\theta,\phi)\odif{y}+\int\biggl(\pdv*{f(y;\theta,\phi)}{\theta}\biggr)^2 f(y;\theta,\phi)\odif{y}                                       & =0 \\
        \int -I(\theta)f(y;\theta,\phi)\odif{y}+\int S(\theta)^2 f(y;\theta,\phi)\odif{y}                                                                                                                    & =0 \\
        \E[\big]{-I(\theta;y)}+\E[\big]{S(\theta;y)^2}                                                                                                                                                       & =0
    \end{align*}
\end{Result}
Now for the exponential family, we apply above results and obtain:
\begin{align*}
    \E[\big]{S(\theta)}                                          & =0,                                             \\
    \E*{\frac{Y-b^{\prime}(\theta)}{a(\phi)}}                    & =0,                                             \\
    \E{Y}                                                        & =b^\prime(\theta),                              \\\\
    \E[\big]{S(\theta)^2}                                        & =\E[\big]{I(\theta)},                           \\
    \E*{\biggl(\frac{Y-b^\prime(\theta)}{a(\phi)} \biggr)^{\!2}} & =\E*{\frac{b^{\prime\prime}(\theta)}{a(\phi)}}, \\
    \frac{1}{a(\phi)^2}\E*{\bigl(Y-\E{Y}\bigr)^2}                & =\frac{b^{\prime\prime}(\theta)}{a(\phi)},      \\
    \Var{Y}                                                      & =b^{\prime\prime}(\theta)a(\phi).
\end{align*}
\begin{Regular}{Mean and Variance for the Exponential Family}
    \begin{itemize}
        \item Mean: $ \E{Y}=b^\prime(\theta)=\mu $.
        \item Variance: $ \Var{Y}=b^{\prime\prime}(\theta)a(\phi) $.
    \end{itemize}
\end{Regular}
Note that:
\begin{itemize}
    \item $ b^\prime(\theta)=\mu $ tells the relationship between \emph{canonical} parameter $ \theta $ and $ \mu $.
    \item $ b^{\prime\prime}(\theta) $ is a function of $ \theta $ and hence can be also expressed as a function of $ \mu $.
    \item Thus, we write $ b^{\prime\prime}(\theta)=\V{\mu} $ and call \textcolor{Red}{$ \V{\mu} $} the \textcolor{Red}{variance function}.
    \item Subsequently, we have:
          \[ \Var{Y}=b^{\prime\prime}(\theta)a(\phi)=\V{\mu}a(\phi), \]
          which is the \textcolor{Red}{mean-variance relationship} for the exponential family.
\end{itemize}
\subsection*{Link Functions}
\begin{Regular}{Definition (Link Function)}
    The \textcolor{Red}{link function} relates the linear predictor $ \eta=\Vector{x}^\top\Vector{\beta} $ to the expected value $ \mu $ of the random variable $ Y $, i.e.,
    \[ g(\mu)=\eta=\Vector{x}^\top\Vector{\beta}, \]
    where $ g(\:\cdot\:) $ is the link function.
\end{Regular}
\begin{Regular}{Definition (Canonical Link Function)}
    When $Y$ is a member of the exponential family we define the \textcolor{Red}{canonical link function} to be:
    \[ g(\mu)=\theta=\eta=\Vector{x}^\top\Vector{\beta} \]
    (i.e., the choice of $ g(\:\cdot\:) $ that sets canonical parameter = linear predictor).
\end{Regular}
\subsection*{Example}
Recall that $ \POI{\lambda} $ is a member of exponential family,
\[ f(y;\lambda)=\frac{\lambda^y \mathrm{e}^{-\lambda}}{y!}=\exp*{\frac{y\log{\lambda}-\lambda}{1}-\log{y!}}  \]
where $ \theta=\log{\lambda} $, $ \phi=1 $, $ b(\theta)=\lambda=\mathrm{e}^{\theta} $, and $ a(\phi)=1 $. Now to find the mean, variance function, and canonical link function:
\begin{itemize}
    \item \textcolor{Blue}{Mean}: $ \E{Y}=b^\prime(\theta)=\mathrm{e}^{\theta}=\mu\implies \theta=\log{\mu} $.
    \item \textcolor{Blue}{Variance Function}: $ \V{\mu}=b^{\prime\prime}(\theta)=\mathrm{e}^{\theta}\implies \V{\mu}=\mu $.
    \item \textcolor{Blue}{Variance}: $ \Var{Y}=\V{\mu}a(\phi)=\mu $ (mean-variance relationship).
    \item \textcolor{Blue}{Canonical link}: set $ \theta=\eta $ using $ \theta=\log{\mu}=\eta=\Vector{x}^\top \Vector{\beta} $, i.e., $ g(\mu)=\log{\mu} $ where $ \log{\:\cdot\:} $
          is the canonical link.
\end{itemize}
Moving forward, we consider a log-linear model: $ \log{\mu_i}=\Vector{x}_i^\top \Vector{\beta} $.

\subsection*{Remarks on Link Function}
\begin{itemize}
    \item We can choose any function $ g(\:\cdot\:) $ as the link function in theory.
    \item The canonical link is a special link function, we often choose to use
          canonical link for its good statistical properties.
    \item Context and goodness of fit should motivate the choice of link function in
          practice.
\end{itemize}
\subsection*{Generalized Linear Models}
\begin{Regular}{Definition (Generalized Linear Model (GLM))}
    A \textcolor{Red}{Generalized Linear Model (GLM)} is composed of three components:
    \begin{itemize}
        \item \textcolor{Red}{Random Component}: The responses $ Y_1,\ldots,Y_n $ are
              independent random variables and each $ Y_i $ is assumed to come from a parametric distribution that is a member of the
              exponential family.
        \item \textcolor{Red}{Systematic Component} (or linear predictor):
              \[ \eta_i=\Vector{x}_i^\top\Vector{\beta}, \]
              a linear combination of explanatory variables $ \Vector{x}_i $ and regression parameters $ \Vector{\beta} $.
        \item \textcolor{Red}{Link function}:
              \[ g(\mu_i)=\eta_i=\Vector{x}_i^\top\Vector{\beta}, \]
              a function that relates the mean of response to the linear predictor.
    \end{itemize}
\end{Regular}
\subsection*{Topic Summary}
2a Formulation of Generalized Linear Models:
\begin{itemize}
    \item Definition of the \textcolor{Blue}{Exponential Family}.
          \begin{itemize}
              \item Exponential form of the probability density (or mass) function.
              \item Derivation of Score and Information.
              \item Properties of exponential family, mean-variance relationship.
              \item Definition of canonical link.
          \end{itemize}
    \item Definition of a \textcolor{Blue}{Generalized Linear Model}.
\end{itemize}
Next Topic: 2b Estimation for Generalized Linear Models.
% Week 3

\makeheading{Week 3}{\daterange{2021-09-20}{2021-09-24}}
\section*{Topic 2b: Maximum Likelihood Estimation for Generalized Linear Models}
\addcontentsline{toc}{section}{Topic 2a: Maximum Likelihood Estimation for Generalized Linear Models}
\subsection*{Generalized Linear Models}
Suppose for each subject $ i=1,\ldots,n $ in a random sample:
\begin{itemize}
    \item $ Y_i $ is the response variable.
    \item $ x_{i1},\ldots,x_{ip} $ are explanatory variables associated with $ Y_i $.
\end{itemize}
We consider a \textcolor{Red}{Generalized Linear Model} (GLM) for the data, by definition the GLM
is composed following three components:
\begin{Regular}{}
    \begin{enumerate}[label=\color{Blue}\protect\circled{\arabic*}]
        \item \textcolor{Red}{Random Component}:
              \[ Y_i \sim \text{exponential family,}\qquad Y_1,\ldots,Y_n\text{ are independent.} \]
        \item \textcolor{Red}{Systematic Component} (or linear predictor):
              \[ \eta_i=\Vector{x}_i^\top \Vector{\beta}=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip}. \]
              \begin{itemize}
                  \item $ \Vector{x}_i=(1,x_{i1},\ldots,x_{ip})^\top $ is a covariate vector.
                  \item $ \Vector{\beta}=(\beta_0,\beta_1,\ldots,\beta_p)^\top $ is a vector of regression coefficients.
              \end{itemize}
        \item \textcolor{Red}{Link function}: a function $ g(\:\cdot\:) $ links $ \E{Y_i}=\mu_i $ to a linear prediction $ \eta_i $:
              \[ g(\mu_i)=\eta_i=\Vector{x}_i^\top\Vector{\beta}. \]
    \end{enumerate}
\end{Regular}
\subsection*{Example: A Poisson Regression Model}
Suppose $ Y_i \ind\POI{\lambda_i} $ with mean $ \E{Y_i}=\lambda_i $, $ i=1,\ldots,n $:
\[ f(y_i)=\frac{\mathrm{e}^{-\lambda_i}\lambda_i^{y_i}}{y_i!}=\exp[\big]{y_i\log{\lambda_i}-\lambda_i-\log{y_i!}}.  \]
Poisson distribution is a member of exponential family with:
\begin{itemize}
    \item Canonical parameter: $ \theta_i=\log{\lambda_i} $.
    \item Canonical link: $ \theta_i=\eta_i\textcolor{Red}{\implies}\log{\lambda_i}=\Vector{x}_i^\top \Vector{\beta} $ (log link).
\end{itemize}
A Poisson regression model with the canonical link takes the form:
\[ \log{\lambda_i}=\Vector{x}_i^\top \Vector{\beta}=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip}\qquad \text{\textcolor{Red}{(log-linear model)}}. \]
\subsection*{Example: A Normal Regression Model}
Assume $ Y_i \ind \N{\mu_i,\sigma^2} $ and $ \sigma^2 $ is known, $ i=1,\ldots,n $:
\begin{align*}
    f(y_i)
     & =(2\pi\sigma^2)^{-1/2}\exp*{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}                                                   \\
     & =\exp*{\frac{y_i\mu_i-\mu_i^2/2}{\sigma^2}-\frac{1}{2}\biggl(\frac{y_i^2}{\sigma^2}+\log{2\pi\sigma^2}\biggr)}.
\end{align*}
A Normal distribution ($ \sigma^2 $ known) is a member of exponential family with:
\begin{itemize}
    \item Canonical parameter: $ \theta_i=\mu_i $.
    \item Canonical link: $ \theta_i=\eta_i \textcolor{Red}{\implies}\mu_i=\Vector{x}_i^\top \Vector{\beta} $ (identity link).
\end{itemize}
A Normal regression model with the canonical link takes the form:
\[ \mu_i=\Vector{x}_i^\top \Vector{\beta}=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip}\qquad \text{\textcolor{Red}{(linear model)}}. \]
Linear regression model (STAT 331) is a Normal GLM using the canonical link!
\subsection*{Likelihood for Generalized Linear Models}
We wish to use likelihood methods for the estimation of the regression parameter $ \Vector{\beta} $ from the GLM: $ g(\mu_i)=\Vector{x}_i^\top \Vector{\beta} $.
Consider the log-likelihood for a \emph{single} observation from the exponential family:
\[ \ell(\theta,\phi;y)=\frac{y\theta-b(\theta)}{a(\phi)}+c(y;\phi). \]
\begin{itemize}
    \item $ \ell $ is a function of $ \textcolor{Red}{\theta} $ (assume that $ \phi $ is known).
    \item $ \theta $ is related to $ \mu $ through the result:
          \[ \textcolor{Red}{\mu=b^{\prime}(\theta)}. \]
    \item $ \eta $ can be expressed in terms of $ \mu $ through the link function:
          \[ \textcolor{Red}{g(\mu)=\eta}. \]
    \item $ \Vector{\beta} $ can be expressed in terms of $ \eta $ through the linear predictor:
          \[ \textcolor{Red}{\eta=\Vector{x}^\top \Vector{\beta}}. \]
\end{itemize}
\subsection*{Score Vector}
To find the maximum likelihood estimator for $ \Vector{\beta} $, we must solve $ \Vector{S}(\Vector{\beta})=\pdv{\ell}{\Vector{\beta}}=\Vector{0} $.
Consider taking derivative with respect to $ \beta_j $ using the chain rule:
\[ \pdv{\ell}{\beta_j}=\pdv{\ell}{\theta}\pdv{\theta}{\mu}\pdv{\mu}{\eta}\pdv{\eta}{\beta_j}, \]
where
\begin{align*}
    \pdv{\ell}{\theta}  & =\frac{y-b^{\prime}(\theta)}{a(\phi)},                                                                                                          \\
    \pdv{\theta}{\mu}   & =\pdv{\mu}{\theta}^{\!-1}=\frac{1}{b^{\prime\prime}(\theta)} &  & \text{since $ \mu=b^\prime(\theta) $},                                        \\
    \pdv{\mu}{\eta}     & =\pdv{\mu}{\eta},                                                                                                                               \\
    \pdv{\eta}{\beta_j} & =x_j                                                         &  & \text{since $ \eta=\beta_0+\beta_1x_1+\cdots+\beta_jx_j+\cdots+\beta_px_p $}.
\end{align*}
Hence, we have:
\begin{align*}
    \pdv{\ell}{\beta_j}
     & =\frac{y-b^{\prime}(\theta)}{a(\phi)}\frac{1}{b^{\prime\prime}(\theta)}\pdv{\mu}{\eta}x_j                                                                                           \\
     & =\frac{y-\mu}{\Var{Y}}\pdv{\mu}{\eta}x_j                                                  &  & \text{since $ \mu=b^{\prime}(\theta) $, $ \Var{Y}=a(\phi)b^{\prime\prime}(\theta) $} \\
     & =\frac{y-\mu}{\Var{Y}}\pdv{\mu}{\eta}^{\!2}\pdv{\eta}{\mu}x_j                             &  & \text{since $ \pdv{\mu}{\eta}\pdv{\eta}{\mu}=1 $}                                    \\
     & =(y-\mu)\Biggl(\Var{Y}\pdv{\mu}{\eta}^{\!2}\Biggr)^{\!-1}\pdv{\eta}{\mu}x_j                                                                                                         \\
     & =(y-\mu)W\pdv{\eta}{\mu}x_j,
\end{align*}
where $ W^{-1}=\Var{Y}\pdv{\eta}{\mu}^2 $. Note that generally $ \pdv{\eta}{\mu} $ is easier to calculate
than $ \pdv{\mu}{\eta} $ since we define the link as $ \eta=g(\mu) $.

For a random sample $ Y_1,\ldots,Y_n $ from exponential family and each $ Y_i $ has a probability density function
\[ f(y_i;\theta,\phi)=\exp*{\frac{y_i\theta_i-b(\theta_i)}{a(\phi)}+c(y_i,\phi)}. \]
We write likelihood and log-likelihood functions as:
\begin{align*}
    L    & =\prod_{i=1}^n f(y_i;\theta_i,\phi)=\prod_{i=1}^n\exp*{\frac{y_i\theta_i-b(\theta_i)}{a(\phi)}+c(y_i,\phi)}, \\
    \ell & =\sum_{i=1}^{n} \ell_i=\sum_{i=1}^{n}\frac{y_i\theta_i-b(\theta_i)}{a(\phi)}+c(y_i,\phi).
\end{align*}
The \textcolor{Red}{element of the score vector} is:
\[ \bigl[\Vector{S}(\Vector{\beta})\bigr]_j=\pdv{\ell}{\beta_j}=\sum_{i=1}^{n} \pdv{\ell_i}{\beta_j}=\sum_{i=1}^{n} (y_i-\mu_i)W_i\pdv{\eta_i}{\mu_i}x_{ij} \]
where $  W_i^{-1}=\Var{Y_i}(\pdv{\eta_i}{\mu_i})^2 $, $ g(\mu_i)=\eta_i=\Vector{x}_i^\top \Vector{\beta} $. In vector and matrix form we can write:
\[ \Vector{S}(\Vector{\beta})=\Matrix{X}\MatrixCal{W}\MatrixCal{A}(\Vector{y}-\Vector{\mu}), \]
where
\begin{itemize}
    \item $ \Vector{y}=(y_1,\ldots,y_n)^\top $ and $ \Vector{\mu}=(\mu_1,\ldots,\mu_n)^\top $ are $ n\times 1 $ vectors,
    \item $ \Matrix{X}=(\Vector{x}_1,\ldots,\Vector{x}_n) $ is a $ (p+1)\times n $ matrix,
    \item $ \MatrixCal{W}=\diag{W_1,\ldots,W_n}=\begin{bmatrix}
                  W_1    & 0      & \cdots & 0      \\
                  \vdots & \ddots & \ddots & \vdots \\
                  0      & \cdots & 0      & W_n
              \end{bmatrix} $, and
    \item $ \MatrixCal{A}=\diag*{\pdv{\eta_1}{\mu_1},\ldots,\pdv{\eta_n}{\mu_n}} $.
\end{itemize}
\subsection*{Example: Poisson Regression Model (Problem 1.4)}
For a random sample from Poisson distribution, $ Y_i \sim \POI{\lambda_i} $, $ i=1,\ldots,n $,
\[ \ell_i=\log[\big]{f(y_i;\lambda_i)}=\bigl(y_i\log{\lambda_i}-\lambda_i-\log{y_i!}\bigr). \]
Poisson regression with a log-link:
\[ \log{\lambda_i}=\eta_i=\Vector{x}_i^\top \Vector{\beta}. \]
To write down the score vector for the regression coefficients $ \Vector{\beta} $, we may
calculate the derivative using standard methods, i.e.,
\begin{align*}
    \bigl[\Vector{S}(\Vector{\beta})\bigr]_j
     & =\sum_i \pdv{\ell_i}{\beta_j}                                                                                  \\
     & =\sum_i \pdv*{\bigl(y_i \textcolor{Red}{\log{\lambda_i}}-\textcolor{Red}{\lambda_i}-\log{y_i!}\bigr)}{\beta_j} \\
     & =\sum_i\bigl(y_i x_{ij}-\mathrm{e}^{\Vector{x}_i^\top \Vector{\beta}}x_{ij}\bigr).
\end{align*}
Or we can use the general results derived for the GLMs on the previous slides.
\subsection*{Solving $ \Vector{S}(\Vector{\beta})=\Vector{0} $ for MLE}
\begin{enumerate}[label=\color{Blue}\protect\circled{\arabic*}]
    \item Newton Raphson update equation is:
          \[ \hat{\Vector{\beta}}^{(r+1)}=\hat{\Vector{\beta}}^{(r)}+\Matrix{I}^{-1}(\hat{\Vector{\beta}}^{(r)})\Vector{S}(\hat{\Vector{\beta}}^{(r)}), \]
          where $ \Matrix{I} $ is the observed information matrix.
          \begin{itemize}
              \item This requires us to find and repeatedly evaluate the information $ \Matrix{I} $ (possibly computationally intensive).
              \item Fisher suggested using the expected information matrix $ \MatrixCal{I} $ rather than the observed information matrix.
          \end{itemize}
    \item Fisher Scoring update equation is:
          \[ \hat{\Vector{\beta}}^{(r+1)}=\hat{\Vector{\beta}}^{(r)}+\MatrixCal{I}^{-1}(\hat{\Vector{\beta}}^{(r)})\Vector{S}(\hat{\Vector{\beta}}^{(r)}). \]
\end{enumerate}
\subsection*{Information Matrix}
Consider the $(j, k)$ element of the Information matrix:
\begin{align*}
    \Matrix{I}_{jk} & =-\pdv{\ell}{\beta_j,\beta_k}                                                                                                                                                                                     \\
                    & =-\pdv*{\pdv{\ell}{\beta_j}}{\beta_k}                                                                                                                                                                             \\
                    & =\sum_i -\pdv*{\biggl[(y_i-\mu_i)W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\biggr]}{\beta_k}                                                                                                                      \\
                    & =\sum_i -(y_i-\mu_i)\Biggl\{\pdv*{\biggl[W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\biggr]}{\beta_k}\Biggr\}-W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\biggl(\textcolor{Red}{\pdv*{(y_i-\mu_i)}{\beta_k}}\biggr) \\
                    & =\sum_i -(y_i-\mu_i)\Biggl\{\pdv*{\biggl[W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\biggr]}{\beta_k}\Biggr\}+W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\textcolor{Red}{\pdv{\mu_i}{\eta_i}\pdv{\eta_i}{\beta_k}}  \\
                    & =\sum_i -(y_i-\mu_i)\pdv*{\biggl[W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\biggr]}{\beta_k}+x_{ij}W_i x_{ik}.
\end{align*}
\subsection*{Fisher Information}
To get an element of the Expected/Fisher Information matrix:
\begin{align*}
    \MatrixCal{I}_{jk}
     & =\sum_i\E*{-\pdv{\ell}{\beta_j,\beta_k}}                                                                                                \\
     & =\sum_i\E*{-(Y_i-\mu_i)\pdv*{\biggl[W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\biggr]}{\beta_k}+x_{ij}W_i x_{ik}}                       \\
     & =\sum_i-\textcolor{Red}{\E[\big]{(Y_i-\mu_i)}}\pdv*{\biggl[W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\biggr]}{\beta_k}+x_{ij}W_i x_{ik} \\
     & =\sum_i x_{ij}W_i x_{ik}.
\end{align*}
Therefore, we can write the $(j, k)$ element of the Fisher information as:
\[ \MatrixCal{I}_{jk}=\sum_{i=1}^{n} x_{ij}W_i x_{ik}=[\Matrix{X}\Matrix{\MatrixCal{W}}\Matrix{X}^\top]_{jk} \]
where again, $ \MatrixCal{W}=\diag{W_1,\ldots,W_n} $ and $ W_i^{-1}=\Var{Y_i}\pdv{\eta_i}{\mu_i}^2 $.
\subsection*{When is Fisher Scoring Equivalent to Newton Raphson?}
Recall information matrix:
\[  \Matrix{I}_{jk}=\sum_i -(y_i-\mu_i)\pdv*{\biggl[\textcolor{Red}{W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}}\biggr]}{\beta_k}+x_{ij}W_i x_{ik}. \]
Now examine:
\begin{align*}
    W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}
     & =\Biggl(\Var{Y_i}\biggl(\pdv{\eta_i}{\mu_i}\biggr)^{\!2}\Biggr)^{\!-1}\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}                                                                                                        \\
     & =\Biggl(a(\phi)b^{\prime\prime}(\theta_i)\pdv{\eta_i}{\mu_i}\Biggr)^{\!-1}x_{ij}                              &  & \text{since $\Var{Y_i}=a_i(\phi)b^{\prime\prime}(\theta_i)$}                                      \\
     & =\Biggl(a(\phi)\pdv{\mu_i}{\theta_i}\pdv{\eta_i}{\mu_i} \Biggr)^{\!-1}x_{ij}                                  &  & \text{since $ b^{\prime}(\theta_i)=\mu_i $, $ b^{\prime\prime}(\theta_i)=\pdv{\mu_i}{\theta_i} $} \\
     & =\bigl(a(\phi)\bigr)^{-1}x_{ij}                                                                               &  & \text{\textcolor{Red}{under the canonical link $\theta_i=\eta_i$.}}
\end{align*}
So under the \textcolor{Red}{canonical link},
\[ \pdv*{\biggl[W_i\biggl(\pdv{\eta_i}{\mu_i}\biggr)x_{ij}\biggr]}{\beta_k}=\pdv*{\Bigl[\textcolor{Red}{\bigl(a(\phi)\bigr)^{-1}x_{ij}}\Bigr]}{\beta_k}=0, \]
therefore information matrix is same as the Fisher information:
\[ \Matrix{I}_{jk}=\sum_{i}x_{ij}W_i x_{ij}=\MatrixCal{I}_{jk}  \]
and Fisher Scoring is equivalent to Newton Raphson.
\subsection*{Iteratively Reweighted Least Squares}
The Fisher Scoring is also called \textcolor{Red}{iteratively reweighted least squares} (IRWLS).
The reason is that the update equation can be rewritten as:
\[ \hat{\Vector{\beta}}^{(r+1)}=\Bigl(\Matrix{X}\MatrixCal{W}\bigl(\hat{\Vector{\beta}}^{(r)}\bigr)\Matrix{X}^\top\Bigr)^{\!-1}\Matrix{X}\MatrixCal{W}\bigl(\hat{\Vector{\beta}}^{(r)}\bigr)\RandomVector{Z}\bigl(\hat{\Vector{\beta}}^{(r)}\bigr) \]
where $ \RandomVector{Z} $ is a transformation of the response vector $ \RandomVector{Y} $ such that:
\[ \RandomVector{Z}=\Vector{\eta}+(\RandomVector{Y}-\Vector{\mu})\ast\pdv{\Vector{\eta}}{\Vector{\mu}} \]
\begin{itemize}
    \item See manipulation in Section 1.2.3 of course notes.
    \item Same form as the weighted LS estimate of $ \Vector{\beta} $ with dependent variable $ \RandomVector{Z} $
          and weight matrix $ \MatrixCal{W} $.
    \item $ \RandomVector{Z} $ and $ \MatrixCal{W} $ are updated at each iteration.
\end{itemize}
\subsection*{Topic Summary}
2b Maximum Likelihood Estimation of Generalized Linear Models:
\begin{itemize}
    \item When $ Y_i $ come from a distribution in the \textcolor{Red}{exponential family}, we can use
          the theory of \textcolor{Red}{Generalized Linear Models} to fit the regression equations of
          the form:
          \[ g(\mu_i)=\Vector{x}_i^\top \Vector{\beta}. \]
    \item The \textcolor{Red}{link function} $ g(\:\cdot\:) $ may be the canonical link, but its choice should
          come from model interpretation and fit.
    \item Can use Fisher Scoring (also known as IRWLS) to estimate the regression parameters $ \Vector{\beta} $ from any GLM based on general forms for $ \Matrix{I}(\Vector{\beta}) $
          and $ \Vector{S}(\Vector{\beta}) $.
    \item \textcolor{Blue}{\textsc{Practice}}: Chapter 1 review problems.
\end{itemize}

\section*{Topic 3a: Binary Data and Odds Ratios}
\addcontentsline{toc}{section}{Topic 3a: Binary Data and Odds Ratios}
\subsection*{Binary Data Set-up}
\begin{itemize}
    \item Consider the simplest case with two \emph{binary} variables:
          \begin{itemize}
              \item COVID-19: infected or not infected (response).
              \item Vaccination: yes or no (explanatory variable).
          \end{itemize}
    \item Use a $ 2\times 2 $ table to summarize the data:
          \begin{table}[!htbp]
              \centering
              \begin{NiceTabular}{c|cc|c}
                  & \multicolumn{2}{c}{\emph{COVID-19}}                                                 \\
                  Vaccination & infected                            & not infected                                        \\
                  \midrule
                  yes & $ y_1 $                            & $ m_1-y_1 $                 & $ m_1 $         \\
                  no   & $ y_2 $                            & $ m_2-y_2 $                 & $ m_2 $         \\
                  \midrule
                  Total& $ y_{\bullet} $                    & $ m_{\bullet}-y_{\bullet} $ & $ m_{\bullet} $
              \end{NiceTabular}
          \end{table}
    \item Treat $ m_1 $ and $ m_2 $ as fixed, assume $ Y_1 $ and $ Y_2 $ are independent binomial r.v.'s
          \[ \textcolor{Red}{Y_k \sim \BIN{m_k,\pi_k},\qquad k=1,2,} \]
          where $ \pi_k=\Prob{\text{infection}\given \text{group $ k $}} $.
    \item How do we measure the associate between COVID-19 infection and vaccination?
\end{itemize}

\subsection*{Measures of Association}
\begin{Regular}{Definition (Odds Ratio)}
    The \textcolor{Red}{Odds Ratio} (OR) is the ratio of the odds of an event occurring in one
    group to the odds of the event in another group (e.g., not vaccinated):
    \[ \text{Odds Ratio}=\frac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)}. \]
\end{Regular}
Interpretation of OR:
\[ \begin{array}{ccccc}
        \pi_1=\pi_2 & \implies & \OR=1   & \implies & \text{equal risk (no association)} \\
        \pi_1>\pi_2 & \implies & \OR>1   & \implies & \text{higher risk in group 1}      \\
        \pi_1<\pi_2 & \implies & 0<\OR<1 & \implies & \text{higher risk in group 2}
    \end{array} \]
\begin{Regular}{Relative Risk (RR)}
    The \textcolor{Red}{Relative Risk} (RR) is the ratio of the probability of an event occurring in one group versus another group:
    \[ \text{Relative Risk}=\frac{\pi_1}{\pi_2} \]
\end{Regular}
In the case of a \textcolor{Red}{rare disease} (i.e., when $ \pi_1 $ and $ \pi_2 $ are very small),
\[ \OR=\frac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)}=\frac{\pi_1}{\pi_2}\underbrace{\biggl(\frac{1-\pi_2}{1-\pi_1}\biggr)}_{\approx 1}\approx \frac{\pi_1}{\pi_2} =\RR,  \]
then
\[ \OR\approx\RR. \]
\subsection*{Maximum Likelihood Estimation of Odds Ratio}
\begin{itemize}
    \item Goal: Estimate odds ratio $ \psi=\frac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)} $ using likelihood method. Based
          on ``grouped'' binomial data, $ \textcolor{Red}{Y_k \sim \BIN{m_k,\pi_k},\; k=1,2} $,
          \begin{align*}
              L(\pi_1,\pi_2)
               & =\binom{m_1}{y_1}\pi_1^{y_1}(1-\pi_1)^{m_1-y_1}\binom{m_2}{y_2}\pi_2^{y_2}(1-\pi_2)^{m_2-y_2}                                                        \\
               & \propto\biggl(\frac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)} \biggr)^{\!y_1}\biggl(\frac{\pi_2}{1-\pi_2}\biggr)^{\! y_2+y_1}(1-\pi_1)^{m_1}(1-\pi_2)^{m_2}.
          \end{align*}
    \item Note that $ \pi_1,\pi_2\in[0,1] $ and odds ratio $ \psi\in(0,\infty) $ are restricted, we consider re-parameterize:
          \[ \theta_1=\log*{\frac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)}}=\log{\psi},\qquad \theta_2=\log*{\frac{\pi_2}{1-\pi_2}}, \]
          and now $ \theta_1,\theta_2\in(-\infty,\infty) $.
    \item Our re-parameterization implies:
          \[ \pi_1=\frac{\mathrm{e}^{\theta_1+\theta_2}}{1+\mathrm{e}^{\theta_1+\theta_2}},\qquad \pi_2=\frac{\mathrm{e}^{\theta_2}}{1+\mathrm{e}^{\theta_2}}. \]
          Now the likelihood becomes:
          \begin{align*}
              L(\theta_1,\theta_2)    & =(\mathrm{e}^{\theta_1})^{y_1}(\mathrm{e}^{\theta_2})^{y_1+y_2}(1+\mathrm{e}^{\theta_1+\theta_2})^{m_1}(1+\mathrm{e}^{\theta_2})^{-m_2}, \\
              \ell(\theta_1,\theta_2) & =y_1\theta_1+(y_1+y_2)\theta_2-m_1\log{1+\mathrm{e}^{\theta_1+\theta_2}}-m_2\log{1+\mathrm{e}^{\theta_2}}.
          \end{align*}
    \item The score vector is:
          \[ S(\theta_1,\theta_2)=\begin{pmatrix}
                  \pdv{\ell}{\theta_1} \\
                  \pdv{\ell}{\theta_2}
              \end{pmatrix}=\begin{pmatrix}
                  y_1-m_1\biggl(\frac{\mathrm{e}^{\theta_1+\theta_2}}{1+\mathrm{e}^{\theta_1+\theta_2}} \biggr) \\
                  y_1+y_2-m_1\biggl(\frac{\mathrm{e}^{\theta_1+\theta_2}}{1+\mathrm{e}^{\theta_1+\theta_2}} \biggr)-m_2\biggl(\frac{\mathrm{e}^{\theta_2}}{1+\mathrm{e}^{\theta_2}} \biggr)
              \end{pmatrix}. \]
    \item Solving $ \Vector{S}(\theta_1,\theta_2)=\Vector{0} $ gives us the MLEs:
          \[ \hat{\theta}_1=\log*{\frac{y_1/(m_1-y_1)}{y_2/(m_2-y_2)}},\qquad \hat{\theta}_2=\log*{\frac{y_2}{m_2-y_2}}. \]
    \item So by the invariance property of MLEs, we have:
          \[ \hat{\pi}_1=\frac{y_1}{m_1},\qquad \hat{\pi}_2=\frac{y_2}{m_2},\qquad\hat{\psi}=\frac{\hat{\pi}_1/(1-\hat{\pi}_1)}{\hat{\pi}_2/(1-\hat{\pi}_2)}=\frac{y_1/(m_1-y_1)}{y_2/(m_2-y_2)}.  \]
\end{itemize}
\subsection*{Inference for Odds Ratio}
\begin{itemize}
    \item In order to do inference we will need the Information Matrix:
          \[ \Matrix{I}(\theta_1,\theta_2)=\begin{bmatrix}
                  I_{11} & I_{12} \\
                  I_{21} & I_{22}
              \end{bmatrix}\qquad\text{where }I_{jk}=-\pdv*{\ell(\theta_1,\theta_2)}{\theta_j,\theta_k}. \]
          Here, we have:
          \begin{align*}
              I_{11}          & =m_1\biggl(\frac{\mathrm{e}^{\theta_1+\theta_2}}{(1+\mathrm{e}^{\theta_1+\theta_2})^2} \biggr),                                                                             \\
              I_{12}  =I_{21} & =m_1\biggl(\frac{\mathrm{e}^{\theta_1+\theta_2}}{(1+\mathrm{e}^{\theta_1+\theta_2})^2} \biggr),                                                                             \\
              I_{22}          & =m_1\biggl(\frac{\mathrm{e}^{\theta_1+\theta_2}}{(1+\mathrm{e}^{\theta_1+\theta_2})^2} \biggr)+m_2\biggl(\frac{\mathrm{e}^{\theta_2}}{(1+\mathrm{e}^{\theta_2})^2} \biggr).
          \end{align*}
    \item We are interested in doing inference on $ \theta_1=\log{\psi} $ (while $ \theta_2 $ is nuisance).
    \item Recall the asymptotic distribution result of a \textcolor{Red}{Wald statistic}:
\end{itemize}
\begin{Regular}{Wald Statistic}
    For a vector $ \Vector{\theta}=(\theta_1,\theta_2)^\top $ where $ \theta_1=\log{\psi} $ is a scalar parameter of interest:
    \[ (\hat{\theta}_1-\theta_1)^2\bigl(I^{11}(\hat{\theta}_1,\hat{\theta}_2)\bigr)^{-1}\sim \chi^2_{(1)},
        \text{ or }(\hat{\theta}_1-\theta)/\sqrt{I^{11}} \sim \N{0,1}, \]
    where $ I^{11} $ is the $ (1,1) $ element of $ \Matrix{I}^{-1} $ evaluated at MLE $ \hat{\theta}_1 $ and $ \hat{\theta}_2 $.
\end{Regular}
\begin{itemize}
    \item Calculation of $ I^{11} $ by using a general result:
          \[ \Matrix{I}=\begin{pmatrix}
                  I_{11} & I_{12} \\
                  I_{21} & I_{22}
              \end{pmatrix},\qquad \Matrix{I}^{-1}=\begin{pmatrix}
                  \textcolor{Red}{I^{11}} & I^{12} \\
                  I^{21}                  & I^{22}
              \end{pmatrix},\qquad \textcolor{Red}{I^{11}}=\bigl(I_{11}-I_{12}I_{22}^{-1}I_{21}\bigr)^{-1}. \]
    \item We can use the Wald result to find a confidence interval for $ \theta_1=\log{\psi} $.
\end{itemize}
\subsection*{Confidence Interval for Odds Ratio}
Here, we obtain:
\[ I^{11}(\hat{\theta}_1,\hat{\theta}_2)=\frac{1}{y_1} +\frac{1}{m_1-y_1}+\frac{1}{y_2}+\frac{1}{m_2-y_2}. \]
Thus, a Wald-based \qty{95}{\percent} confidence interval for $ \theta_1=\log{\psi} $ is:
\[ \hat{\theta}_1\pm 1.96\sqrt{\frac{1}{y_1} +\frac{1}{m_1-y_1} +\frac{1}{y_2} +\frac{1}{m_2-y_2}}=(\hat{\theta}_{1\text{L}},\hat{\theta}_{1\text{U}}). \]
A \qty{95}{\percent} confidence interval for the Odds Ratio $ \psi $ is:
\[ \bigl(\exp{\hat{\theta}_{1\text{L}}},\exp{\hat{\theta}_{1\text{U}}}\bigr). \]
\subsection*{Example: Prenatal Care from Two Clinics}
Consider the data below for the relationship between:
\begin{itemize}
    \item \textcolor{Blue}{Response}: Fetal Mortality.
    \item \textcolor{Blue}{Explanatory variable}: Level of Care.
\end{itemize}
\begin{table}[!htbp]
    \centering
    \begin{NiceTabular}{l|cc|c}
        & \multicolumn{2}{c}{\emph{Fetal Mortality}}                                                 \\
        Level of Care & Died                            & Survived     & Total                                   \\
        \midrule
        Intensive & $ 20 $                            & $ 316 $                 & $ 336 $         \\
        Regular   & $ 46 $                            & $ 373 $                 & $ 419 $         \\
        \midrule
        & $ 66 $                    & $ 689 $ & $ 755 $
    \end{NiceTabular}
\end{table}
\begin{itemize}
    \item Using the above data, we obtain MLE of odds ratio $ \psi $:
          \[ \hat{\psi}=\frac{y_1/(m_1-y_1)}{y_2/(m_2-y_2)}=\frac{20/316}{46/373}=0.51. \]
          $ \hat{\psi}=0.51<1 $, the risk of mortality is lower with intensive care.
    \item A \qty{95}{\percent} CI for $ \theta_1=\log{\psi} $:
          \[ \log{0.51}\pm 1.96\sqrt{\frac{1}{20}+\frac{1}{316}+\frac{1}{46}+\frac{1}{373}}=(-1.219,-0.127). \]
    \item A \qty{95}{\percent} CI for odds ratio $ \psi $:
          \[ \bigl(\exp{-1.219},\exp{-0.127}\bigr)=(0.30,0.89). \]
          Note that the CI does not cover the value $ \psi=1 $ (no association), so we reject the null
          hypothesis of no association between fetal mortality and level of care. In other words,
          there is evidence of association.
\end{itemize}
\subsection*{Example: Prenatal Care from Two Clinics}
There is an \textcolor{Blue}{additional explanatory variable}: Clinic (A vs B).
\begin{Example}{Prenatal Care Data Stratified by Clinic}
    \begin{center}
        \begin{NiceTabular}{l|cc|c|cc|c}
            & \multicolumn{3}{c}{\emph{Clinic A}} & \multicolumn{3}{c}{\emph{Clinic B}} \\
            Level of Care & Died                            & Survived & Total    & Died & Survived & Total                                    \\
            \midrule
            Intensive & $ 16 $                            & $ 293 $                 & $ 309 $ & $ 4 $ & $ 23 $ & $ 27 $        \\
            Regular   & $ 12 $                            & $ 176 $                 & $ 188 $ & $ 34 $ & $ 197 $ & $ 231 $        \\
            \midrule
            & $ 28 $                    & $ 469 $ & $ 497 $ & $ 38 $ & $ 220 $ & $ 258 $
        \end{NiceTabular}
    \end{center}
\end{Example}
\begin{itemize}
    \item $ \hat{\psi}_\text{A}=0.80\; (0.37,1.73) $ and $ \hat{\psi}_\text{B}=1.01\;(0.33,3.10) $.
          These cover value $ 1 $, different from the results from the pooled analysis on the previous slide.
    \item These results do NOT agree with the results from the pooled analysis on
          the previous slide.
\end{itemize}
\begin{Example}{Association Between Clinic and Level of Care}
    \begin{center}
        \begin{NiceTabular}{l|cc|c}
            & A                            & B &                                         \\
            \midrule
            Intensive & $ 309 $                            & $ 27 $                 & $ 336 $         \\
            Regular   & $ 118 $                            & $ 231 $                 & $ 419 $         \\
            \midrule
            & $ 497 $                    & $ 258 $ & $ 755 $
        \end{NiceTabular}
    \end{center}
\end{Example}
\begin{itemize}
    \item $ \hat{\psi}=14.06\;(9.12,21.76) $.
\end{itemize}
\begin{Example}{Association Between Clinic and Mortality}
    \begin{center}
        \begin{NiceTabular}{l|cc|c}
            & A                            & B &                                         \\
            \midrule
            Died & $ 28 $                            & $ 38 $                 & $ 66 $         \\
            Survived   & $ 469 $                            & $ 220 $                 & $ 689 $         \\
            \midrule
            & $ 497 $                    & $ 258 $ & $ 755 $
        \end{NiceTabular}
    \end{center}
\end{Example}
\begin{itemize}
    \item $ \hat{\psi}=0.35\;(0.21,0.58) $.
\end{itemize}
\begin{itemize}
    \item The initial strong association between Level of Care and Infant Morality
          disappeared when we stratified by clinic.
          \begin{figure}[!htbp]
              \centering
              \begin{tikzpicture}[thick]
                  \node (1) at (0,1) {Clinic};
                  \node (2) at (-2,0) {Level of Care};
                  \node (3) at (2,0) {Mortality};
                  \draw[<->] (1) to (2);
                  \draw[<->] (1) to (3);
                  \draw[->,Purple] (2) to (3) node[anchor=south,midway] {$?$};
              \end{tikzpicture}
          \end{figure}
    \item Instead of having to examine multiple $ 2\times 2 $ tables we'd like to estimate the $ \OR $
          and compute associations using a multiple regression model.
    \item One way to do this is by fitting a Binomial GLM to the data.
\end{itemize}
% Week 4

\makeheading{Week 4}{\daterange{2021-09-27}{2021-10-01}}
\section*{Topic 3b: Binomial Regression Models for Binary Data}
\addcontentsline{toc}{section}{Topic 3b: Binomial Regression Models for Binary Data}
\subsection*{Recall Topic 3a: Binary Data and Odds Ratios}
Last week, we introduce a simple method for association between two binary
variables, \textcolor{Blue}{$ 2\times 2 $ contingency table analysis}:
\begin{table}[!htbp]
      \centering
      \begin{NiceTabular}{l|ccc}
            & \multicolumn{2}{c}{\emph{Mortality}}                                                 \\
            Level of Care & Died                            & Survived                                 \\
            \cmidrule{1-3}
            Intensive & $ y_1 $                            & $ m_1-y_1 $    & $ Y_1 \sim \BIN{m_1,\pi_1} $         \\
            Regular   & $ y_2 $                            & $ m_2-y_2 $    & $ Y_2 \sim \BIN{m_2,\pi_2} $      \\
            \cmidrule{1-3}
      \end{NiceTabular}
\end{table}
Measure of Association: $ \displaystyle \OR = \psi=\frac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)} $,
\begin{itemize}
      \item $ \OR=1 $ (equal risk).
      \item $ 0<\OR<1 $ (lower risk in group 1).
      \item $ \OR>1 $ (higher risk in group 1).
\end{itemize}
Maximum likelihood estimator for $ \OR $ is:
\[ \hat{\psi}=\frac{y_1/(m_1-y_1)}{y_2/(m_2-y_2)}, \]
and a Wald-based \qty{95}{\percent} CI is:
\[ \exp[\bigg]{\log{\hat{\psi}_1}\pm 1.96\underbrace{\sqrt{\frac{1}{y_1} +\frac{1}{m_1-y_1} +\frac{1}{y_2} +\frac{1}{m_2-y_2}}}_{\se{\log{\hat{\psi}}}}}. \]
\subsection*{Prenatal Care Data Example}
\begin{table}[!htbp]
      \centering
      \begin{tabular}{ccc}
            OR (\textcolor{Blue}{Mortality and Care}) & Est.                   & \qty{95}{\percent} CI             \\
            \midrule
            Intensive vs Regular                      & \textcolor{Blue}{0.51} & \textcolor{Blue}{$ (0.30,0.89) $} \\
            \bottomrule
      \end{tabular}
      \caption{$ 1\notin (0.30,0.89)\implies $ evidence of association between Mortality and Care.}
\end{table}

However, Mortality and Care are also related to another variable, Clinic:
\begin{table}[!htbp]
      \centering
      \begin{tabular}{ccc}
            OR (\textcolor{Blue}{Mortality and Clinic}) & Est.                   & \qty{95}{\percent} CI             \\
            \midrule
            Clinic A vs Clinic B                        & \textcolor{Blue}{0.35} & \textcolor{Blue}{$ (0.12,0.58) $} \\
            \bottomrule
      \end{tabular}
      \caption{Association between Mortality and Clinic.}
\end{table}
\begin{table}[!htbp]
      \centering
      \begin{tabular}{ccc}
            OR (\textcolor{Blue}{Care and Clinic}) & Est.                    & \qty{95}{\percent} CI              \\
            \midrule
            Clinic A vs Clinic B                   & \textcolor{Blue}{14.06} & \textcolor{Blue}{$ (9.12,21.76) $} \\
            \bottomrule
      \end{tabular}
      \caption{Association between Care and Clinic.}
\end{table}
\begin{itemize}
      \item Therefore, we wish to consider how a variable, e.g., Mortality ($ Y $), is related to
            multiple explanatory variables together, e.g., Care ($ x_1 $) and Clinic ($ x_2 $).
      \item This can be done using \textcolor{Blue}{multiple regression methodology} for binary data $ \implies $
            Topic 3b: Binomial Regression Models for Binary Data.
\end{itemize}
\subsection*{Multiple Regression for Binary Data}
\begin{itemize}
      \item Often we need to consider the relationship between a binary outcome and
            multiple explanatory variables, using multiple regression methodology.
      \item This is because we may want to:
            \begin{itemize}
                  \item control for cofounding variables and hence want to examine the effect of
                        several variables simultaneously;
                  \item examine the effect of categorical variables ($ >2 $ levels) or continuous covariates;
                  \item develop sophisticated models that describe complex relationship.
            \end{itemize}
      \item Suppose \textcolor{Blue}{\emph{subject level data}} is binary with a value of 1 indicating that an event
            of interest occurs and a value of 0 indicating that event doesn't occur.
      \item Subjects can be classified according to the values of explanatory
            variables into $n$ groups (i.e., common covariates values within each group), so
            we have \textcolor{Blue}{\emph{grouped data}} such that:
            \begin{itemize}
                  \item $ m_i $ denotes number of subjects in group $i$;
                  \item $Y_i$ denotes number of subjects experienced the event in group $i$;
                  \item $ x_{i1},\ldots,x_{ip} $ denote the covariates values associated with group $i$
                        where $ i=1,\ldots,n $.
            \end{itemize}
\end{itemize}
\subsection*{Set-up of a Binomial Regression Model}
\begin{enumerate}[label=\color{Blue}\protect\circled{\arabic*}]
      \item \textcolor{Blue}{Response Variable}: $ Y_i \sim \BIN{m_i,\pi_i} $, $ i=1,\ldots,n $, and Binomial
            distribution is a member of Exponential family!
            \begin{align*}
                  f(y_i)
                   & =\binom{m_i}{y_i}\pi_i^{y_i}(1-\pi_i)^{m_i-y_i}                                   \\
                   & =\exp*{y_i\log*{\frac{\pi_i}{1-\pi_i}}+m_i\log{1-\pi_i}+\log*{\binom{m_i}{y_i}}},
            \end{align*}
            where
            \begin{align*}
                  \theta_i     & =\log*{\frac{\pi_i}{1-\pi_i}},                       \\
                  a(\phi)=\phi & =1,                                                  \\
                  b(\theta_i)  & =-m_i\log{1-\pi_i}=m_i\log{1+\mathrm{e}^{\theta_i}}. \\
                  c(y_i;\phi)  & =\log*{\binom{m_i}{y_i}}.
            \end{align*}
      \item \textcolor{Blue}{Linear Predictor}:
            \[ \eta_i=\Vector{x}_i^\top \Vector{\beta}=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip}. \]
      \item \textcolor{Blue}{Link Function}: Recall that for Binomial distribution, we have $ \E{Y_i}=\mu_i=m_i\pi_i $,
            therefore we typically re-write the link function in terms of $ \pi_i $,
            \[ \textcolor{Blue}{g(\pi_i)}=\Vector{x}_i^\top \Vector{\beta}. \]
            As $ \pi_i\in(0,1) $, any function $ g\colon (0,1)\to(-\infty,\infty) $ may work, and here are some link functions we
            can consider:
            \begin{table}[!htbp]
                  \centering
                  \begin{tabular}{cc}
                        \toprule
                        log-log                             & $ g(\pi)=\log[\big]{-\log{\pi}} $                                               \\
                        complementary log-log               & $ g(\pi)=\log[\big]{-\log{1-\pi}} $                                             \\
                        Probit$^a$                          & $ g(\pi)=\Phi^{-1}(\pi) $                                                       \\
                        Logit (\textcolor{Blue}{canonical}) & $ g(\pi)=\log[\big]{\pi/(1-\pi)} $                                              \\
                        \bottomrule
                        \multicolumn{2}{l}{\footnotesize{$ {}^a $For the Probit link, $ \Phi(\:\cdot\:) $ is the \emph{CDF} of $ \N{0,1} $.}} \\
                  \end{tabular}
            \end{table}
\end{enumerate}
\subsection*{Canonical Link and Logistic Regression}
Recall for Binomial distribution $ \theta_i=\log*{\frac{\pi_i}{1-\pi_i} } $, and by setting $ \theta_i=\eta_i $, we have:
\[ \log*{\frac{\pi_i}{1-\pi_i}}=\eta_i. \]
The \textcolor{Blue}{Logit link}, $ g(\pi_i)=\log[\big]{\pi_i/(1-\pi_i)} $, is the canonical link for the Binomial!
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-22-1} 

}


\end{knitrout}
% \end{noindent}
This leads to a Logistic Regression Model:
\[ \log*{\frac{\pi_i}{1-\pi_i}}=\Vector{x}_i^\top \Vector{\beta}=\beta_0+\beta_1x_{i1}+\cdots+\beta_p x_{ip}. \]
\subsection*{Prediction from Logistic Regression}
Aside: The inverse of the logit function is called the expit function:
\[ \logit{\pi_i}=\log*{\frac{\pi_i}{1-\pi_i}}=\Vector{x}_i^\top \Vector{\beta}\iff \pi_i=\frac{\exp{\Vector{x}_i^\top \Vector{\beta}}}{1+\exp{\Vector{x}_i^\top \Vector{\beta}}}=\expit{\Vector{x}_i^\top \Vector{\beta}}.  \]
Suppose we have found MLE $ \hat{\Vector{\beta}} $ using Fisher scoring, then the fitted value for the \textcolor{Blue}{probability of response} $ \pi_i $ given explanatory
variables $ \Vector{x}_i $ is:
\[ \hat{\pi}_i=\frac{\exp{\Vector{x}_i^\top \hat{\Vector{\beta}}}}{1+\exp{\Vector{x}_i^\top \hat{\Vector{\beta}}}}. \]
The predicted number of responses are: $ \hat{Y}_i=m_i\hat{\pi}_i $.

\subsection*{Interpretation of $ \Vector{\beta} $ in Logistic Regression}
\begin{itemize}
      \item Consider a simple logistic model with a single binary explanatory variable:
            \[ \log*{\frac{\pi_i}{1-\pi_i}}=\beta_0+\beta_1x_{i1}, \]
            where $ x_{i1}=0 $ (group 0) and $ x_{i1}=1 $ (group 1).
      \item Let's compare the model when $ x_{i1}=1 $ vs $ x_{i1}=0 $.
            \begin{table}[!htbp]
                  \centering
                  \begin{tabular}{ccrl}
                        Group & $ \Vector{x}_i^\top $ & $ \eta_i $          & $ =\log[\big]{\pi_i/(1-\pi_i)} $                              \\
                        \midrule
                        1     & $ (1,1)^\top $        & $ \beta_0+\beta_1 $ & $ =\log[\big]{\pi_1/(1-\pi_1)} $                              \\
                        0     & $ (1,0)^\top $        & $ \beta_0 $         & $ =\log[\big]{\pi_0/(1-\pi_0)} $                              \\
                        \midrule
                              &                       & $ \beta_1 $         & $ =\log*{\frac{\pi_1/(1-\pi_1)}{\pi_0/(1-\pi_0)}}=\log{\OR} $
                  \end{tabular}
            \end{table}
      \item We subtract line 2 from line 1 to isolate $ \beta_1 $ and find its interpretation.
      \item $ \beta_1= $ log odds ratio of response for subjects with $ x_{i1}=1 $ vs $ x_{i1}=0 $.
      \item Please see Section 2.4.2 for general interpretations of $ \Vector{\beta} $'s in multiple logistic regression models.
\end{itemize}
\subsection*{Logistic Regression for Prenatal Care Example}
\begin{itemize}
      \item \textcolor{Blue}{Response}: Fetal Mortality, that is,
            \[ Y_i \sim \BIN{m_i,\pi_i},\; i=1,2,\ldots. \]
      \item Explanatory Variables:
            \begin{align*}
                  x_{i1} & =\begin{cases*}
                                  1 & Intensive Care \\
                                  0 & Regular Care
                            \end{cases*}                           \\
                  x_{i2} & =\begin{cases*}
                                  1 & Clinic A \\
                                  0 & Clinic B \\
                            \end{cases*}                                \\
                  x_{i3} & =x_{i1}x_{i2}=\begin{cases*}
                                               1 & Intensive care and Clinic A \\
                                               0 & Otherwise
                                         \end{cases*}
            \end{align*}
      \item We will use the context of this example to illustrate how to:
            \begin{itemize}
                  \item fit (simple and multiple) logistic regression models using R, and
                  \item interpret regression parameters.
            \end{itemize}
\end{itemize}
\subsection*{Model 1: Level of Care only model}
\[ \log*{\frac{\pi_i}{1-\pi_i}}=\beta_0+\beta_1x_{i1}. \]
\begin{table}[!htbp]
      \centering
      \begin{tabular}{cccl}
            Level of Care & Clinic & $ \Vector{x}_i^\top $ & $ \log[\big]{\pi_i/(1-\pi_i)} $ \\
            \midrule
            Intensive     & ---    & $ (1,1)^\top $        & $ \beta_0+\beta_1 $             \\
            Regular       & ---    & $ (1,0)^\top $        & $ \beta_0 $                     \\
            \bottomrule
      \end{tabular}
\end{table}
\begin{itemize}
      \item $ \beta_0= $ \textcolor{Blue}{log odds} of mortality for babies born to mothers treated with regular care.
      \item $ \beta_1= $ \textcolor{Blue}{log odds ratio} of mortality for babies born to mothers treated
            with intensive vs regular care.
\end{itemize}
\subsection*{Model 2: Main effects model}
\[ \log*{\frac{\pi_i}{1-\pi_i}}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}. \]
\begin{table}[!htbp]
      \centering
      \begin{tabular}{cccl}
            Level of Care & Clinic & $ \Vector{x}_i^\top $ & $ \log[\big]{\pi_i/(1-\pi_i)} $ \\
            \midrule
            Intensive     & A      & $ (1,1,1)^\top $      & $ \beta_0+\beta_1+\beta_2 $     \\
            Regular       & A      & $ (1,0,1)^\top $      & $ \beta_0+\beta_2 $             \\
            Intensive     & B      & $ (1,1,0)^\top $      & $ \beta_0+\beta_1 $             \\
            Regular       & B      & $ (1,0,0)^\top $      & $ \beta_0 $                     \\
            \bottomrule
      \end{tabular}
\end{table}
\begin{itemize}
      \item $ \beta_0= $ \textcolor{Blue}{log odds} of mortality with regular care at Clinic B.
      \item $ \beta_1= $ \textcolor{Blue}{log odds ratio} of mortality for babies born to mothers treated with
            \textcolor{Blue}{intensity vs regular} care at the \emph{same clinic}.
      \item $ \beta_2= $ \textcolor{Blue}{log odds ratio} of mortality for babies born to mothers treated at
            \textcolor{Blue}{Clinic A vs Clinic B} at the \emph{same level of care}.
\end{itemize}
\subsection*{Model 3: Interaction model}
\[ \log*{\frac{\pi_i}{1-\pi_i}}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i3}. \]
\begin{table}[!htbp]
      \centering
      \begin{tabular}{cccl}
            Level of Care & Clinic & $ \Vector{x}_i^\top $ & $ \log[\big]{\pi_i/(1-\pi_i)} $     \\
            \midrule
            Intensive     & A      & $ (1,1,1)^\top $      & $ \beta_0+\beta_1+\beta_2+\beta_3 $ \\
            Regular       & A      & $ (1,0,1)^\top $      & $ \beta_0+\beta_2 $                 \\
            Intensive     & B      & $ (1,1,0)^\top $      & $ \beta_0+\beta_1 $                 \\
            Regular       & B      & $ (1,0,0)^\top $      & $ \beta_0 $                         \\
            \bottomrule
      \end{tabular}
\end{table}
\begin{itemize}
      \item $ \beta_1= $ \textcolor{Blue}{log odds ratio} of mortality for babies born to mothers treated with
            \textcolor{Blue}{intensity vs regular} care at \emph{Clinic B}.
      \item $ \beta_1+\beta_3= $ \textcolor{Blue}{log odds ratio} of mortality for babies born to mothers treated
            with \textcolor{Blue}{intensity vs regular} care at \emph{Clinic A}.
      \item $ \beta_2= $ \textcolor{Blue}{log odds ratio} of mortality for babies born to mothers treated at
            \textcolor{Blue}{Clinic A vs Clinic B} with \emph{regular} care.
      \item $ \beta_2+\beta_3= $ \textcolor{Blue}{log odds ratio} of mortality for babies born to mothers treated at
            \textcolor{Blue}{Clinic A vs Clinic B} with \emph{intensive} care.
      \item $ \beta_3 $ represents the \textcolor{Blue}{difference in log odds ratios}.
      \item If $ \beta_3=0 $ then association between mortality and level of care does not
            dependent on clinic.
      \item Equivalently, if $ \beta_3=0 $ then the association between mortality and clinic
            does not depend on level of care.
\end{itemize}
\begin{Example}{Data file \texttt{prenatal.dat}}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
  clinic loc  y   m
1      0   0 34 231
2      0   1  4  27
3      1   0 12 188
4      1   1 16 309
\end{verbatim}
\end{kframe}
\end{knitrout}
      \begin{itemize}
            \item The first line contains the variable names/labels.
            \item We are using indicator variables for the explanatory variables:
                  \begin{align*}
                        x_{i1} & =\text{\texttt{loc}}    &  & \text{(1 for Intensive, 0 for Regular)} \\
                        x_{i2} & =\text{\texttt{clinic}} &  & \text{(1 for Clinic A, 0 for Clinic B)}
                  \end{align*}
            \item The variable \texttt{y} records the number of deaths (events).
      \end{itemize}
\end{Example}
\subsection*{Fit GLMs using R}
The \texttt{glm()} function in R is used to fit the generalized linear models:
\[ \text{\texttt{fit = glm(formula, family = (link = ), data = )}}. \]
\begin{itemize}
      \item \texttt{formula}: a linear formula describing the model, e.g.,
            \[ \texttt{resp \textasciitilde{} loc + clinic}. \]
      \item \texttt{family}: a description of the exponential family distribution and link
            function to be used in the model, e.g.,
            \[ \text{\texttt{family = binomial, gaussian, poisson, Gamma, etc.}}. \]
            \[ \text{\texttt{link = logit, log, loglog, cloglog, identity, probit, etc.}}. \]
      \item The default is the canonical link.
\end{itemize}
\subsection*{R Code and Output for Analysis of Prenatal Care data}
For binomial data, we need to construct ``\texttt{resp}'' variable as the pair $ (y_i,m_i-y_i) $.
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# read file prenatal.data}
\hlstd{prenatal.dat} \hlkwb{=} \hlkwd{read.table}\hlstd{(}\hlstr{"prenatal.dat"}\hlstd{,} \hlkwc{header} \hlstd{= T)}
\hlcom{# construct the binomial response for the logistic regression}
\hlcom{# analysis}
\hlstd{prenatal.dat}\hlopt{$}\hlstd{resp} \hlkwb{=} \hlkwd{cbind}\hlstd{(prenatal.dat}\hlopt{$}\hlstd{y, prenatal.dat}\hlopt{$}\hlstd{m} \hlopt{-} \hlstd{prenatal.dat}\hlopt{$}\hlstd{y)}
\hlstd{prenatal.dat}
\end{alltt}
\begin{verbatim}
  clinic loc  y   m resp.1 resp.2
1      0   0 34 231     34    197
2      0   1  4  27      4     23
3      1   0 12 188     12    176
4      1   1 16 309     16    293
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
The logistic regression models are fit using the \texttt{glm()} commands like:
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# fit the logistic model using the glm function}
\hlstd{model1} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{loc,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),} \hlkwc{data} \hlstd{= prenatal.dat)}
\hlkwd{summary}\hlstd{(model1)}
\end{alltt}
\end{kframe}
\end{knitrout}
% \end{noindent}
\subsection*{Fit of Model 1: Level of Care Model}
\[ \log*{\frac{\pi_i}{1-\pi_i}}=\beta_0+\beta_1x_{i1}. \]
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# fit the logistic model using the glm function}
\hlstd{model1} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{loc,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),} \hlkwc{data} \hlstd{= prenatal.dat)}
\hlkwd{summary}\hlstd{(model1)}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
              Estimate Std. Error    z value     Pr(>|z|)
(Intercept) -2.0929370  0.1562692 -13.393150 6.630754e-41
loc         -0.6670729  0.2785400  -2.394891 1.662530e-02
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
Components of the \texttt{summary()} output for \texttt{glm} objects:
\begin{itemize}
      \item \textcolor{Blue}{\texttt{Estimate}}: the maximum likelihood estimates of the regression coefficients $ \hat{\beta}_0 $
            and $ \hat{\beta}_1 $.
      \item \textcolor{Blue}{\texttt{Std.\ Error}}: estimated standard errors, the square root of the diagonal
            of the inverse of the Information matrix:
            \[ \se{\hat{\beta}_j}=\sqrt{\bigl[\Matrix{I}^{-1}(\hat{\Vector{\beta}})\bigr]_{jj}}=\sqrt{I^{jj}(\hat{\Vector{\beta}})}. \]
      \item \textcolor{Blue}{\texttt{z value}}: Wald-type test statistics for testing the hypotheses:
            \begin{center}
                  $ \HN $: $ \beta_j=0 $ vs $ \HA $: $ \beta_j\ne 0 $.
            \end{center}
      \item \textcolor{Blue}{\texttt{Pr(>|z|)}}: $ p $-value for above Wald test.
\end{itemize}
For this model:
\begin{itemize}
      \item $ \beta_1 $ is the log odds ratio of mortality for infants born to mothers treated
            with intensive versus regular care.
\end{itemize}
\subsection*{Hypothesis test for $ \beta_j $}
\begin{itemize}
      \item We may wish to test:
            \begin{center}
                  $ \HN $: $ \beta_j=\beta^\star $ versus $ \HA $: $ \beta_j\ne \beta^\star $.
            \end{center}
      \item The general \textcolor{Blue}{Wald} result for a single parameter $ \beta_j $ is:
            \[ (\hat{\beta}_j-\beta^\star)^2\bigl(I^{jj}(\hat{\Vector{\beta}})\bigr)^{-1} \sim \chi^2_1, \]
            equivalently
            $ \displaystyle \frac{\hat{\beta}_j-\beta^\star}{\se{\hat{\beta}_j}}\sim \N{0,1} $
            where $ \se{\hat{\beta}_j}=\sqrt{I^{jj}(\hat{\Vector{\beta}})} $.
      \item We can find the $ p $-value of this test using:
            \[ p=2\Prob*{Z>\frac{\abs{\hat{\beta}_j-\beta^\star}}{\se{\hat{\beta}_j}}}. \]
      \item The \texttt{summary()} output gives the test statistics and $p$-values for testing
            \begin{center}
                  $ \HN $: $ \beta_j=0 $ vs $ \HA $: $ \beta_j\ne 0 $.
            \end{center}
\end{itemize}
\subsection*{Hypothesis test for $ \beta_1 $ from Model 1: Level of Care Model}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(model1)}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
              Estimate Std. Error    z value     Pr(>|z|)
(Intercept) -2.0929370  0.1562692 -13.393150 6.630754e-41
loc         -0.6670729  0.2785400  -2.394891 1.662530e-02
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\begin{itemize}
      \item We wish to test:
            \begin{center}
                  $ \HN $: $ \beta_1=0 $ vs $ \HA $: $ \beta_1\ne 0 $
            \end{center}
      \item Wald test:
            \[ z=\frac{\hat{\beta}_1-0}{\se{\hat{\beta}_1}}=\frac{-0.6671}{0.2785}=-2.3949   \]
      \item $ p $-value:
            \[ p=2\Prob{Z>\abs{-2.3949}}=0.0166<0.05 \]
      \item Therefore, we reject the null hypothesis that $ \beta_1=0 $.
\end{itemize}
\begin{itemize}
      \item Estimate of $ \OR $ for Mortality for Intensive vs Regular Care:
            \[ \hat{\psi}=\exp{\hat{\beta}_1}=\exp{-0.6670729}=0.51. \]
      \item Confidence Interval for OR:
            \begin{align*}
                  \exp[\big]{\hat{\beta}_1\pm 1.96\se{\hat{\beta}_1}}
                   & =\exp{-0.6671\pm 1.96(0.2785)} \\
                   & =(\exp{-1.2130},\exp{-0.1211}) \\
                   & =(0.30,0.89)
            \end{align*}
      \item The estimate and Wald \qty{95}{\percent} CI here match those found previously from
            the $ 2\times 2 $ table analysis. That is, the $ 2\times 2 $ table analysis is equivalent to
            a simple logistic regression with a single binary covariate.
\end{itemize}
\subsection*{Fit of Model 2: Main Effects Model}
\[ \log*{\frac{\pi_i}{1-\pi_i}}=\beta_0+\beta_1x_{i1}+\textcolor{Blue}{\beta_2x_{i2}}. \]
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{<-} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{loc} \hlopt{+} \hlstd{clinic,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),}
  \hlkwc{data} \hlstd{= prenatal.dat)}
\hlkwd{summary}\hlstd{(model2)}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
              Estimate Std. Error    z value     Pr(>|z|)
(Intercept) -1.7410476  0.1784691 -9.7554560 1.748132e-22
loc         -0.1503053  0.3301670 -0.4552402 6.489365e-01
clinic      -0.9862793  0.3089322 -3.1925427 1.410261e-03
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\begin{itemize}
      \item What is the OR for mortality for Intensive vs Regular Care, now controlling for
            Clinic?
            \[ \widehat{\OR}=\hat{\psi}=\exp{-0.1503}=0.86. \]
      \item \qty{95}{\percent} CI:
            \[ \exp{-0.1503\pm 1.96\times 0.3302}=(0.4505,1.6436). \]
\end{itemize}
\subsection*{Fit of Model 3: Interaction Model}
\[ \log*{\frac{\pi_i}{1-\pi_i}}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\textcolor{Blue}{\beta_3x_{i3}}. \]
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model3} \hlkwb{<-} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{loc} \hlopt{+} \hlstd{clinic} \hlopt{+} \hlstd{loc} \hlopt{*} \hlstd{clinic,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),}
  \hlkwc{data} \hlstd{= prenatal.dat)}
\hlkwd{summary}\hlstd{(model3)}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
                Estimate Std. Error     z value     Pr(>|z|)
(Intercept) -1.756843204  0.1857092 -9.46018403 3.074017e-21
loc          0.007643349  0.5726827  0.01334657 9.893513e-01
clinic      -0.928734141  0.3514300 -2.64272868 8.224091e-03
loc:clinic  -0.229649891  0.6949054 -0.33047646 7.410400e-01
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\begin{table}[!htbp]
      \centering
      \begin{tabular}{cccl}
            Level of Care & Clinic & $ \Vector{x}_i^\top $ & $ \log[\big]{\pi_i/(1-\pi_i)} $     \\
            \midrule
            Intensive     & A      & $ (1,1,1)^\top $      & $ \beta_0+\beta_1+\beta_2+\beta_3 $ \\
            Regular       & A      & $ (1,0,1)^\top $      & $ \beta_0+\beta_2 $                 \\
            Intensive     & B      & $ (1,1,0)^\top $      & $ \beta_0+\beta_1 $                 \\
            Regular       & B      & $ (1,0,0)^\top $      & $ \beta_0 $                         \\
            \bottomrule
      \end{tabular}
\end{table}
\begin{itemize}
      \item What is the OR for Mortality for Intensive vs Regular Care at Clinic A?
            \[ \OR=\psi=\exp{\beta_1+\beta_3}\implies \hat{\psi}=\exp{0.0076-0.2296}=0.8. \]
      \item $ \se{\hat{\beta}_1+\hat{\beta}_3} $ is required for calculation of \qty{95}{\percent} CI.
            \begin{itemize}
                  \item Recall $ \Var{\hat{\beta}}=\Matrix{I}^{-1}(\hat{\Vector{\beta}}) $, now for any linear function of $ \Vector{\beta} $'s,
                        e.g., $ \Vector{c}\Vector{\beta} $ where $ \Vector{c} $ is a row vector of constants, then MLE of $ \Vector{c}\Vector{\beta} $
                        is $ \Vector{c}\hat{\Vector{\beta}} $, and $ \se{\hat{\Vector{c}\hat{\Vector{\beta}}}}=\sqrt{\Vector{c}\Matrix{I}^{-1}(\hat{\Vector{\beta}})\Vector{c}^\top} $.
            \end{itemize}
      \item Therefore, $ \log{\psi}=\beta_1+\beta_3=\Vector{c}\Vector{\beta} $, $ \Vector{c}=(0,1,0,1) $. In R, \texttt{vcov(model3)}
            gives $ \Matrix{I}^{-1}(\hat{\Vector{\beta}}) $.
      \item What is OR for Mortality for Intensive vs Regular Care at Clinic B?
            \[ \OR=\psi=\exp{\beta_1}\implies \hat{\psi}=\exp{0.0076}=1.01. \]
\end{itemize}

\section*{Topic 3c: Likelihood Ratio Test for Logistic Regression Models}
\addcontentsline{toc}{section}{Topic 3c: Likelihood Ratio Test for Logistic Regression Models}
\subsection*{Logistic Regression Models}
Recall major developments of Binomial logistic regression from last topic 3b: $ Y_i \sim \BIN{m_i,\pi_i} $,
$ i=1,\ldots,n $ independently, with covariate vector $ \Vector{x}_i $ and
\[ \textcolor{Blue}{\log*{\frac{\pi_i}{1-\pi_i}}}=\Vector{x}_i^\top \Vector{\beta}. \]
\begin{itemize}
      \item Estimation: $ \hat{\Vector{\beta}} $ come from Fisher scoring using R function \texttt{glm()}.
      \item Interpretation: $ \exp{\beta_j} $ has $ \OR $ interpretation.
      \item Hypothesis tests of $ \HN $: $ \beta_j=0 $ using Wald statistic.
      \item Confidence Intervals: $ \hat{\beta}_j\pm z_{1-\alpha/2}\se{\hat{\beta}_j} $.
\end{itemize}
\subsection*{Likelihood for Logistic Regression Models}
\begin{itemize}
      \item Log-likelihood for Binomial Distribution:
            \begin{align*}
                  \ell
                   & =\log[\bigg]{\prod_{i=1}^n \pi_i^{y_i}(1-\pi_i)^{m_i-y_i}}        \\
                   & =\sum_{i=1}^{n} y_i\log*{\frac{\pi_i}{1-\pi_i}}+m_i\log{1-\pi_i}.
            \end{align*}
      \item Using logit link we can re-parameterize the log-likelihood in terms of $ \Vector{\beta} $:
            \[ \log*{\frac{\pi_i}{1-\pi_i}}=\Vector{x}_i^\top \Vector{\beta},\qquad \pi_i=\frac{\exp{\Vector{x}_i^\top \Vector{\beta}}}{1+\exp{\Vector{x}_i^\top \Vector{\beta}}}. \]
      \item Log likelihood for logistic regression:
            \[ \ell=\sum_{i=1}^{n} y_i\Vector{x}_i^\top \Vector{\beta}-m_i\log[\big]{1+\exp{\Vector{x}_i^\top \Vector{\beta}}}. \]
      \item Maximization of log-likelihood $ \ell(\Vector{\beta}) $ gives MLE $ \hat{\Vector{\beta}} $, and
            \begin{itemize}
                  \item estimated probability of response:
                        \[ \hat{\pi}_i=\mathrm{e}^{\Vector{x}_i^\top \hat{\Vector{\beta}}}/(1+\mathrm{e}^{\Vector{x}_i^\top \hat{\Vector{\beta}}})=\expit{\Vector{x}_i^\top \hat{\Vector{\beta}}}, \]
                  \item estimated number of responses: $ \hat{y}_i=m_i\hat{\pi}_i $.
            \end{itemize}
      \item Questions:
            \begin{itemize}
                  \item How good is the model? How well do the estimated number of events $ \hat{y}_i $ approximate the observed data $ y_i $? (\textcolor{Red}{goodness of fit}).
                  \item How much worse is the fit of a model when several of the covariates are excluded? (\textcolor{Red}{nested models}):
                        \begin{center}
                              $ \HN $: $ \beta_k=\beta_{k+1}=0 $ vs $ \HA $: $ \beta_k\ne 0 $ or $ \beta_{k+1}\ne 0 $.
                        \end{center}
            \end{itemize}
\end{itemize}
\subsection*{Likelihood Ratio Test (General Setting)}
\begin{itemize}
      \item Suppose $ \ell(\Vector{\theta}) $ is the likelihood for a $ q $-dimension parameter vector $ \Vector{\theta} $ and let
            \begin{itemize}
                  \item $ \tilde{\Vector{\theta}} $ be the $ q $-dim MLE of $ \Vector{\theta} $ (unconstrained/\textcolor{Blue}{saturated, $ q=n $}),
                  \item $ \hat{\Vector{\theta}} $ be the $ p $-dim MLE of $ \Vector{\theta} $ (constrained/\textcolor{Blue}{unsaturated, $ p<q $}).
            \end{itemize}
      \item Hypotheses:
            \begin{itemize}
                  \item $ \HN $: the constrained model is adequate (i.e., as good as the unconstrained).
                  \item $ \HA $: constrained model is not adequate.
            \end{itemize}
      \item Recall the Likelihood Ratio (LR) result:
            \[ \text{Under $ \HN $:}\quad -2\log*{\frac{L(\hat{\Vector{\theta}})}{L(\tilde{\Vector{\theta}})}}
                  =-2\bigl[\ell(\hat{\Vector{\theta}})-\ell(\tilde{\Vector{\theta}})\bigr]\sim \chi^2_{q-p}. \]
      \item Reject $ \HN $ at $ \theta $ if
            \[ p\text{-value}=\Prob[\Big]{\chi^2_{q-p}>-2\bigl[\ell(\hat{\Vector{\theta}})-\ell(\tilde{\Vector{\theta}})\bigr]}<\alpha. \]
\end{itemize}
\subsection*{Likelihood Ratio Test (Logistic Regression Model)}
\begin{itemize}
      \item \textcolor{Blue}{Saturated} (unconstrained) model MLEs:
            \[ \textcolor{Red}{\tilde{\pi}_i=\frac{y_i}{m_i}},\; i=1,\ldots,n. \]
            \begin{itemize}
                  \item Binomial MLE without imposing any constraint.
                  \item We will have $ \tilde{y}_i=m_i\tilde{\pi}_i=y_i $, \textcolor{Blue}{a perfect fit}!
            \end{itemize}
      \item \textcolor{Blue}{Unsaturated} (constrained) model MLEs:
            \[ \textcolor{Red}{\hat{\pi}_i=\expit{\Vector{x}_i^\top \hat{\Vector{\beta}}}}. \]
            \begin{itemize}
                  \item Regression models are a way of imposing constraints on the estimation of $ \pi_i $ through $ p $-dim regression coefficients $ \Vector{\beta} $.
                  \item We will have fitted number of responses $ \hat{y}_i=m_i\hat{\pi}_i=m_i\expit{\Vector{x}_i^\top \hat{\Vector{\beta}}} $.
            \end{itemize}
      \item \textcolor{Blue}{Hypotheses}:
            \begin{itemize}
                  \item $ \HN $: the $ p $-dim model, e.g., $ \logit{\pi_i}=\Vector{x}_i^\top \Vector{\beta} $ is adequate.
                  \item $ \HA $: the $ p $-dim model, e.g., $ \logit{\pi_i}=\Vector{x}_i^\top \Vector{\beta} $ is not adequate \emph{compared
                              to the $ n $-dim saturated model}.
            \end{itemize}
      \item \textcolor{Blue}{Likelihood Ratio Statistic} (also referred to as the \textcolor{Blue}{Deviance}):
            \begin{align*}
                  \textcolor{Blue}{D}
                   & \textcolor{Blue}{=}\textcolor{Blue}{-2\bigl[\ell(\hat{\Vector{\pi}})-\ell(\tilde{\Vector{\pi}})\bigr]}                     \\
                   & =-2\biggl(\sum_{i=1}^{n}\Bigl(y_i\log{\hat{\pi}_i}+(m_i-y_i)\log{1-\hat{\pi}_i}\Bigr)
                  -\sum_{i=1}^{n}\Bigl(y_i\log{\tilde{\pi}_i}+(m_i-y_i)\log{1-\tilde{\pi}_i}\Bigr)\biggr)                                       \\
                   & =-2 \sum_{i=1}^{n} \biggl(y_i\log*{\frac{y_i}{m_i\hat{\pi}_i}}+(m_i-y_i)\log*{\frac{m_i-y_i}{m_i(1-\hat{\pi}_i)} }\biggr).
            \end{align*}
      \item The \textcolor{Blue}{LR/Deviance} can also be written in a general form as:
            \[ D=2 \sum_{i=1}^{n} \sum_{j=1}^{2} \biggl(O_{ij}\log*{\frac{O_{ij}}{E_{ij}}}\biggr). \]
            \begin{itemize}
                  \item $ O_{i1}=y_i $, $ E_{i1}=m_i\hat{\pi}_i $ (observed and expected \# of events).
                  \item $ O_{i2}=m_i-y_i $, $ E_{i2}=m_i(1-\hat{\pi}_i) $ (observed and expected \# of non-events).
            \end{itemize}
      \item We expect $ D \sim \chi^2_{n-p} $ under $ \HN $, and reject $ \HN $ if $ \Prob{\chi^2_{n-p}>D}<\alpha $.
            \begin{itemize}
                  \item Unfortunately, this is not a great approximation.
                  \item Approximation is much better for testing nested unsaturated models though.
            \end{itemize}
\end{itemize}
\subsection*{Example: Prenatal Care Data}
\begin{itemize}
      \item Model 2: Main Effects Model,
            \[ \logit{\pi_i}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}. \]
            \begin{itemize}
                  \item $ \HN $: Model 2 is adequate.
                  \item $ \HA $: Model 2 is not adequate compared to the saturated model.
            \end{itemize}
      \item In R, the \texttt{summary()} output $ D $ is reported as the \textcolor{Red}{Residual Deviance}.
            % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{loc} \hlopt{+} \hlstd{clinic,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),}
  \hlkwc{data} \hlstd{= prenatal.dat)}
\hlkwd{summary}\hlstd{(model2)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = resp ~ loc + clinic, family = binomial(link = logit), 
    data = prenatal.dat)

Deviance Residuals: 
       1         2         3         4  
-0.08521   0.25805   0.13909  -0.11719  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -1.7410     0.1785  -9.755  < 2e-16 ***
loc          -0.1503     0.3302  -0.455  0.64894    
clinic       -0.9863     0.3089  -3.193  0.00141 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 16.91763  on 3  degrees of freedom
Residual deviance:  0.10693  on 1  degrees of freedom
AIC: 23.262

Number of Fisher Scoring iterations: 3
\end{verbatim}
\end{kframe}
\end{knitrout}
      % \end{noindent}
            \begin{itemize}
                  \item Deviance: $ D=0.10693 $.
                  \item $ p $-value: $ \Prob{\chi^2_{n-p}>D}=\Prob{\chi^2_1>D}=0.7436689\gg 0.05 $.
            \end{itemize}
      \item Do not reject the null hypothesis that Model 2 is adequate.
\end{itemize}
\subsection*{Pearson Statistic}
\begin{itemize}
      \item The Pearson statistic is another statistic that can be used for assessing
            ``overall'' fit (or goodness of fit) of a Binomial model:
            \[ \textcolor{Blue}{P=\sum_{i=1}^{n} \frac{(y_i-m_i\hat{\pi}_i)^2}{m_i\hat{\pi}_i(1-\hat{\pi}_i)}}. \]
            \begin{itemize}
                  \item As with LR/Deviance statistic, $ P \sim \chi^2_{n-p} $ under $ \HN $: the model is adequate.
                  \item Note that $ P $ has the general form:
                        \[ P=\sum_{i}\frac{(O_i-E_i)^2}{V_i}.   \]
                  \item The $ \chi^2 $ approximation is a bit better than for deviance statistic $ D $.
                  \item Both are poor if the sample size ($ m_i $) is small though.
            \end{itemize}
\end{itemize}
\subsection*{Testing Nested Non-saturated Models}
\begin{itemize}
      \item The previous LR/Deviance test was for an unsaturated model vs a saturated model.
      \item Now consider two unsaturated models ($ p<q<n $).
            \begin{align*}
                  \logit{\pi_i} & =\beta_0+\beta_1x_{i1}+\cdots+\beta_{p-1}x_{ip-1}\tag*{(1)}\label{2.6M1}                            \\
                  \logit{\pi_i} & =\beta_0+\beta_1x_{i1}+\cdots+\beta_{p-1}x_{ip-1}+\cdots+\beta_{q-1}x_{iq-1}\tag*{(2)}\label{2.6M2}
            \end{align*}
      \item Model~\ref{2.6M1} is \textcolor{Blue}{\emph{nested}} within Model~\ref{2.6M2}.
      \item $ \HN $: Model~\ref{2.6M1} fits the data as well as Model~\ref{2.6M2}.
            \begin{itemize}
                  \item $ \HN $: $ \beta_p=\cdots=\beta_{q-1}=0 $.
            \end{itemize}
      \item $ \HA $: Model~\ref{2.6M1} is inadequate compared to Model~\ref{2.6M2}.
            \begin{itemize}
                  \item $ \HA $: at least one of $ \beta_p,\ldots,\beta_{q-1}\ne 0 $.
            \end{itemize}
\end{itemize}
\begin{table}[!htbp]
      \centering
      \begin{tabular}{lcc}
            \textcolor{Blue}{Model}   & \textcolor{Blue}{Dimension} & \textcolor{Blue}{MLEs}    \\
            \midrule
            \ref{2.6M1} Reduced model & $ p $                       & $ \hat{\pi}_i $           \\
            \ref{2.6M2} Full model    & $ q $                       & $ \tilde{\pi}_i $         \\
            Saturated model           & $ n $                       & $ \tilde{\tilde{\pi}}_i $ \\
            \bottomrule
      \end{tabular}
\end{table}
\begin{itemize}
      \item LR/Deviance test of Model~\ref{2.6M1} vs Saturated Model:
            \[ \textcolor{Blue}{D_0=-2\bigl(\ell(\hat{\Vector{\pi}})-\ell(\tilde{\tilde{\Vector{\pi}}})\bigr)}. \]
      \item LR/Deviance test of Model~\ref{2.6M2} vs Saturated Model:
            \[ \textcolor{Blue}{D_\text{A}=-2\bigl(\ell(\tilde{\Vector{\pi}})-\ell(\tilde{\tilde{\Vector{\pi}}})\bigr)}. \]
      \item Now, we wish to conduct LR test of Model~\ref{2.6M1} vs Model~\ref{2.6M2}:
            \[ \textcolor{Blue}{\Delta D} = \textcolor{Blue}{D_0-D_{\text{A}}}=-2\bigl(\ell(\hat{\Vector{\pi}})-\ell(\tilde{\Vector{\pi}})\bigr). \]
      \item It can be shown that under $ \HN $: Model~\ref{2.6M1} is as adequate as Model~\ref{2.6M2},
            \[ \Delta D \sim \chi^2_{q-p}. \]
            \begin{itemize}
                  \item This approximation is much better than when testing an unsaturated
                        model vs the saturated model.
            \end{itemize}
      \item If $ p=\Prob{\chi^2_{q-p}>\Delta D}<\alpha $, reject $ \HN $.
            \begin{itemize}
                  \item Reduced model does not fit the data as well as Full model.
                  \item One or more of covariates $ x_{ip},\ldots,x_{iq-1} $ is important (i.e., associated with the response).
            \end{itemize}
\end{itemize}
\subsection*{Example: Prenatal Care Data}
\begin{itemize}
      \item Summary of Deviance (``\texttt{residual deviance}'') from R output:
            \begin{table}[!htbp]
                  \centering
                  \begin{tabular}{clccc}
                        \toprule
                        Model & Covariates                         & Deviance  & Parameters & $ n-p $ \\
                        \midrule
                        1     & \texttt{loc}                       & 10.814378 & 2          & 2       \\
                        2     & \texttt{loc + clinic}              & 0.106928  & 3          & 1       \\
                        3     & \texttt{loc + clinic + loc*clinic} & 0         & 4          & 0       \\
                        4     & \texttt{clinic}                    & 0.314841  & 2          & 2       \\
                        \bottomrule
                  \end{tabular}
            \end{table}
      \item Compare nested models:
            \begin{itemize}
                  \item Model 2: $ \logit{\pi_i}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2} $.
                  \item Model 4: $ \logit{\pi_i}=\beta_0+\beta_2x_{i2} $.
            \end{itemize}
      \item Is level of care associated with fetal mortality after accounting for clinic?
            \begin{itemize}
                  \item $ \HN $: Model 4 is as adequate as Model 2 (e.g., $ \beta_1=0 $).
                  \item $ \HA $: Model 4 is inadequate compared to Model 2 (e.g., $ \beta_1\ne 0 $).
            \end{itemize}
      \item LR test for comparing Model 4 vs Model 2, or equivalently testing hypotheses:
            \begin{center}
                  $ \HN $: $ \beta_1=0 $ vs $ \HA $: $ \beta_1\ne 0 $.
            \end{center}
            \begin{itemize}
                  \item We do not reject $ \HN $ of no association between level and care and fetal
                        mortality after controlling for Clinic.
                        % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{loc} \hlopt{+} \hlstd{clinic,} \hlkwc{family} \hlstd{= binomial,} \hlkwc{data} \hlstd{= prenatal.dat)}
\hlstd{model4} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{clinic,} \hlkwc{family} \hlstd{= binomial,} \hlkwc{data} \hlstd{= prenatal.dat)}
\hlstd{D} \hlkwb{=} \hlstd{model4}\hlopt{$}\hlstd{deviance} \hlopt{-} \hlstd{model2}\hlopt{$}\hlstd{deviance}
\hlnum{1} \hlopt{-} \hlkwd{pchisq}\hlstd{(D,} \hlnum{2} \hlopt{-} \hlnum{1}\hlstd{)}
\end{alltt}
\begin{verbatim}
[1] 0.6484081
\end{verbatim}
\end{kframe}
\end{knitrout}
            % \end{noindent}
                  \item This implies that level of care is no longer important when clinic is
                        included in the model.
                  \item It also implies that Model 4 is as adequate compared to Model 2.
            \end{itemize}
      \item Finally, when testing a single parameter, e.g., $ \HN $: $ \beta_1=0 $, LR/Deviance
            test result is consistent with the Wald test result provided in the R output:
            % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{loc} \hlopt{+} \hlstd{clinic,} \hlkwc{family} \hlstd{= binomial,} \hlkwc{data} \hlstd{= prenatal.dat)}
\hlkwd{summary}\hlstd{(model2)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = resp ~ loc + clinic, family = binomial, data = prenatal.dat)

Deviance Residuals: 
       1         2         3         4  
-0.08521   0.25805   0.13909  -0.11719  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -1.7410     0.1785  -9.755  < 2e-16 ***
loc          -0.1503     0.3302  -0.455  0.64894    
clinic       -0.9863     0.3089  -3.193  0.00141 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 16.91763  on 3  degrees of freedom
Residual deviance:  0.10693  on 1  degrees of freedom
AIC: 23.262

Number of Fisher Scoring iterations: 3
\end{verbatim}
\end{kframe}
\end{knitrout}
            % \end{noindent}
\end{itemize}
\subsection*{Summary of LR/Deviance Test for Logistic Regression}
\begin{itemize}
      \item For Binomial GLM with logit link the LR/Deviance test statistic is:
            \[ D=\sum_{i=1}^{n} 2\biggl(y_i\log*{\frac{y_i}{m_i\hat{\pi}_i}}+(m_i-y_i)\log*{\frac{m_i-y_i}{m_i(1-\hat{\pi}_i)}}\biggr). \]
      \item This is reported as the ``\texttt{Residual Deviance}'' in R \texttt{glm} summary output.
      \item Deviance statistic $ D $ can be used to:
            \begin{itemize}
                  \item Test adequacy/goodness of fit of a non-saturated logistic model:
                        \[ D \stackrel{\HN}{\sim}\chi^2_{n-p}. \]
                  \item Compare the fit of two nested-non saturated logistic models:
                        \[ \Delta D=D_0-D_\text{A} \stackrel{\HN}{\sim}\chi^2_{q-p}. \]
            \end{itemize}
\end{itemize}
% Week 5

\makeheading{Week 5}{\daterange{2021-10-03}{2021-10-08}}
\section*{Topic 3d: Residuals for Binomial Data and Neuroblastoma Example}
\addcontentsline{toc}{section}{Topic 3d: Residuals for Binomial Data and Neuroblastoma Example}
\subsection*{Recall: Residuals in Linear Regression Models}
\begin{itemize}
    \item Normal linear regression models (STAT 331),
          \[ y_i=\Vector{x}_i^\top \Vector{\beta}+\varepsilon_i,\qquad \varepsilon_i\iid\N{0,\sigma^2}. \]
    \item Fitted values:
          \[ \hat{y}_i=\Vector{x}_i^\top \hat{\Vector{\beta}}. \]
    \item Residuals:
          \[ r_i=y_i-\hat{y}_i. \]
    \item The overall fit of the model and validity of the model assumptions are assessed using various \textcolor{Blue}{\emph{residual plots}},
          e.g.,
          \begin{itemize}
              \item Residuals $ r_i $ vs fitted value $ \hat{y}_i $ plot (check normality and constant variance).
              \item QQ plot of residuals $ r_i $'s (check normality).
          \end{itemize}
\end{itemize}
\subsection*{Residuals for Binomial Data}
\begin{itemize}
    \item When fit a logistic regression model to Binomial data, we evaluate the adequacy of the model by using the LR deviance test statistic:
          \begin{align*}
              D
               & =\sum_{i=1}^{n} 2\biggl(y_i\log*{\frac{y_i}{m_i\hat{\pi}_i}}+(m_i-y_i)\log*{\frac{m_i-y_i}{m_i(1-\hat{\pi}_i)}}\biggr) \\
               & =\sum_{i=1}^{n} d_i.
          \end{align*}
    \item \textcolor{Blue}{Deviance Residual}:
          \[ r_i^D=\sign{y_i-m_i\hat{\pi}_i}\sqrt{\abs{d_i}}. \]
    \item Under $ \HN $: the model is adequate:
          \[ D=\sum_{i=1}^{n} d_i\stackrel{\text{approx}}{\sim}\chi^2_{n-p}\implies r_i^D\stackrel{\text{approx}}{\sim}\N{0,1}. \]
    \item We can use the plots of deviance residuals to assess whether $ r_i^D $'s look independent observations from $ \N{0,1} $.
\end{itemize}
\subsection*{Example: Prenatal Care Data}
\[ \logit{\pi_i}=\beta_0+\beta_1\texttt{clinic}_i \]
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model4} \hlkwb{<-} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{clinic,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),} \hlkwc{data} \hlstd{= prenatal.dat)}
\hlkwd{summary}\hlstd{(model4)}\hlopt{$}\hlstd{deviance.resid}
\end{alltt}
\begin{verbatim}
           1            2            3            4 
-0.004318004  0.012618764  0.436709170 -0.352063013 
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\begin{itemize}
    \item \textcolor{Blue}{Pearson Residual}:
          \[ r_i^P=\frac{y_i-m_i\hat{\pi}_i}{\sqrt{m_i\hat{\pi}_i(1-\hat{\pi}_i)}}=\frac{O_i-E_i}{\sqrt{V_i}}.  \]
    \item Under $ \HN $: the model is adequate,
          \[ r_i^P \sim \N{0,1}. \]
    \item Note: if $ m_i\hat{\pi}_i<5 $ (or $ m_i(1-\hat{\pi}_i)<5 $) for one or more cases, we should be concerned about
          the validity of the approximation ($ \chi^2 $ or $ \N{0,1} $) and hence our conclusions.
\end{itemize}
\subsection*{Prognosis for Children with Neuroblastoma}
\begin{itemize}
    \item A study is conducted to investigate the probability of \textcolor{Blue}{\emph{disease-free survival}}
          (surviving 2 years free of disease) following the treatment for neuroblastoma.
    \item Associated risk factors include \textcolor{Blue}{\emph{age at diagnosis}} and \textcolor{Blue}{\emph{stage of disease at diagnosis}}.
          \begin{table}
              \centering
              \begin{tabular}{cccccc}
                  \toprule
                               & \multicolumn{5}{c}{Stage}                                               \\
                  \midrule
                  Age (months) & I                         & II        & III      & IV       & V         \\
                  \midrule
                  0-11         & $ 11/12 $                 & $ 15/16 $ & $ 2/4 $  & $ 5/18 $ & $ 18/19 $ \\
                  12-23        & $ 3/4 $                   & $ 3/7 $   & $ 5/8 $  & $ 0/25 $ & $ 1/3 $   \\
                  24+          & $ 4/5 $                   & $ 4/12 $  & $ 3/15 $ & $ 3/93 $ & $ 2/5 $   \\
                  \bottomrule
              \end{tabular}
          \end{table}
          \begin{itemize}
              \item Cell entries are of the form $y/m$ with $y$ representing the number of patients surviving 2
                    years, and $m$ representing the number of patients in that age-stage combination at the
                    start of the study.
          \end{itemize}
    \item As an initial look at the data, consider the marginal distributions.
          \begin{table}[!htbp]
              \centering
              \begin{NiceTabular}{cccccc|c}
                  \toprule
                  &\multicolumn{5}{c}{Stage}\\
                  \midrule
                  Age (months) & I & II & III & IV & V & Total\\
                  \midrule
                  0-11 & $ 11/12 $ & $ 15/16 $ & $ 2/4 $ & $ 5/18 $ & $ 18/19 $ & $ 51/69 $\\
                  12-23 & $ 3/4 $ & $ 3/7 $ & $ 5/8 $ & $ 0/25 $ & $ 1/3 $ & $ 12/47 $\\
                  24+ & $ 4/5 $ & $ 4/12 $ & $ 3/15 $ & $ 3/93 $ & $ 2/5 $ & $ 16/130 $\\
                  \midrule
                  Total & $ 18/21 $ & $ 22/35 $ & $ 10/27 $ & $ 8/136 $ & $ 21/27 $ & $ 79/246 $\\
                  \bottomrule
              \end{NiceTabular}
          \end{table}
          \begin{itemize}
              \item Higher chance of survival at younger age at diagnosis.
              \item Higher chance of survival with lower stage of disease at diagnosis.
          \end{itemize}
\end{itemize}
\subsection*{Setup Regression Models for Neuroblastoma Data}
\begin{itemize}
    \item Response Variable:
          \begin{itemize}
              \item \textcolor{Blue}{$ Y_i $} is the number of 2-yr disease-free survivors out of $ m_i $ total children in
                    group $ i $, assume $ Y_i \sim \BIN{m_i,\pi_i} $, $ i=1,\ldots,15 $, and
                    \[ \pi_i=\Prob{\text{2-yr disease-free survival in group $ i $}}. \]
          \end{itemize}
    \item Explanatory Variables:
          \begin{itemize}
              \item \textcolor{Blue}{Age} (0-11, 12-23, 24+ months); age 0-11 month is the baseline/reference,
                    \[ x_{i1}  = \begin{cases*}
                            1 & if age 12-23 months \\
                            0 & o.w.
                        \end{cases*} \qquad
                        x_{i2}  = \begin{cases*}
                            1 & if age 24+ months \\
                            0 & o.w.
                        \end{cases*} \]
              \item \textcolor{Blue}{Stage} (I, II, III, IV, V); stage 1 is the baseline/reference,
                    \[ \begin{array}{ll}
                            x_{i3} = \begin{cases*}
                                         1 & stage II \\
                                         0 & o.w.
                                     \end{cases*}  &
                            x_{i4}  = \begin{cases*}
                                          1 & if stage III \\
                                          0 & o.w.
                                      \end{cases*} \\
                            x_{i5}  = \begin{cases*}
                                          1 & if stage IV \\
                                          0 & o.w.
                                      \end{cases*} &
                            x_{i6}  = \begin{cases*}
                                          1 & if stage V \\
                                          0 & o.w.
                                      \end{cases*}
                        \end{array} \]
          \end{itemize}
    \item Consider the following logistic regression models:
          \begin{itemize}
              \item Model 1: Age \& Stage
                    \[ \logit{\pi_i}=\beta_0+\underbrace{\beta_1x_{i1}+\beta_2x_{i2}}_{\text{Age}}+\underbrace{\beta_3x_{i3}+\beta_4x_{i4}+\beta_5x_{i5}+\beta_6x_{i6}}_{\text{Stage}}. \]
              \item Model 2: Age only
                    \[ \logit{\pi_i}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}. \]
              \item Model 3: Stage only
                    \[ \logit{\pi_i}=\beta_0+\beta_3x_{i3}+\beta_4x_{i4}+\beta_5x_{i5}+\beta_6x_{i6}. \]
          \end{itemize}
\end{itemize}
\subsection*{Fitting Logistic Regression Models Using R}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{neuro.dat} \hlkwb{=} \hlkwd{read.table}\hlstd{(}\hlstr{"neuro.dat"}\hlstd{,} \hlkwc{header} \hlstd{= T)}
\hlstd{neuro.dat}
\end{alltt}
\begin{verbatim}
   age stage  y  m
1    1     1 11 12
2    1     2 15 16
3    1     3  2  4
4    1     4  5 18
5    1     5 18 19
6    2     1  3  4
7    2     2  3  7
8    2     3  5  8
9    2     4  0 25
10   2     5  1  3
11   3     1  4  5
12   3     2  4 12
13   3     3  3 15
14   3     4  3 93
15   3     5  2  5
\end{verbatim}
\begin{alltt}
\hlcom{# here we construct the response variable for logistic}
\hlcom{# regression}
\hlstd{neuro.dat}\hlopt{$}\hlstd{resp} \hlkwb{=} \hlkwd{cbind}\hlstd{(neuro.dat}\hlopt{$}\hlstd{y, neuro.dat}\hlopt{$}\hlstd{m} \hlopt{-} \hlstd{neuro.dat}\hlopt{$}\hlstd{y)}
\hlstd{neuro.dat}
\end{alltt}
\begin{verbatim}
   age stage  y  m resp.1 resp.2
1    1     1 11 12     11      1
2    1     2 15 16     15      1
3    1     3  2  4      2      2
4    1     4  5 18      5     13
5    1     5 18 19     18      1
6    2     1  3  4      3      1
7    2     2  3  7      3      4
8    2     3  5  8      5      3
9    2     4  0 25      0     25
10   2     5  1  3      1      2
11   3     1  4  5      4      1
12   3     2  4 12      4      8
13   3     3  3 15      3     12
14   3     4  3 93      3     90
15   3     5  2  5      2      3
\end{verbatim}
\begin{alltt}
\hlstd{neuro.dat}\hlopt{$}\hlstd{age} \hlkwb{<-} \hlkwd{factor}\hlstd{(neuro.dat}\hlopt{$}\hlstd{age,} \hlkwc{levels} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{3}\hlstd{),} \hlkwc{labels} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"0-11"}\hlstd{,}
  \hlstr{"12-23"}\hlstd{,} \hlstr{"24+"}\hlstd{))}
\hlstd{neuro.dat}\hlopt{$}\hlstd{stage} \hlkwb{<-} \hlkwd{factor}\hlstd{(neuro.dat}\hlopt{$}\hlstd{stage,} \hlkwc{levels} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{4}\hlstd{,}
  \hlnum{5}\hlstd{),} \hlkwc{labels} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"I"}\hlstd{,} \hlstr{"II"}\hlstd{,} \hlstr{"III"}\hlstd{,} \hlstr{"IV"}\hlstd{,} \hlstr{"V"}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}
% \end{noindent}
\subsection*{Summary of Model 1: Age \& Stage}
\[ \logit{\pi_i}=\beta_0+\underbrace{\beta_1x_{i1}+\beta_2x_{i2}}_{\text{Age}}+\underbrace{\beta_3x_{i3}+\beta_4x_{i4}+\beta_5x_{i5}+\beta_6x_{i6}}_{\text{Stage}}. \]
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}

Call:
glm(formula = resp ~ age + stage, family = binomial(link = logit), 
    data = neuro.dat)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.47408  -0.61913  -0.09643   0.53163   1.52114  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   3.3175     0.7721   4.297 1.73e-05 ***
age12-23     -2.1181     0.5736  -3.693 0.000222 ***
age24+       -2.6130     0.5017  -5.208 1.91e-07 ***
stageII      -1.2529     0.7837  -1.599 0.109860    
stageIII     -1.7759     0.8003  -2.219 0.026478 *  
stageIV      -4.3678     0.7902  -5.528 3.25e-08 ***
stageV       -1.0222     0.8644  -1.183 0.236980    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 162.832  on 14  degrees of freedom
Residual deviance:   9.625  on  8  degrees of freedom
AIC: 55.382

Number of Fisher Scoring iterations: 4
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\begin{itemize}
    \item Before interpreting these results too much, we should look to see how good the
          fit is to the data.
          \begin{itemize}
              \item \texttt{fv1}: $ \hat{\pi}_i=\expit{\Vector{x}_i^\top \hat{\Vector{\beta}}} $.
              \item \texttt{yhat}: $ \hat{y}_i=m_i\hat{\pi}_i $.
              \item \texttt{rd1}: $ r_i^D $ (deviance residual).
              \item \texttt{rp1}: $ r_i^P $ (Pearson residual).
          \end{itemize}
          % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{y} \hlkwb{=} \hlstd{neuro.dat}\hlopt{$}\hlstd{y}
\hlstd{m} \hlkwb{=} \hlstd{neuro.dat}\hlopt{$}\hlstd{m}
\hlstd{fv1} \hlkwb{=} \hlstd{model1}\hlopt{$}\hlstd{fitted.values}
\hlstd{yhat} \hlkwb{=} \hlstd{m} \hlopt{*} \hlstd{fv1}
\hlstd{rd1} \hlkwb{=} \hlkwd{residuals.glm}\hlstd{(model1,} \hlstr{"deviance"}\hlstd{)}
\hlstd{rp1} \hlkwb{=} \hlstd{(y} \hlopt{-} \hlstd{m} \hlopt{*} \hlstd{fv1)}\hlopt{/}\hlkwd{sqrt}\hlstd{(m} \hlopt{*} \hlstd{fv1} \hlopt{*} \hlstd{(}\hlnum{1} \hlopt{-} \hlstd{fv1))}
\hlkwd{cbind}\hlstd{(rd1, rp1, yhat, y)}
\end{alltt}
\begin{verbatim}
           rd1         rp1      yhat  y
1  -0.77808711 -0.91184050 11.580304 11
2   0.68559153  0.63381666 14.198641 15
3  -1.47407847 -1.69888561  3.294804  2
4   0.17884403  0.18019371  4.665014  5
5   0.63431439  0.58779486 17.261237 18
6  -0.08658336 -0.08736144  3.073705  3
7  -0.30801258 -0.30734393  3.406432  3
8   1.52114028  1.56325351  2.877982  5
9  -1.43545385 -1.02556686  1.009324  0
10 -0.73520283 -0.73328264  1.632557  1
11  0.64949774  0.62163765  3.345991  4
12 -0.23825133 -0.23663531  4.394927  4
13 -0.50305728 -0.48993834  3.827214  3
14  0.42894854  0.44782015  2.325662  3
15 -0.09643089 -0.09619454  2.106206  2
\end{verbatim}
\end{kframe}
\end{knitrout}
    % \end{noindent}
          % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-37-1} 

}

\caption[Plot of Residuals by Fitted Values for Neuroblastoma Data based on Logistic Regression Model with main effects of Age and Stage]{Plot of Residuals by Fitted Values for Neuroblastoma Data based on Logistic Regression Model with main effects of Age and Stage.}\label{fig:unnamed-chunk-37}
\end{figure}

\end{knitrout}
    % \end{noindent}
    \item Residuals are a random scatter around $ 0 $ and $ \in(-2,2) $ therefore $ r_i^D $ (or $ r_i^P $) $ \sim \N{0,1} $. Therefore,
          model 1 is adequate.
    \item We can test $ \HN $: model 1 is adequate using LR/D statistic $ p\text{-value}=\Prob{\chi^2_8>9.625}>0.05 $, do not reject $ \HN $.
\end{itemize}
\subsection*{Summary of Model 2: Age only}
\begin{itemize}
    \item Now we consider simplifying the model further by examining the decrease in
          the quality of the fit that results from dropping the stage variable(s).
          \[ \logit{\pi_i}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}. \]
          % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model3} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{age,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),} \hlkwc{data} \hlstd{= neuro.dat)}
\hlkwd{summary}\hlstd{(model3)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = resp ~ age, family = binomial(link = logit), data = neuro.dat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-4.0853  -0.3591   1.5613   2.0684   3.4667  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   1.0415     0.2742   3.799 0.000145 ***
age12-23     -2.1119     0.4325  -4.883 1.05e-06 ***
age24+       -3.0051     0.3827  -7.853 4.06e-15 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 162.832  on 14  degrees of freedom
Residual deviance:  83.583  on 12  degrees of freedom
AIC: 121.34

Number of Fisher Scoring iterations: 5
\end{verbatim}
\end{kframe}
\end{knitrout}
    % \end{noindent}
\end{itemize}
\subsection*{Summary of Model 3: Stage only}
\begin{itemize}
    \item Now we fit the model excluding the age variable to examine the drop in the
          quality of fit from model 1.
          \[ \logit{\pi_i}=\beta_0+\beta_3x_{i3}+\beta_4x_{i4}+\beta_5x_{i5}+\beta_6x_{i6}. \]
          % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{stage,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),} \hlkwc{data} \hlstd{= neuro.dat)}
\hlkwd{summary}\hlstd{(model2)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = resp ~ stage, family = binomial(link = logit), 
    data = neuro.dat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0699  -1.5375  -0.5639   1.0444   2.9391  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   1.7918     0.6236   2.873  0.00406 ** 
stageII      -1.2657     0.7150  -1.770  0.07671 .  
stageIII     -2.3224     0.7401  -3.138  0.00170 ** 
stageIV      -4.5643     0.7223  -6.319 2.63e-10 ***
stageV       -0.5390     0.7766  -0.694  0.48768    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 162.832  on 14  degrees of freedom
Residual deviance:  42.446  on 10  degrees of freedom
AIC: 84.203

Number of Fisher Scoring iterations: 5
\end{verbatim}
\end{kframe}
\end{knitrout}
    % \end{noindent}
\end{itemize}
\subsection*{Testing Nested Models}
\begin{itemize}
    \item Now we can compare nested models using \textcolor{Blue}{LR/Deviance} Tests:
          \begin{table}[!htbp]
              \centering
              \begin{tabular}{cllcc}
                  \toprule
                  Model & Covariates   & Deviance ($ D $) & Parameters ($ p $) & DF ($ n-p $) \\
                  \midrule
                  M1    & Age \& Stage & $9.625$          & $ 7 $              & $ 8 $        \\
                  M2    & Age          & $83.583$         & $ 3 $              & $ 12 $       \\
                  M3    & Stage        & $42.446$         & $ 5 $              & $ 10 $       \\
                  \bottomrule
              \end{tabular}
          \end{table}
    \item Recall:
          \[ \Delta D=D_0-D_\text{A}=-2\bigl(\ell(\hat{\Vector{\pi}})-\ell(\tilde{\Vector{\pi}})\bigr) \sim \chi^2_{q-p} \]
          \begin{itemize}
              \item $ D_0 $ and $ D_\text{A} $ are deviances from the \textcolor{Blue}{reduced} and \textcolor{Blue}{full} models respectively.
              \item $ \hat{\Vector{\pi}} $ and $ \tilde{\Vector{\pi}} $ represents the MLEs from the \textcolor{Blue}{reduced} and \textcolor{Blue}{full} models respectively.
          \end{itemize}
\end{itemize}
\begin{Example}{}
    \textcolor{Blue}{Objective}: Pick the model that best represents the important associations between the
    outcome and explanatory variables.
\end{Example}
\begin{enumerate}[1.]
    \item \textcolor{Blue}{Is Stage important?}
          \begin{align*}
              \HN & \colon \beta_3=\cdots=\beta_6=0             &  & \text{(Model 2 is as adequate as Model 1)} \\
              \HA & \colon \text{at least one of them is not 0} &  & \text{(Model 2 is not adequate)}
          \end{align*}
          \[ \Delta D=D_2-D_1=83.583-9.625=73.958 \]
          \[ p=\Prob{\chi^2_{7-3}>73.958}<0.001 \]
          % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlnum{1} \hlopt{-} \hlkwd{pchisq}\hlstd{(}\hlnum{83.583} \hlopt{-} \hlnum{9.625}\hlstd{,} \hlnum{7} \hlopt{-} \hlnum{3}\hlstd{)}
\end{alltt}
\begin{verbatim}
[1] 3.330669e-15
\end{verbatim}
\end{kframe}
\end{knitrout}
        % \end{noindent}
          We reject $ \HN $ and conclude that there is evidence that Stage is important.
    \item \textcolor{Blue}{Is Age important?}
          \begin{align*}
              \HN & \colon \beta_1=\beta_2=0                    &  & \text{(Model 3 is as adequate as Model 1)} \\
              \HA & \colon \text{at least one of them is not 0} &  & \text{(Model 3 is not adequate)}
          \end{align*}
          \[ \Delta D=D_3-D_1=42.446 - 9.625 = 32.821 \]
          \[ p=\Prob{\chi^2_{7-5}>32.821}<0.001 \]
          % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlnum{1} \hlopt{-} \hlkwd{pchisq}\hlstd{(}\hlnum{42.446} \hlopt{-} \hlnum{9.625}\hlstd{,} \hlnum{7} \hlopt{-} \hlnum{5}\hlstd{)}
\end{alltt}
\begin{verbatim}
[1] 7.464666e-08
\end{verbatim}
\end{kframe}
\end{knitrout}
      % \end{noindent}
          We reject $ \HN $ and conclude that there is evidence that Age is important.
    \item \textcolor{Blue}{Do we need an Age$*$Stage interaction?}
          % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlnum{1} \hlopt{-} \hlkwd{pchisq}\hlstd{(model1}\hlopt{$}\hlstd{deviance, model1}\hlopt{$}\hlstd{df.residual)}
\end{alltt}
\begin{verbatim}
[1] 0.292341
\end{verbatim}
\end{kframe}
\end{knitrout}
        % \end{noindent}
          \[ p\text{-value}=\Prob{\chi^2_8>9.625}=0.292>0.05 \]
          \begin{itemize}
              \item Model with age, stage, and age$*$stage is the saturated model!
              \item Do not reject $ \HN $: model 1 is as adequate as the saturated model (interaction model).
              \item Do not need to consider age$*$stage.
          \end{itemize}
\end{enumerate}
\subsection*{Interpret the Selected Model}
So we select \textcolor{Blue}{Model 1} for interpretation.
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model1} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{age} \hlopt{+} \hlstd{stage,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),}
  \hlkwc{data} \hlstd{= neuro.dat)}
\hlkwd{summary}\hlstd{(model1)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = resp ~ age + stage, family = binomial(link = logit), 
    data = neuro.dat)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.47408  -0.61913  -0.09643   0.53163   1.52114  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   3.3175     0.7721   4.297 1.73e-05 ***
age12-23     -2.1181     0.5736  -3.693 0.000222 ***
age24+       -2.6130     0.5017  -5.208 1.91e-07 ***
stageII      -1.2529     0.7837  -1.599 0.109860    
stageIII     -1.7759     0.8003  -2.219 0.026478 *  
stageIV      -4.3678     0.7902  -5.528 3.25e-08 ***
stageV       -1.0222     0.8644  -1.183 0.236980    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 162.832  on 14  degrees of freedom
Residual deviance:   9.625  on  8  degrees of freedom
AIC: 55.382

Number of Fisher Scoring iterations: 4
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\begin{enumerate}[label={Q\arabic*:}]
    \item What is the \textcolor{Blue}{odds ratio} of 2 yr disease-free survival for a child \textcolor{Blue}{aged 24+ months}
          versus \textcolor{Blue}{aged $<12$ months}?
          \begin{table}[!htbp]
              \centering
              \begin{tabular}{cccl}
                  Age  & Stage & $ \Vector{x}_i^\top $                        & $ \log[\big]{\pi_i/(1-\pi_i)} $                                            \\
                  \midrule
                  0-11 & ---   & $ (1,0,0,x_{i3},x_{i4},x_{i5},x_{i6})^\top $ & $ \beta_0+\beta_3x_{i3}+\beta_4x_{i4}+\beta_5x_{i5}+\beta_6x_{i6} $        \\
                  24+  & ---   & $ (1,0,1,x_{i3},x_{i4},x_{i5},x_{i6})^\top $ & $ \beta_0+\beta_2+\beta_3x_{i3}+\beta_4x_{i4}+\beta_5x_{i5}+\beta_6x_{i6}$ \\
                  \bottomrule
              \end{tabular}
          \end{table}
          \begin{itemize}
              \item The odds ratio is therefore $ \psi=\exp{\beta_2} $, its MLE is:
                    \[ \hat{\psi}=\exp{\hat{\beta}_2}=\exp{-2.614}=0.0733. \]
              \item The \textcolor{Blue}{\qty{95}{\percent} CI} for this odds ratio is:
                    \[ \exp{\hat{\beta}_2\pm 1.96\se{\hat{\beta}_2}}=\exp{-2.613\pm 1.96\times 0.5017}=(0.0274,0.1960). \]
              \item \emph{When controlling for stage at the diagnosis, the odds of 2-yr DFS for children aged 24+ months
                        is only about \qty{7}{\percent} [\qty{95}{\percent} CI\@: (0.0274,0.1960)] of that for those aged less than 12 months}.
          \end{itemize}
    \item What is the \textcolor{Blue}{odds ratio} of 2 yr disease-free survival for a child with
          \textcolor{Blue}{stage V} versus \textcolor{Blue}{stage II} cancer?
          \begin{table}[!htbp]
              \centering
              \begin{tabular}{cccl}
                  Age & Stage & $ \Vector{x}_i^\top $              & $ \log[\big]{\pi_i/(1-\pi_i)} $                 \\
                  \midrule
                  --- & V     & $ (1,x_{i1},x_{i2},0,0,0,1)^\top $ & $ \beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_6 $ \\
                  --- & II    & $ (1,x_{i1},x_{i2},1,0,0,0)^\top $ & $ \beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3 $ \\
                  \bottomrule
              \end{tabular}
          \end{table}
          \begin{itemize}
              \item The odds ratio is therefore $ \psi=\exp{\beta_6-\beta_3} $, its MLE is:
                    \[ \hat{\psi}=\exp{\hat{\beta}_6-\hat{\beta}_3}=\exp{-1.022+1.253}=1.26. \]
              \item \emph{When controlling for age at the diagnosis, the odds of a 2-yr DFS for those
                        diagnosed in stage V is 1.26 times of that for those diagnosed in stage II}.
          \end{itemize}
    \item What is the \textcolor{Blue}{\qty{95}{\percent} CI for OR $ \psi=\exp{\beta_6-\beta_3} $}?
          \begin{enumerate}[1.]
              \item Finding the \qty{95}{\percent} CI for $ \eta=\beta_6-\beta_3=C \Vector{\beta} $, where
                    \[ C=\begin{bmatrix}
                            0 & 0 & 0 & -1 & 0 & 0 & 1
                        \end{bmatrix},\qquad \Vector{\beta}=\begin{bmatrix}
                            \beta_0 \\
                            \beta_1 \\
                            \vdots  \\
                            \beta_6
                        \end{bmatrix}. \]
                    Standard error for $ \hat{\eta}=\hat{\beta}_6-\hat{\beta}_3=C^\top\hat{\Vector{\beta}} $:
                    \begin{align*}
                        \estVar{\hat{\Vector{\beta}}} & =\Matrix{I}^{-1}(\hat{\Vector{\beta}})                \\
                        \se{C\Vector{\beta}}          & =\sqrt{C\Matrix{I}^{-1}(\hat{\Vector{\beta}})C^\top}.
                    \end{align*}
                    % \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{C} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlopt{-}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{se} \hlkwb{=} \hlkwd{sqrt}\hlstd{(C} \hlopt{%*%} \hlkwd{vcov}\hlstd{(model1)} \hlopt{%*%} \hlstd{C)}
\hlstd{se}
\end{alltt}
\begin{verbatim}
          [,1]
[1,] 0.6729361
\end{verbatim}
\end{kframe}
\end{knitrout}
        % \end{noindent}
                    The \qty{95}{\percent} CI for $ \eta=\beta_6-\beta_3 $ is:
                    \[ \hat{\eta}\pm 1.96\se{\hat{\eta}}=(-1.0222+1.2529)\pm 1.96\times 0.6729=(-1.0882,1.5496). \]
              \item Exponentiate it to obtain the \qty{95}{\percent} CI for $ \psi=\exp{\eta}=\exp{\beta_6-\beta_3} $:
                    \[ \exp{\hat{\eta}\pm 1.96\se{\hat{\eta}}}=(0.3368,4.7098). \]
          \end{enumerate}
\end{enumerate}

\section*{Topic 3e: Dose-Response Models}
\addcontentsline{toc}{section}{Topic 3e: Dose-Response Models}
\subsection*{Bioassay Experiments}
\begin{itemize}
      \item \textcolor{Blue}{Bioassay experiment}: Several groups of subjects are exposed to varying levels
            of a drug/toxin to determine how many responses within a fixed period of time.
      \item \textcolor{Blue}{Stimulus}: Each group is subjected to a particular dose of the drug/toxin:
            \[ \textcolor{Blue}{\text{dose}=\log{\text{concentration}}} \]
      \item \textcolor{Blue}{Response}: As a result of the stimulus, subjects will often manifest a binary
            response indicating the occurrence of an adverse event (e.g., death).
      \item \textcolor{Blue}{Tolerance}: We assume that for each subject there is a certain dose level above
            which the response will always occur.
            \begin{itemize}
                  \item This level is called the tolerance or threshold.
                  \item The tolerance varies from one individual to another in the population and therefore
                        from subject to subject in the sample.
                  \item We can therefore ascribe a distribution to it.
            \end{itemize}
\end{itemize}
\subsection*{The Tolerance Distribution}
\begin{itemize}
      \item $ z =$ concentration of the stimulus (toxin/drug).
      \item $ x=\log{z} =$ dose/intensity of the stimulus.
      \item $ f(x)= $ pdf for the distribution of the tolerance in the population (\emph{i.e., the
                  distribution for the stimulus/dose at which response occurs}).
      \item Suppose a dose of $ x_0 $ were applied to the population. What proportion would
            respond?
            \[ \pi_0=\int_{-\infty}^{x_0}f(s)\odif{s}=F(x_0) \]
      \item If $ x_0<x_1 $, then $ \pi_0<\pi_1 $.
\end{itemize}
\subsection*{Modelling the Dose-Response Relationship}
For each group $ i=1,\ldots,n $ let:
\begin{itemize}
      \item $ x_i= $ dose applied to subjects in group $ i $,
      \item $ m_i= $ number of subjects in group $ i $,
      \item $ y_i= $ the number of subjects with response in group $ i $.
            \begin{table}[!htbp]
                  \centering
                  \begin{tabular}{cccc}
                        \toprule
                        Dose       & Responders & Total                  \\
                        $ x_i $    & $ y_i $    & $ m_i $  & $ y_i/m_i $ \\
                        \midrule
                        $ 1.6907 $ & $ 6 $      & $ 59 $   & $ 0.10 $    \\
                        $ 1.7242 $ & $ 13 $     & $ 60 $   & $ 0.22 $    \\
                        $ 1.7552 $ & $ 18 $     & $ 62 $   & $ 0.29 $    \\
                        $\vdots$   & $\vdots$   & $\vdots$ & $\vdots$    \\
                        \bottomrule
                  \end{tabular}
            \end{table}
      \item Assume
            \[ \textcolor{Blue}{Y_i \sim \BIN{m_i,\pi_i},\; i=1,\ldots,n}, \]
            \[ \textcolor{Blue}{\pi_i} = \text{probability of response in group $ i $ with dose $ x_i $}. \]
      \item \textcolor{Red}{Objective}: To model probability of response $ \pi_i $ as a function of dose $ x_i $.
      \item Binomial Regression Models:
            \[ g(\pi_i)=\eta_i=\beta_0+\beta_1x_i, \]
            where $ g(\:\cdot\:) $ is a choice of link function.
      \item Then we have:
            \[ \pi_i=g^{-1}(\beta_0+\beta_1x_i), \]
            that is, the probability of response as a function of dose $ x_i $ via $ g^{-1}(\:\cdot\:) $.
      \item Question: What link function should we select?
      \item Realize that:
            \begin{itemize}
                  \item If we assume a tolerance distribution $ f(x) $, the probability of response to dose $ x_i $ is:
                        \[ \pi_i=\int_{-\infty}^{x_i}f(x)\odif{x} =F(x_i). \]
                  \item With a Binomial regression model and a link function $ g(\:\cdot\:) $, we have:
                        \[ \pi_i=g^{-1}(\beta_0+\beta_1x_i). \]
            \end{itemize}
      \item These suggest that the choice of the tolerance distribution determines the form of the link function, i.e., selecting
            $ g(\:\cdot\:) $ such that $ g^{-1}(\:\cdot\:) $ is a cdf:
            \[ \pi_i=g^{-1}(\beta_0+\beta_1x_i)=F^\star(\beta_0+\beta_1x_i). \]
\end{itemize}
\subsection*{Some Choices for the Tolerance Distribution}
\begin{enumerate}[label=\color{Blue}\protect\circled{\arabic*}]
      \item \textcolor{Blue}{Normal Tolerance Distribution}:
            \begin{align*}
                  \pi(x)
                   & =\int_{-\infty}^{x}f(s)\odif{s}                                                                                        \\
                   & =\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi\sigma^2}} \exp*{-\frac{1}{2} \biggl(\frac{s-\mu}{\sigma} \biggr)^{\!2}}\odif{s} \\
                   & =\Phi\biggl(\frac{x-\mu}{\sigma}\biggr)
            \end{align*}
            where $ \Phi $ is the $ \N{0,1} $ cdf. This implies that
            \[ \Phi^{-1}(\pi)=\frac{x-\mu}{\sigma},  \]
            i.e., the \textcolor{Blue}{Probit link} s.t.,
            \[ g(\pi)=\Phi^{-1}(\pi)=-\frac{\mu}{\sigma} +\frac{1}{\sigma} x=\beta_0+\beta_1x. \]
            A Binomial Probit Model:
            \[ \Phi^{-1}(\pi)=\beta_0+\beta_1x. \]
            How do we interpret $ \beta_0 $ and $ \beta_1 $?
            \begin{itemize}
                  \item They are no longer log odds ratios (as with logistic link)
                  \item Interpretation is in terms of $ \mu $ and $ \sigma $ the parameters of the Normal
                        distribution for tolerance, i.e.,
                        \[ \beta_0=-\frac{\mu}{\sigma} ,\qquad\beta_1=\frac{1}{\sigma} . \]
            \end{itemize}
      \item \textcolor{Blue}{Logistic Distribution}:
            \[ f(x;\mu,s)=\frac{\exp*{-\frac{x-\mu}{s}}}{s\biggl[1+\exp*{-\frac{x-\mu}{s}}\biggr]^2},\; s>0,\,\E{X}=\mu.  \]
            The probability of response:
            \begin{align*}
                  \pi(x)                         & =\int_{-\infty}^{x}f(x;\mu,s)\odif{s}=\biggl[1+\exp*{-\frac{x-\mu}{s} }\biggr]^{-1} \\
                  1-\pi(x)                       & =\frac{\exp*{-\frac{x-\mu}{s}}}{1+\exp*{-\frac{x-\mu}{s}}}                          \\
                  \log*{\frac{\pi(x)}{1-\pi(x)}} & =\frac{x-\mu}{s}.
            \end{align*}
            This implies the \textcolor{Blue}{Logit link} s.t.,
            \[ g(\pi)=\logit{\pi}=-\frac{\mu}{s}+\frac{1}{s} x=\beta_0+\beta_1x. \]
      \item \textcolor{Blue}{Extreme Value Distribution}:
            \[ f(x;\mu,s)=\frac{1}{s} \exp*{\frac{x-\mu}{s}-\exp*{\frac{x-\mu}{s}}},\;s>0. \]
            The probability of response:
            \begin{align*}
                  \pi(x)
                                                    & =\int_{-\infty}^{x}f(x;\mu,s)\odif{s} \\
                                                    & =1-\exp*{-\exp*{-\frac{x-\mu}{s}}}    \\
                  \log[\Big]{-\log[\big]{1-\pi(x)}} & =\frac{x-\mu}{s}.
            \end{align*}
            This implies the \textcolor{Blue}{Complementary log-log link} s.t.,
            \[ g(\pi)=\log[\big]{-\log{1-\pi}}=-\frac{\mu}{s} +\frac{1}{s} x=\beta_0+\beta_1x. \]
\end{enumerate}
\begin{table}[!htbp]
      \centering
      \begin{tabular}{ccc}
            \toprule
            \textcolor{Blue}{Tolerance Distribution} & \textcolor{Blue}{Link Function} & \textcolor{Blue}{Dose-Response Model}         \\
            \midrule
            Normal                                   & Probit                          & $ \Phi^{-1}(\pi)=\beta_0+\beta_1x $           \\
            Logistic                                 & Logit                           & $ \logit{\pi}=\beta_0+\beta_1x $              \\
            Extreme Value                            & Complementary log-log           & $ \log[\big]{-\log{1-\pi}}=\beta_0+\beta_1x $ \\
            \bottomrule
      \end{tabular}
\end{table}
\subsection*{Median Lethal/Effective Dose}
\begin{itemize}
      \item The \textcolor{Blue}{median lethal/effective dose} (ED50) is the dose at which \qty{50}{\percent} of the
            population has the response.
      \item That is, if we let $ \delta $ be the ED50, then by definition:
            \[ \pi(\delta)=\int_{-\infty}^{\delta}f(x)\odif{x} =0.50. \]
      \item How do we find the expression of $ \delta $ given a Dose-Response model? Suppose we fit a Binomial Probit model (i.e., Normal tolerance distribution):
            \[ \Phi^{-1}(\pi)=\beta_0+\beta_1x. \]
            Note that at dose $ \delta $ (ED50), $ \pi=0.50 $.
            \begin{align*}
                  \Phi^{-1}(0.50) & =\beta_0+\beta_1\delta    \\
                  0               & =\beta_0+\beta_1\delta    \\
                  \delta          & =-\frac{\beta_0}{\beta_1}
            \end{align*}
\end{itemize}
\subsection*{A Dose-Response Example --- Beetle Mortality}
\begin{Example}{Beetle Mortality}
      Consider an experiment by Bliss (Annals of Applied Biology, 1935) in which groups of
      beetles were exposed to varying concentrations of carbon disulphide ($\text{CS}_2$) gas.
      \begin{center}
            \begin{tabular}{cccc}
                  \toprule
                                 & \text{\# of insects} & \text{\# of insects}               \\
                  Dose ($ x_i $) & killed ($ x_i $)     & $ m_i $              & $ y_i/m_i $ \\
                  \midrule
                  $ 1.6907 $     & $ 6 $                & $ 59 $               & $ 0.10 $    \\
                  $ 1.7242 $     & $ 13 $               & $ 60 $               & $ 0.22 $    \\
                  $ 1.7552 $     & $ 18 $               & $ 62 $               & $ 0.29 $    \\
                  $1.7842$       & $28$                 & $56$                 & $0.50$      \\
                  $1.8113$       & $52$                 & $63$                 & $0.83$      \\
                  $1.8369$       & $53$                 & $59$                 & $0.89$      \\
                  $1.8610$       & $61$                 & $62$                 & $0.98$      \\
                  $1.8839$       & $60$                 & $60$                 & $1.00$      \\
                  \bottomrule
            \end{tabular}
      \end{center}
\end{Example}
\begin{itemize}
      \item \textcolor{Blue}{Objective}: modelling the dose-response relationship.
      \item We will fit several binomial regression models to this data:
            \[ g(\pi_i)=\beta_0+\beta_1x_i, \]
            where $ x_i= $ dose in group $ i $, $ i=1,\ldots,8 $.
      \item Various link functions will be used to find the best fitted model:
            \begin{itemize}
                  \item \texttt{Logistic} link.
                  \item \texttt{Probit} link.
                  \item \texttt{Cloglog} link.
            \end{itemize}
\end{itemize}
\subsection*{Dose-Response Analysis using R}
% \begin{noindent}

% \end{noindent}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# read beetle data}
\hlstd{beetle.dat} \hlkwb{=} \hlkwd{read.table}\hlstd{(}\hlstr{"beetle.dat"}\hlstd{,} \hlkwc{header} \hlstd{= T)}
\hlcom{# here we construct the response variable for Binomial}
\hlcom{# regression}
\hlstd{beetle.dat}\hlopt{$}\hlstd{resp} \hlkwb{<-} \hlkwd{cbind}\hlstd{(beetle.dat}\hlopt{$}\hlstd{y, beetle.dat}\hlopt{$}\hlstd{m} \hlopt{-} \hlstd{beetle.dat}\hlopt{$}\hlstd{y)}
\hlstd{beetle.dat}
\end{alltt}
\begin{verbatim}
    dose  y  m resp.1 resp.2
1 1.6907  6 59      6     53
2 1.7242 13 60     13     47
3 1.7552 18 62     18     44
4 1.7842 28 56     28     28
5 1.8113 52 63     52     11
6 1.8369 53 59     53      6
7 1.8610 61 62     61      1
8 1.8839 60 60     60      0
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\subsection*{Fit of the Logistic Model}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model1} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{dose,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= logit),} \hlkwc{data} \hlstd{= beetle.dat)}
\hlkwd{summary}\hlstd{(model1)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = resp ~ dose, family = binomial(link = logit), data = beetle.dat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5941  -0.3944   0.8329   1.2592   1.5940  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -60.717      5.181  -11.72   <2e-16 ***
dose          34.270      2.912   11.77   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 284.202  on 7  degrees of freedom
Residual deviance:  11.232  on 6  degrees of freedom
AIC: 41.43

Number of Fisher Scoring iterations: 4
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\subsection*{Fit of the Probit Model}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{dose,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= probit),} \hlkwc{data} \hlstd{= beetle.dat)}
\hlkwd{summary}\hlstd{(model2)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = resp ~ dose, family = binomial(link = probit), 
    data = beetle.dat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5714  -0.4703   0.7501   1.0632   1.3449  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -34.935      2.648  -13.19   <2e-16 ***
dose          19.728      1.487   13.27   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 284.20  on 7  degrees of freedom
Residual deviance:  10.12  on 6  degrees of freedom
AIC: 40.318

Number of Fisher Scoring iterations: 4
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\subsection*{Fit of the Complementary Log-log Model}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model3} \hlkwb{=} \hlkwd{glm}\hlstd{(resp} \hlopt{~} \hlstd{dose,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{= cloglog),} \hlkwc{data} \hlstd{= beetle.dat)}
\hlkwd{summary}\hlstd{(model3)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = resp ~ dose, family = binomial(link = cloglog), 
    data = beetle.dat)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.80329  -0.55135   0.03089   0.38315   1.28883  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -39.572      3.240  -12.21   <2e-16 ***
dose          22.041      1.799   12.25   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 284.2024  on 7  degrees of freedom
Residual deviance:   3.4464  on 6  degrees of freedom
AIC: 33.644

Number of Fisher Scoring iterations: 4
\end{verbatim}
\end{kframe}
\end{knitrout}
% \end{noindent}
\subsection*{Deviance Residual Plots}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-50-1} 

}


\end{knitrout}
% \end{noindent}
\subsection*{Choice of Tolerance Distribution or Binomial Model}
\begin{itemize}
      \item Observed probability of response:
            \[ \tilde{\pi}_i=\frac{y_i}{m_i}. \]
      \item Fitted probability of response:
            \[ \hat{\pi}_i=g^{-1}(\hat{\beta}_0+\hat{\beta}_1x_i). \]
      \item The tolerance distribution (or the Binomial model) that provides the ``best''
            agreement between the observed and fitted probability of response is the one that fits the data the ``best.''
      \item We can check this by plotting the observed and fitted probability of response $ \tilde{\pi}_i $ and $ \hat{\pi}_i $,
            against dose $ x_i $.
\end{itemize}
\subsection*{Fitted Dose-Response Curves}
% \begin{noindent}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-51-1} 

}


\end{knitrout}
% \end{noindent}
\begin{itemize}
      \item Note that the curve for the complementary log-log link fits the data better than
            the other two, as expect from the residual plots and the deviance statistics.
      \item (The R code for generating above plot see course notes, 2.10.3, page 47).
\end{itemize}
\subsection*{Interpretation of Dose-Response Models}
\begin{itemize}
      \item Interpretation of regression parameter $ \beta_1 $ will depend on the link function.
            \begin{itemize}
                  \item Logistic model: $ \logit{\pi}=\beta_0+\beta_1x $.
                        \begin{itemize}
                              \item $ \beta_1= $ log odds ratio for response associated with a one unit increase in dose.
                        \end{itemize}
                  \item Probit model: $ \Phi^{-1}(\pi)=\beta_0+\beta_1x $, or Complementary log-log model $ \log[\big]{-\log{1-\pi}}=\beta_0+\beta_1x $,
                        interpretation of $ \beta $ parameters is not as natural as in logistic models.
            \end{itemize}
      \item Estimation of $ \delta $ (ED50) from a Binomial model $ g(\pi)=\beta_0+\beta_1x $:
            \[ g(\pi=0.5)=\beta_0+\beta_1\delta\implies \hat{\delta}=\frac{g(0.5)-\hat{\beta}_0}{\hat{\beta}_1}.  \]
      \item \textcolor{Blue}{Exercise}: What is $ \delta_{0.25} $, the dose at which \qty{25}{\percent} of the population has the response?
\end{itemize}
% Week 6 (Reading Week), Week 7

\makeheading{Week 6}{\daterange{2021-10-11}{2021-10-15}}
Reading week.
\makeheading{Week 7}{\daterange{2021-10-18}{2021-10-22}}
\section*{Topic 3f: Summary of Binomial Regression Models}
\addcontentsline{toc}{section}{Topic 3f: Summary of Binomial Regression Models}
\subsection*{Binomial GLM Specification}
\begin{itemize}
      \item $ Y_i \sim \BIN{m_i,\pi_i} $, $ i=1,\ldots,n $ independently and
            \[ g(\pi_i)=\Vector{x}_i^\top \Vector{\beta}, \]
            where
            \begin{itemize}
                  \item $ \Vector{x}_i $ is a vector of explanatory variables,
                  \item $ \pi_i $ is the probability of event of interest,
                  \item $ g(\:\cdot\:) $ is a link function that relates explanatory variables $ \Vector{x}_i $
                        to probability $ \pi_i $, and
                  \item $ \Vector{\beta} $ is a vector of regression parameters.
            \end{itemize}
      \item When using the canonical link of Binomial distribution, i.e., $ g(\:\cdot\:)=\logit{\:\cdot\:} $, we
            have
            \[ \logit{\pi_i}=\log*{\frac{\pi_i}{1-\pi_i}}=\Vector{x}_i^\top \Vector{\beta}, \]
            which is called a \emph{\textcolor{Red}{logistic regression model}} which is commonly used in practice.
\end{itemize}
\subsection*{Parameters Estimation}
\begin{itemize}
      \item Likelihood methods are used for parameter estimating and inference.
      \item MLE $ \hat{\Vector{\beta}} $ come from Fisher Scoring using R function \textcolor{Red}{\texttt{glm()}}.
      \item Interpretation: $ \beta_k $ has a \textcolor{Red}{log OR interpretation} for logistic models.
      \item Variance covariance estimate for $ \hat{\Vector{\beta}}=\estVar{\hat{\Vector{\beta}}}=\Matrix{I}^{-1}(\hat{\Vector{\beta}}) $,
            where $ \Matrix{I}^{-1} $ is the inverse of the information matrix evaluated at MLE $ \hat{\Vector{\beta}} $.
      \item Standard error: $ \se{\hat{\beta}_k}=\sqrt{\bigl[\Matrix{I}^{-1}(\hat{\Vector{\beta}})\bigr]_{kk}}=\sqrt{I^{kk}(\hat{\Vector{\beta}})} $.
      \item Wald-test of a single parameter: $ \HN $: $ \beta_k=\beta^\star $ vs $ \HA $: $ \beta_k\ne \beta^\star $:
            \[ \frac{(\hat{\beta}_k-\beta^\star)^2}{I^{kk}(\hat{\beta}_k)}\stackrel{\HN}{\sim}\chi^2_1,  \]
            or
            \[ \frac{\hat{\beta}_k-\beta^\star}{\se{\hat{\beta}_k}}\sim \N{0,1}\text{ under $ \HN $}.  \]
            For testing $ \HN $: $ \beta_k=0 $, we have $ \frac{\hat{\beta_k}}{\se{\hat{\beta}_k}} $, reported as ``\texttt{z-value}'' in \texttt{glm()} summary.
      \item Confidence interval for a single $ \beta_k $:
            \[ \hat{\beta}_k\pm Z_{1-\alpha/2}\se{\hat{\beta}_k}. \]
      \item Confidence interval for $ \eta_i=\Vector{x}_i^\top \Vector{\beta} $:
            \[ \Vector{x}_i^\top \hat{\Vector{\beta}}\pm Z_{1-\alpha/2}\sqrt{\Vector{x}_i^\top \Matrix{I}^{-1}(\hat{\Vector{\beta}})\Vector{x}_i}, \]
            or equivalently
            \[ \Vector{x}_i^\top \hat{\Vector{\beta}}\pm Z_{1-\alpha/2}\sqrt{\Vector{x}_i^\top \estVar{\Vector{x}_i^\top \hat{\Vector{\beta}}}\Vector{x}_i}. \]
            How about CI for $ \pi_i=\expit{\Vector{x}_i^\top \Vector{\beta}} $?
\end{itemize}
\subsection*{Model Checking and Selection}
\begin{itemize}
      \item LR/Deviance: Recall $ \text{LR}=-2\log*{\frac{L(\hat{\Vector{\pi}})}{L(\tilde{\Vector{\pi}})}}=
                  -2\bigl(\ell(\hat{\Vector{\pi}})-\ell(\tilde{\Vector{\pi}})\bigr) $.
            \begin{align*}
                  D
                   & =-2\bigl[\ell(\hat{\Vector{\pi}})-\ell(\tilde{\Vector{\pi}})\bigr]                                                        \\
                   & =-2 \sum_{i=1}^{n} \biggl(y_i\log*{\frac{y_i}{m_i\hat{\pi}_i}}+(m_i-y_i)\log*{\frac{m_i-y_i}{m_i(1-\hat{\pi}_i)} }\biggr) \\
                   & =\sum_{i=1}^{n} d_i.
            \end{align*}
      \item LR/Deviance test for \textcolor{Red}{\emph{adequacy of a model}} ($ \HN $: fitted model is as adequate as the saturated model):
            \[ D \sim \chi^2_{n-p}\text{ under $\HN$}. \]
      \item LR/Difference in Deviance test for \textcolor{Red}{\emph{comparing nested models}} ($ \HN $: reduced/simpler model is as adequate as the fitted model):
            \[ \Delta D=D_0-D_\text{A} \sim \chi^2_{p-q}\text{ under $ \HN $}. \]
      \item Deviance Residuals:
            \[ r_i^D = \sign{y_i-m_i\hat{\pi}_i}\sqrt{\abs{d}}, \]
            where $ r_i^D $'s should behave like an iid sample from $ \N{0,1} $ for a well-fitted model.
      \item Residuals plots:
            \begin{itemize}
                  \item \textcolor{Red}{\emph{deviance residual vs fitted value}} (i.e., $ r_i^D $ vs $ \hat{\pi}_i $),
                  \item \textcolor{Red}{\emph{deviance residual vs covariate}} (i.e., $ r_i^D $ vs $ \Vector{x}_i $).
                  \item In both cases, we expect a pattern of random scatter around $ 0 $, within $ (-2,2) $.
            \end{itemize}
      \item Residual plots can be used to evaluate the fit of a model or compare multiple models in general.
            \begin{itemize}
                  \item For example, non-nested models, using different link functions.
            \end{itemize}
\end{itemize}
\subsection*{Binomial Model for Dose-Response Relationship}
\begin{itemize}
      \item Dose: $ X=\log{\text{concentration}} $.
      \item Tolerance distribution is $ f(x) $ and probability of responding to dose $ x $ is:
            \[ \pi(x)=\Prob{X\le x}=\int_{-\infty}^{x}f(x)\odif{x}=F(x). \]
      \item Binomial GLMs can be utilized to evaluate the dose-response relationship:
            \[ g(\pi)=\beta_0+\beta_1x \]
            \begin{table}[!htbp]
                  \centering
                  \begin{tabular}{cc}
                        \toprule
                        \textcolor{Red}{Link} & \textcolor{Red}{Tolerance Distribution} \\
                        \midrule
                        Logit                 & Logistic$(\mu,s)$                       \\
                        Probit                & Normal$(\mu,s)$                         \\
                        Cloglog               & Extreme Value$(\mu,s)$                  \\
                        \bottomrule
                  \end{tabular}
            \end{table}
            In every case,
            \[ \beta_0=-\frac{\mu}{s},\qquad \beta_1=\frac{1}{s}. \]
      \item Estimation of the \textcolor{Red}{\emph{median lethal/effective dose}} ($ \delta_{0.5} $):
            \[ g(0.5)=\hat{\beta}_0+\hat{\beta}_1\delta_{0.5}\implies \delta_{0.5}=\frac{g(0.5)-\hat{\beta}_0}{\hat{\beta}_1}.  \]
      \item Calculation of dose related to $ q\textsuperscript{th} $ percentile of response.
\end{itemize}

\section*{Topic 4a: Poisson GLMs for Count Data}
\addcontentsline{toc}{section}{Topic 4a: Poisson GLMs for Count Data}
\subsection*{The Poisson Distribution}
\begin{itemize}
      \item Recall for $ Y \sim \POI{\mu} $,
            \[ f(y)=\frac{\mu^y e^{-\mu}}{y!}=\exp[\big]{y\log{\mu}-\mu-\log{y!}},\; \mu>0,\, y=0,1,2,\ldots.  \]
            Examples of count data:
            \begin{itemize}
                  \item Health service, \# of emergency visits, \# of hospitalizations.
                  \item Insurance, \# of claims.
                  \item Engineering/manufacturing, \# of defects.
            \end{itemize}
      \item The Poisson is a member of the \textcolor{Red}{\emph{exponential family}} with
            \begin{align*}
                  \theta       & =\log{\mu},      \\
                  a(\phi)=\phi & =1,              \\
                  b(\theta)    & =e^{\theta}=\mu, \\
                  c(y;\theta)=-\log{y!}.
            \end{align*}
      \item Mean and variance:
            \begin{align*}
                  \E{Y}   & =b^\prime(\theta)=e^\theta=\mu,                  \\
                  \Var{Y} & =b^{\prime\prime}(\theta)a(\phi)=e^{\theta}=\mu.
            \end{align*}
            Therefore, $ \E{Y}=\Var{Y} $.
      \item The \textcolor{Red}{\emph{Canonical link}}:
            \[ \theta=\eta\implies \log{\mu}=\eta=x^\top \beta, \]
            the log link, $ g(\mu)=\log{\mu} $, is the canonical link.
\end{itemize}
\subsection*{Poisson Log Linear Model and Likelihood Function}
\begin{itemize}
      \item Now, suppose we have a random sample of size $ n $:
            \[ Y_i \sim \POI{\mu_i},\; i=1,2,\ldots,n, \]
            and association with each $ y_i $ there is a covariate vector $ \Vector{x}_i^\top=(1,x_{i1},\ldots,x_{ip-1})^\top $.
      \item The likelihood and log-likelihood are then
            \begin{align*}
                  L(\Vector{\mu})               & =\prod_{i=1}^n \frac{\mu_i^{y_i}e^{-\mu_i}}{y_i!},           \\
                  \ell(\Vector{\mu};\Vector{y}) & =\sum_{i=1}^{n} \bigl(y_i\log{\mu_i}-\mu_i-\log{y_i!}\bigr).
            \end{align*}
      \item Using the \textcolor{Red}{Canonical link} (i.e., log link):
            \[ \log{\mu_i}=\Vector{x}_i^\top \Vector{\beta}=\sum_{j=0}^{p-1} x_{ij}\beta_j, \]
            which is referred to as \textcolor{Red}{log linear regression} because the use of the log link.
      \item We can obtain the log-likelihood in terms of $ \Vector{\beta} $ by substitution:
            \begin{align*}
                  \ell(\Vector{\mu};\Vector{y})
                   & =\sum_{i=1}^{n} \bigl(y_i\log{\mu_i}-\mu_i-\log{y_i!}\bigr)                                                         \\
                   & =\sum_{i=1}^{n} \bigl(y_i \Vector{x}_i^\top \Vector{\beta}-\exp{\Vector{x}_i^\top \Vector{\beta}}-\log{y_i!}\bigr).
            \end{align*}
\end{itemize}
\subsection*{Estimation of $ \Vector{\beta} $ from log linear regression}
\begin{itemize}
      \item The $ j\textsuperscript{th} $ contribution to the Score vector is:
            \[ \pdv{\ell}{\beta_j}=\sum_{i=1}^{n} \bigl(y_i x_{ij}-x_{ij}\exp{\Vector{x}_i^\top \Vector{\beta}}\bigr). \]
      \item The $ (j,k) $ element of the Information Matrix is:
            \[ -\pdv{\ell}{\beta_j,\beta_k}=\sum_{i=1}^{n} \bigl(x_{ij}x_{ik}\exp{\Vector{x}_i^\top \Vector{\beta}}\bigr). \]
      \item These can also be found using general exponential family results.
      \item Use the above to estimate $ \hat{\Vector{\beta}} $ via Fisher Scoring.
\end{itemize}
\subsection*{Poisson Deviance/LR Tests}
\begin{itemize}
      \item Let $ \tilde{\mu}_i $ be the MLE under the \textcolor{Red}{saturated model} (i.e., $ \tilde{\mu}_i=y_i $ which is the Poisson MLE for $ \mu_i $).
      \item Let $ \hat{\mu}_i $ be the MLE under a $ p $-dimensional \textcolor{Red}{constrained model} (e.g., $ \hat{\mu}_i\exp{\Vector{x}_i^\top \hat{\Vector{\beta}}} $).
      \item Recall the Likelihood Ratio or Deviance Statistic has the form:
            \[ D=-2\log*{\frac{L(\hat{\Vector{\mu}})}{L(\tilde{\Vector{\mu}})}}=2\bigl(\ell(\tilde{\Vector{\mu}})-\ell(\hat{\Vector{\mu}})\bigr). \]
      \item Under $ \HN $: constrained model is as adequate as saturated model, we have the following asymptotic distribution result:
            \[ D \sim \chi^2_{n-p}. \]
      \item For the Poisson we have:
            \begin{align*}
                  D
                   & =2 \sum_{i=1}^{n} \Bigl(\bigl(y_i\log{\tilde{\mu}_i}-\tilde{\mu}_i\bigr)-\bigl(y_i\log{\hat{\mu}_i}-\hat{\mu}_i\bigr)\Bigr) \\
                   & =2 \sum_{i=1}^{n} \biggl(y_i\log*{\frac{y_i}{\hat{\mu}_i}}-(y_i-\hat{\mu}_i)\biggr)                                         \\
                   & =2 \sum_{i=1}^{n} \biggl(O_i\log*{\frac{O_i}{E_i}}-(O_i-E_i)\biggr).
            \end{align*}
      \item Question: does the Deviance Statistic have the form as the Binomial case, i.e.,
            \[ D=2\sum O_i\log*{\frac{O_i}{E_i}}\text{ ?} \]
            When there is an intercept included in the Poisson log-linear model:
            \[ \pdv{\ell}{\beta_0}=\pdv*{\biggl[\sum_{i=1}^{n} y_i \Vector{x}_i^\top \Vector{\beta}-\exp{\Vector{x}_i^\top \Vector{\beta}}\biggr]}{\beta_0}
                  =\sum_{i=1}^{n} (y_i-\mu_i)\implies \sum_{i=1}^{n} (y_i-\hat{\mu}_i)=0, \]
            then the Deviance takes the form
            \[ D=2 \sum_{i=1}^{n} y_i\log*{\frac{y_i}{\hat{\mu}_i}}=2 \sum_{i=1}^{n} O_i \log*{\frac{O_i}{E_i}}. \]
      \item Use the Deviance to test nested models:
            \begin{itemize}
                  \item $ \HN $: the reduced model with $ p $ parameters is adequate versus
                        \[ \log{\mu_i}=\beta_0+\beta_1x_{i1}+\cdots+\beta_{p-1}x_{ip-1} \]
                  \item $ \HA $: the full model with $ q $ parameters ($ p<q $)
                        \[ \log{\mu_i}=\beta_0+\beta_1x_{i1}+\cdots+\beta_{p-1}x_{ip-1}+\cdots+\beta_{q-1}x_{iq-1}. \]
            \end{itemize}
      \item LR/Difference in Deviance test statistic:
            \[ \Delta D=D_0-D_\text{A} \sim \chi^2_{q-p}\text{ under $ \HN $}. \]
      \item The $ p $-value for this test is given by:
            \[ p\text{-value}=\Prob{\chi^2_{q-p}>\Delta D}. \]
\end{itemize}
\subsection*{Deviance Residuals}
\begin{itemize}
      \item We can write the Deviance as a sum:
            \[ D=2 \sum_{i=1}^{n} \biggl(y_i\log*{\frac{y_i}{\hat{\mu}_i}}-(y_i-\hat{\mu}_i)\biggr)=\sum_{i=1}^{n} d_i. \]
      \item The \textcolor{Red}{Deviance Residuals} are given by:
            \[ r_i^D=\sign{y_i-\hat{\mu}_i}\sqrt{\abs{d_i}}, \]
            and are approximately $ \N{0,1} $ if $ \HN $ holds.
      \item We can use the residual plots to evaluate the fit of a model.
\end{itemize}
\subsection*{Regression for Poisson Processes}
\begin{itemize}
      \item The Poisson distribution assumes a \textcolor{Red}{\emph{common observation period}} for all individuals,
            so that the number of event does not depend on the time at risk.
      \item However, this may not be the case for many situations in practice.
\end{itemize}
\begin{Regular}{Counting Process $ N(t) $}
      A \textcolor{Red}{counting process} $ N(t) $ is any non-decreasing integer function of time such that
      $ N(0)=0 $ and $ N(t) $ is the number of events occurring in $ (0,t] $.
\end{Regular}
\begin{itemize}
      \item \textcolor{Green}{Example}: Suppose events occurred at times $ (2,4,5,7) $.
      \item Draw a plot of $ N(t) $ versus $ t $:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-52-1} 

}


\end{knitrout}
\end{itemize}
\begin{Regular}{Poisson Process $ N(t) $}
      A counting process $ N(t) $ is a \textcolor{Red}{Poisson process} if it satisfies:
      \begin{enumerate}[1.]
            \item \textcolor{Red}{Independent increments}: For $ s_1<t_1<s_2<t_2 $:
                  \[ N(t_1)-N(s_1)=\text{\# events in $(s_1,t_1]$}, \]
                  is independent of
                  \[ N(t_2)-N(s_2)=\text{\# events in $(s_2,t_2]$}. \]
            \item The number of events over $ (0,t] $ has a Poisson distribution, i.e.,
                  \[ \Prob[\big]{N(t)=n;\lambda}=\frac{(\lambda t)^n e^{-\lambda t}}{n!},\; \lambda>0,\, n=0,1,2,\ldots.  \]
      \end{enumerate}
\end{Regular}
\begin{itemize}
      \item Expected number of events in $ (0,t] $ is
            \[ \E[\big]{N(t)}=\mu(t)=\lambda t. \]
            Parameter $ \lambda $ is a constant representing the \emph{rate of occurrence of the event per unit of time}:
            \begin{align*}
                  \lambda & =\textcolor{Red}{\text{rate parameter}},               \\
                  t       & =\textcolor{Red}{\text{length of observation period}}.
            \end{align*}
      \item Since $ \lambda $ is constant (not a function of $t$) we call this a \textcolor{Red}{time homogeneous Poisson process}.
      \item Use the log link to do regression:
            \[ \log[\big]{\mu(t)}=\log{\lambda t}=\log{\lambda}+\log{t}. \]
\end{itemize}
For each subject $ i=1,\ldots, n $ we observe:
\begin{itemize}
      \item $ N_i(t_i)= $ the number of events observed over $ (0,t_i] $.
      \item Explanatory variables: $ \Vector{x}_i=(1,x_{i1},\ldots,x_{ip-1})^\top $.
\end{itemize}
\begin{Regular}{Log Linear Regression Model for a Time Homogeneous Poisson Process}
      \begin{align*}
            \log[\big]{\mu_i(t_i)}
             & =\log{\lambda_i}+\log{t_i}                                                                                          \\
             & =\Vector{x}_i^\top \Vector{\beta}+\log{t_i} &  & \text{e.g., $ \log{\lambda_i}=\Vector{x}_i^\top \Vector{\beta} $.}
      \end{align*}
\end{Regular}
\begin{itemize}
      \item The term $ \log{t_i} $ is called an ``\textcolor{Red}{\emph{offset term}}''.
      \item It accounts for different lengths of observation.
      \item It \emph{explains} some variation in the event counts across subjects, but does so in a deterministic way.
\end{itemize}
Next week: an example of fitting Poisson GLM using R.
% Week 8

\section*{Topic 4b: Ship Damage Example}
\addcontentsline{toc}{section}{Topic 4b: Ship Damage Example}
\subsection*{Recall: Regression for Poisson Processes}
\begin{itemize}
    \item Last week in Topic 4a, we introduced Poisson GLMs for count data.
    \item Suppose from each subject $ i=1,\ldots,n $ we observe:
          \begin{itemize}
              \item $ N_i(t_i)= $ the number of events observed over $ (0,t_i] $.
              \item $ \Vector{x}_i=(1,x_{i1},\ldots,x_{ip-1})^\top $, the covariate vector associated with count $ N_i(t_i) $.
          \end{itemize}
    \item Assume $ N_i(t) $ is a \textcolor{Red}{\emph{time-homogenous Poisson process}} s.t. $ N_i(t_i)\sim \POI{\:\cdot\:} $ and
          \[ \E[\big]{N_i(t_i)}=\mu_i(t_i)=\lambda_i t_i, \]
          where $ \lambda_i $ is the rate of occurrence that is constant, therefore TH process.
    \item A log linear regression model:
          \[ \log[\big]{\mu_i(t_i)}=\log{\lambda_i}+\underbrace{\textcolor{Blue}{\log{t_i}}}_{\text{offset}}=\Vector{x}_i^\top \Vector{\beta}+\log{t_i}. \]
          Regression part: $ \log{\lambda_i}=\Vector{x}_i^\top \Vector{\beta} $.
    \item Maximum likelihood methods apply as usual for the estimation and inference of regression coefficients $ \Vector{\beta} $.
\end{itemize}
\end{document}
% For fonts.
\begin{itemize}
    \item \sbox0{c}\the\ht0\ versus \sbox0{\texttt{c}}\the\ht0\ versus \sbox0{$c$}\the\ht0\ versus \sbox0{\textsf{c}}\the\ht0\
    \item c\texttt{c}$c$\textsf{c}
\end{itemize}
Font shape `T1/zi4/m/it' undefined
(Font)	using `T1/zi4/m/n' instead.
U/stmry/b/n'
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}
